{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IntelOwl Centralized Documentation","text":"<p>Welcome to the IntelOwl Centralized Documentation. Here you will be able to find all documentation for all projects under IntelOwl.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Intel Owl is an Open Source Intelligence, or OSINT solution, to get Threat Intelligence data about a specific digital artifact from a single API at scale. It integrates a high number of services available online and a lot of cutting-edge malware analysis tools. It is for everyone who needs a single point to query for info about a specific file or observable. If you are a Security Analyst, do not waste any more time in performing enrichment tasks! IntelOwl saves your time and allows you to concentrate on more serious tasks.</p>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li> Introduction  More</li> </ul> <ul> <li> Installation  More</li> </ul> <ul> <li> Contribute  More</li> </ul> <ul> <li> Usage  More</li> </ul> <ul> <li> Advanced Usage  More</li> </ul> <ul> <li> Advanced Configuration  More</li> </ul> <p>Need more help?</p> <p>We are doing our best to keep this documentation complete, accurate and up to date.</p> <p>If you still have questions or you find something which is not sufficiently explained, join the IntelOwl channel under HoneyNet Community on Slack.</p> <p> </p>"},{"location":"Guide-docstrings/","title":"Docstrings guide","text":""},{"location":"Guide-docstrings/#implementing-docstrings-in-python-code","title":"Implementing Docstrings in Python Code","text":"<p>When you write or modify Python code in the codebase, it's important to add or update the docstrings accordingly. If you wish to display these docstrings in the documentation, follow these steps.</p> <p>Suppose the docstrings are located in the following path: <code>docs/Submodules/IntelOwl/api_app/analyzers_manager/classes</code>, and you want to show the description of a class, such as BaseAnalyzerMixin.</p> <p>To include this in the documentation, use the following command:</p> <pre><code>:::docs.Submodules.IntelOwl.api_app.analyzers_manager.classes.BaseAnalyzerMixin\n</code></pre> <p>Warning</p> Make sure your path is correct and syntax is correct. If you face any issues even path is correct then read the Submodules Guide."},{"location":"Guide-docstrings/#this-is-how-it-would-look-in-documentation","title":"This is how it would look in documentation:","text":"<p>               Bases: <code>Plugin</code></p> <p>Abstract Base class for Analyzers. Never inherit from this branch, always use either one of ObservableAnalyzer or FileAnalyzer classes.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/analyzers_manager/classes.py</code> <pre><code>class BaseAnalyzerMixin(Plugin, metaclass=ABCMeta):\n    \"\"\"\n    Abstract Base class for Analyzers.\n    Never inherit from this branch,\n    always use either one of ObservableAnalyzer or FileAnalyzer classes.\n    \"\"\"\n\n    HashChoices = HashChoices\n    ObservableTypes = ObservableTypes\n    TypeChoices = TypeChoices\n\n    @classmethod\n    @property\n    def config_exception(cls):\n        \"\"\"Returns the AnalyzerConfigurationException class.\"\"\"\n        return AnalyzerConfigurationException\n\n    @property\n    def analyzer_name(self) -&gt; str:\n        \"\"\"Returns the name of the analyzer.\"\"\"\n        return self._config.name\n\n    @classmethod\n    @property\n    def report_model(cls):\n        \"\"\"Returns the AnalyzerReport model.\"\"\"\n        return AnalyzerReport\n\n    @classmethod\n    @property\n    def config_model(cls):\n        \"\"\"Returns the AnalyzerConfig model.\"\"\"\n        return AnalyzerConfig\n\n    def get_exceptions_to_catch(self):\n        \"\"\"\n        Returns additional exceptions to catch when running *start* fn\n        \"\"\"\n        return (\n            AnalyzerConfigurationException,\n            AnalyzerRunException,\n        )\n\n    def _validate_result(self, result, level=0, max_recursion=190):\n        \"\"\"\n        function to validate result, allowing to store inside postgres without errors.\n\n        If the character \\u0000 is present in the string, postgres will throw an error\n\n        If an integer is bigger than max_int,\n        Mongodb is not capable to store and will throw an error.\n\n        If we have more than 200 recursion levels, every encoding\n        will throw a maximum_nested_object exception\n        \"\"\"\n        if level == max_recursion:\n            logger.info(\n                f\"We have reached max_recursion {max_recursion} level. \"\n                f\"The following object will be pruned {result} \"\n            )\n            return None\n        if isinstance(result, dict):\n            for key, values in result.items():\n                result[key] = self._validate_result(\n                    values, level=level + 1, max_recursion=max_recursion\n                )\n        elif isinstance(result, list):\n            for i, _ in enumerate(result):\n                result[i] = self._validate_result(\n                    result[i], level=level + 1, max_recursion=max_recursion\n                )\n        elif isinstance(result, str):\n            return result.replace(\"\\u0000\", \"\")\n        elif isinstance(result, int) and result &gt; 9223372036854775807:  # max int 8bytes\n            result = 9223372036854775807\n        return result\n\n    def after_run_success(self, content):\n        \"\"\"\n        Handles actions after a successful run.\n\n        Args:\n            content (any): The content to process after a successful run.\n        \"\"\"\n        super().after_run_success(self._validate_result(content, max_recursion=15))\n</code></pre>"},{"location":"Guide-docstrings/#docs.Submodules.IntelOwl.api_app.analyzers_manager.classes.BaseAnalyzerMixin.analyzer_name","title":"<code>analyzer_name: str</code>  <code>property</code>","text":"<p>Returns the name of the analyzer.</p>"},{"location":"Guide-docstrings/#docs.Submodules.IntelOwl.api_app.analyzers_manager.classes.BaseAnalyzerMixin.config_exception","title":"<code>config_exception</code>  <code>classmethod</code> <code>property</code>","text":"<p>Returns the AnalyzerConfigurationException class.</p>"},{"location":"Guide-docstrings/#docs.Submodules.IntelOwl.api_app.analyzers_manager.classes.BaseAnalyzerMixin.config_model","title":"<code>config_model</code>  <code>classmethod</code> <code>property</code>","text":"<p>Returns the AnalyzerConfig model.</p>"},{"location":"Guide-docstrings/#docs.Submodules.IntelOwl.api_app.analyzers_manager.classes.BaseAnalyzerMixin.report_model","title":"<code>report_model</code>  <code>classmethod</code> <code>property</code>","text":"<p>Returns the AnalyzerReport model.</p>"},{"location":"Guide-docstrings/#docs.Submodules.IntelOwl.api_app.analyzers_manager.classes.BaseAnalyzerMixin.after_run_success","title":"<code>after_run_success(content)</code>","text":"<p>Handles actions after a successful run.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>any</code> <p>The content to process after a successful run.</p> required Source code in <code>docs/Submodules/IntelOwl/api_app/analyzers_manager/classes.py</code> <pre><code>def after_run_success(self, content):\n    \"\"\"\n    Handles actions after a successful run.\n\n    Args:\n        content (any): The content to process after a successful run.\n    \"\"\"\n    super().after_run_success(self._validate_result(content, max_recursion=15))\n</code></pre>"},{"location":"Guide-docstrings/#docs.Submodules.IntelOwl.api_app.analyzers_manager.classes.BaseAnalyzerMixin.get_exceptions_to_catch","title":"<code>get_exceptions_to_catch()</code>","text":"<p>Returns additional exceptions to catch when running start fn</p> Source code in <code>docs/Submodules/IntelOwl/api_app/analyzers_manager/classes.py</code> <pre><code>def get_exceptions_to_catch(self):\n    \"\"\"\n    Returns additional exceptions to catch when running *start* fn\n    \"\"\"\n    return (\n        AnalyzerConfigurationException,\n        AnalyzerRunException,\n    )\n</code></pre>"},{"location":"Guide-documentation/","title":"Setting Up the New Documentation Site Locally","text":"<p>To set up and run the documentation site on your local machine, please follow the steps below:</p>"},{"location":"Guide-documentation/#1-create-a-virtual-environment","title":"1. Create a Virtual Environment","text":"<p>To create a virtual environment named <code>venv</code> in your project directory, use the following command:</p> <pre><code>python3 -m venv venv\n</code></pre>"},{"location":"Guide-documentation/#2-activate-the-virtual-environment","title":"2. Activate the Virtual Environment","text":"<p>Activate the virtual environment to ensure that all dependencies are installed locally within your project directory.</p> <p>On Linux/MacOS:</p> <pre><code>source venv/bin/activate\n</code></pre> <p>On Windows:</p> <pre><code>venv\\Scripts\\activate\n</code></pre>"},{"location":"Guide-documentation/#3-install-dependencies","title":"3. Install Dependencies","text":"<p>To install all the necessary Python packages listed in requirements.txt, run:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Please run these commands to update and fetch the local Submodules.</p> <pre><code>git submodule foreach --recursive 'git fetch --all'\ngit submodule update --init --remote --recursive --depth 1\ngit submodule sync --recursive\ngit submodule update --remote --recursive\n</code></pre>"},{"location":"Guide-documentation/#4-serve-the-documentation-locally","title":"4. Serve the Documentation Locally","text":"<p>Start a local development server to preview the documentation in your web browser. The server will automatically reload whenever you make changes to the documentation files.</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"Guide-documentation/#5-make-changes-and-review","title":"5. Make Changes and Review","text":"<p>As you edit the documentation, you can view your changes in real-time through the local server. This step ensures everything looks as expected before deploying.</p>"},{"location":"Guide-documentation/#6-push-changes-to-github","title":"6. Push Changes to GitHub","text":"<p>Once you are satisfied with your changes, commit and push them to the GitHub repository. The documentation will be automatically deployed via GitHub Actions, making it live on the documentation site.</p>"},{"location":"Submodules/","title":"Submodules","text":""},{"location":"Submodules/#implementing-docstrings-in-intelowl-documentation","title":"Implementing Docstrings in IntelOwl Documentation","text":"<p>In the IntelOwl documentation site, we use Git submodules to manage multiple repositories as child repositories. This allows us to fetch updated code (including docstrings and API specs) automatically, reducing redundant work for developers.</p>"},{"location":"Submodules/#current-submodules","title":"Current Submodules","text":"<p>There are four submodules under the IntelOwlProject:</p> <ol> <li>IntelOwl</li> <li>GreedyBear</li> <li>pyintelowl</li> <li>GoIntelOwl</li> </ol> <p>These submodules are updated whenever we push new changes to our documentation site, here's the Github Action file.</p>"},{"location":"Submodules/#making-changes-to-documentation","title":"Making Changes to Documentation","text":"<p>When you make changes to the IntelOwl codebase, it typically does not update automatically in the github repository of documentation site.</p> <p>While development if you want to update the submodules to latest changes you can do the following:</p> <pre><code>git submodule foreach --recursive 'git fetch --all'\ngit submodule update --init --remote --recursive --depth 1\ngit submodule sync --recursive\ngit submodule update --remote --recursive\n</code></pre> <p>However, if you need to test changes immediately, you can do the following:</p>"},{"location":"Submodules/#add-custom-submodules-for-testing","title":"Add Custom Submodules for Testing:","text":"<p>Point the submodule in <code>.gitmodules</code> to your fork of the repository to check the updates instantly.</p>"},{"location":"Submodules/#update-submodules","title":"Update Submodules:","text":"<p>After modifying <code>.gitmodules</code>, run the following command to fetch the latest changes:</p> <pre><code>git submodule update --remote --merge\n</code></pre> <p>This ensures that your documentation reflects the most recent code changes.</p>"},{"location":"GoIntelOwl/","title":"index","text":"<p> Go-IntelOwl Repository</p>"},{"location":"GoIntelOwl/#go-intelowl","title":"go-intelowl","text":"<p> go-intelowl is a client library/SDK that allows developers to easily automate and integrate IntelOwl with their own set of tools!</p>"},{"location":"GoIntelOwl/#table-of-contents","title":"Table of Contents","text":"<ul> <li>go-intelowl</li> <li>Getting Started<ul> <li>Pre requisites</li> <li>Installation</li> <li>Usage</li> <li>Examples</li> </ul> </li> <li>Contribute</li> <li>License</li> <li>Links</li> <li>FAQ<ul> <li>Generate API key<ul> <li>v4.0 and above</li> <li>v4.0 below</li> </ul> </li> </ul> </li> </ul>"},{"location":"GoIntelOwl/#getting-started","title":"Getting Started","text":""},{"location":"GoIntelOwl/#pre-requisites","title":"Pre requisites","text":"<ul> <li>Go 1.17+</li> </ul>"},{"location":"GoIntelOwl/#installation","title":"Installation","text":"<p>Use go get to retrieve the SDK to add it to your GOPATH workspace, or project's Go module dependencies.</p> <pre><code>$ go get github.com/intelowlproject/go-intelowl\n</code></pre>"},{"location":"GoIntelOwl/#usage","title":"Usage","text":"<p>This library was built with ease of use in mind! Here are some quick examples to get you started. If you need more example you can go to the examples directory</p> <p>To start using the go-intelowl library you first need to import it:</p> <pre><code>import \"github.com/intelowlproject/go-intelowl/gointelowl\"\n</code></pre> <p>Construct a new <code>IntelOwlClient</code>, then use the various services to easily access different parts of Intelowl's REST API. Here's an example of getting all jobs:</p> <pre><code>clientOptions := gointelowl.IntelOwlClientOptions{\n    Url:         \"your-cool-URL-goes-here\",\n    Token:       \"your-super-secret-token-goes-here\",\n    // This is optional\n    Certificate: \"your-optional-certificate-goes-here\",\n}\n\nintelowl := gointelowl.NewIntelOwlClient(\n    &amp;clientOptions,\n    nil\n)\n\nctx := context.Background()\n\n// returns *[]Jobs or an IntelOwlError!\njobs, err := intelowl.JobService.List(ctx)\n</code></pre> <p>For easy configuration and set up we opted for <code>options</code> structs. Where we can customize the client API or service endpoint to our liking! For more information go here. Here's a quick example!</p> <pre><code>// ...Making the client and context!\n\ntagOptions = gointelowl.TagParams{\n  Label: \"NEW TAG\",\n  Color: \"#ffb703\",\n}\n\ncreatedTag, err := intelowl.TagService.Create(ctx, tagOptions)\nif err != nil {\n    fmt.Println(err)\n} else {\n    fmt.Println(createdTag)\n}\n</code></pre>"},{"location":"GoIntelOwl/#examples","title":"Examples","text":"<p>The examples directory contains a couple for clear examples, of which one is partially listed here as well:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n\n    \"github.com/intelowlproject/go-intelowl/gointelowl\"\n)\n\nfunc main(){\n    intelowlOptions := gointelowl.IntelOwlClientOptions{\n        Url:         \"your-cool-url-goes-here\",\n        Token:       \"your-super-secret-token-goes-here\",\n        Certificate: \"your-optional-certificate-goes-here\",\n    }\n\n    client := gointelowl.NewIntelOwlClient(\n        &amp;intelowlOptions,\n        nil,\n    )\n\n    ctx := context.Background()\n\n    // Get User details!\n    user, err := client.UserService.Access(ctx)\n    if err != nil {\n        fmt.Println(\"err\")\n        fmt.Println(err)\n    } else {\n        fmt.Println(\"USER Details\")\n        fmt.Println(*user)\n    }\n}\n</code></pre> <p>For complete usage of go-intelowl, see the full package docs.</p>"},{"location":"GoIntelOwl/#contribute","title":"Contribute","text":"<p>If you want to follow the updates, discuss, contribute, or just chat then please join our slack channel we'd love to hear your feedback!</p>"},{"location":"GoIntelOwl/#license","title":"License","text":"<p>Licensed under the GNU AFFERO GENERAL PUBLIC LICENSE.</p>"},{"location":"GoIntelOwl/#links","title":"Links","text":"<ul> <li>Intelowl</li> <li>Documentation</li> <li>API documentation</li> <li>Examples</li> </ul>"},{"location":"GoIntelOwl/#faq","title":"FAQ","text":""},{"location":"GoIntelOwl/#generate-api-key","title":"Generate API key","text":"<p>You need a valid API key to interact with the IntelOwl server.</p>"},{"location":"GoIntelOwl/#v40-and-above","title":"v4.0 and above","text":"<p>You can get an API by doing the following:</p> <ol> <li>Log / Signin into intelowl</li> <li>At the upper right click on your profile from the drop down select <code>API Access/ Sessions</code></li> <li>Then generate an API key or see it!</li> </ol>"},{"location":"GoIntelOwl/#v40-below","title":"v4.0 below","text":"<p>Keys should be created from the admin interface of IntelOwl: you have to go in the Durin section (click on <code>Auth tokens</code>) and generate a key there.</p>"},{"location":"GreedyBear/Api-docs/","title":"API Documentation","text":""},{"location":"GreedyBear/Api-docs/#enrichment","title":"<code>enrichment</code>","text":"<p>Handle enrichment requests for a specific observable (domain or IP address).</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object containing query parameters.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>A JSON response indicating whether the observable was found,</p> <p>and if so, the corresponding IOC.</p> Source code in <code>docs/Submodules/GreedyBear/api/views.py</code> <pre><code>@api_view([GET])\n@authentication_classes([CookieTokenAuthentication])\n@permission_classes([IsAuthenticated])\ndef enrichment_view(request):\n    \"\"\"\n    Handle enrichment requests for a specific observable (domain or IP address).\n\n    Args:\n        request: The incoming request object containing query parameters.\n\n    Returns:\n        Response: A JSON response indicating whether the observable was found,\n        and if so, the corresponding IOC.\n    \"\"\"\n    observable_name = request.query_params.get(\"query\")\n    logger.info(f\"Enrichment view requested for: {str(observable_name)}\")\n    serializer = EnrichmentSerializer(data=request.query_params, context={\"request\": request})\n    serializer.is_valid(raise_exception=True)\n\n    source_ip = str(request.META[\"REMOTE_ADDR\"])\n    request_source = Statistics(source=source_ip, view=viewType.ENRICHMENT_VIEW.value)\n    request_source.save()\n\n    return Response(serializer.data, status=status.HTTP_200_OK)\n</code></pre>"},{"location":"GreedyBear/Api-docs/#feeds","title":"<code>feeds</code>","text":"<p>Handle requests for IOC feeds with specific parameters and format the response accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object.</p> required <code>feed_type</code> <code>str</code> <p>Type of feed (e.g., log4j, cowrie, etc.).</p> required <code>attack_type</code> <code>str</code> <p>Type of attack (e.g., all, specific attack types).</p> required <code>age</code> <code>str</code> <p>Age of the data to filter (e.g., recent, persistent).</p> required <code>format_</code> <code>str</code> <p>Desired format of the response (e.g., json, csv, txt).</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>The HTTP response with formatted IOC data.</p> Source code in <code>docs/Submodules/GreedyBear/api/views.py</code> <pre><code>@api_view([GET])\ndef feeds(request, feed_type, attack_type, age, format_):\n    \"\"\"\n    Handle requests for IOC feeds with specific parameters and format the response accordingly.\n\n    Args:\n        request: The incoming request object.\n        feed_type (str): Type of feed (e.g., log4j, cowrie, etc.).\n        attack_type (str): Type of attack (e.g., all, specific attack types).\n        age (str): Age of the data to filter (e.g., recent, persistent).\n        format_ (str): Desired format of the response (e.g., json, csv, txt).\n\n    Returns:\n        Response: The HTTP response with formatted IOC data.\n    \"\"\"\n    logger.info(f\"request /api/feeds with params: feed type: {feed_type}, \" f\"attack_type: {attack_type}, Age: {age}, format: {format_}\")\n\n    iocs_queryset = get_queryset(request, feed_type, attack_type, age, format_)\n    return feeds_response(request, iocs_queryset, feed_type, format_)\n</code></pre>"},{"location":"GreedyBear/Api-docs/#feeds_pagination","title":"<code>feeds_pagination</code>","text":"<p>Handle requests for paginated IOC feeds based on query parameters.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>The paginated HTTP response with IOC data.</p> Source code in <code>docs/Submodules/GreedyBear/api/views.py</code> <pre><code>@api_view([GET])\ndef feeds_pagination(request):\n    \"\"\"\n    Handle requests for paginated IOC feeds based on query parameters.\n\n    Args:\n        request: The incoming request object.\n\n    Returns:\n        Response: The paginated HTTP response with IOC data.\n    \"\"\"\n    params = request.query_params\n    logger.info(f\"request /api/feeds with params: {params}\")\n\n    paginator = CustomPageNumberPagination()\n    iocs_queryset = get_queryset(\n        request,\n        params[\"feed_type\"],\n        params[\"attack_type\"],\n        params[\"age\"],\n        \"json\",\n    )\n    iocs = paginator.paginate_queryset(iocs_queryset, request)\n    resp_data = feeds_response(request, iocs, params[\"feed_type\"], \"json\", dict_only=True)\n    return paginator.get_paginated_response(resp_data)\n</code></pre>"},{"location":"GreedyBear/Api-docs/#statistics","title":"<code>Statistics</code>","text":"<p>               Bases: <code>ViewSet</code></p> <p>A viewset for viewing and editing statistics related to feeds and enrichment data.</p> <p>Provides actions to retrieve statistics about the sources and downloads of feeds, as well as statistics on enrichment data.</p> Source code in <code>docs/Submodules/GreedyBear/api/views.py</code> <pre><code>class StatisticsViewSet(viewsets.ViewSet):\n    \"\"\"\n    A viewset for viewing and editing statistics related to feeds and enrichment data.\n\n    Provides actions to retrieve statistics about the sources and downloads of feeds,\n    as well as statistics on enrichment data.\n    \"\"\"\n\n    @action(detail=True, methods=[\"GET\"])\n    def feeds(self, request, pk=None):\n        \"\"\"\n        Retrieve feed statistics, including the number of sources and downloads.\n\n        Args:\n            request: The incoming request object.\n            pk (str): The type of statistics to retrieve (e.g., \"sources\", \"downloads\").\n\n        Returns:\n            Response: A JSON response containing the requested statistics.\n        \"\"\"\n        if pk == \"sources\":\n            annotations = {\n                \"Sources\": Count(\n                    \"source\",\n                    distinct=True,\n                    filter=Q(view=viewType.FEEDS_VIEW.value),\n                )\n            }\n        elif pk == \"downloads\":\n            annotations = {\"Downloads\": Count(\"source\", filter=Q(view=viewType.FEEDS_VIEW.value))}\n        else:\n            logger.error(\"this is impossible. check the code\")\n            return HttpResponseServerError()\n        return self.__aggregation_response_static_statistics(annotations)\n\n    @action(detail=True, methods=[\"get\"])\n    def enrichment(self, request, pk=None):\n        \"\"\"\n        Retrieve enrichment statistics, including the number of sources and requests.\n\n        Args:\n            request: The incoming request object.\n            pk (str): The type of statistics to retrieve (e.g., \"sources\", \"requests\").\n\n        Returns:\n            Response: A JSON response containing the requested statistics.\n        \"\"\"\n        if pk == \"sources\":\n            annotations = {\n                \"Sources\": Count(\n                    \"source\",\n                    distinct=True,\n                    filter=Q(view=viewType.ENRICHMENT_VIEW.value),\n                )\n            }\n        elif pk == \"requests\":\n            annotations = {\"Requests\": Count(\"source\", filter=Q(view=viewType.ENRICHMENT_VIEW.value))}\n        else:\n            logger.error(\"this is impossible. check the code\")\n            return HttpResponseServerError()\n        return self.__aggregation_response_static_statistics(annotations)\n\n    @action(detail=False, methods=[\"get\"])\n    def feeds_types(self, request):\n        \"\"\"\n        Retrieve statistics for different types of feeds, including Log4j, Cowrie,\n        and general honeypots.\n\n        Args:\n            request: The incoming request object.\n\n        Returns:\n            Response: A JSON response containing the feed type statistics.\n        \"\"\"\n        # FEEDS\n        annotations = {\n            \"Log4j\": Count(\"name\", distinct=True, filter=Q(log4j=True)),\n            \"Cowrie\": Count(\"name\", distinct=True, filter=Q(cowrie=True)),\n        }\n        # feed_type for each general honeypot in the list\n        generalHoneypots = GeneralHoneypot.objects.all().filter(active=True)\n        for hp in generalHoneypots:\n            annotations[hp.name] = Count(\"name\", Q(general_honeypot__name__iexact=hp.name.lower()))\n        return self.__aggregation_response_static_ioc(annotations)\n\n    def __aggregation_response_static_statistics(self, annotations: dict) -&gt; Response:\n        \"\"\"\n        Helper method to generate statistics response based on annotations.\n\n        Args:\n            annotations (dict): Dictionary containing the annotations for the query.\n\n        Returns:\n            Response: A JSON response containing the aggregated statistics.\n        \"\"\"\n        delta, basis = self.__parse_range(self.request)\n        qs = Statistics.objects.filter(request_date__gte=delta).annotate(date=Trunc(\"request_date\", basis)).values(\"date\").annotate(**annotations)\n        return Response(qs)\n\n    def __aggregation_response_static_ioc(self, annotations: dict) -&gt; Response:\n        \"\"\"\n        Helper method to generate IOC response based on annotations.\n\n        Args:\n            annotations (dict): Dictionary containing the annotations for the query.\n\n        Returns:\n            Response: A JSON response containing the aggregated IOC data.\n        \"\"\"\n        delta, basis = self.__parse_range(self.request)\n\n        qs = (\n            IOC.objects.filter(last_seen__gte=delta)\n            .exclude(general_honeypot__active=False)\n            .annotate(date=Trunc(\"last_seen\", basis))\n            .values(\"date\")\n            .annotate(**annotations)\n        )\n        return Response(qs)\n\n    @staticmethod\n    def __parse_range(request):\n        \"\"\"\n        Parse the range parameter from the request query string to determine the time range for the query.\n\n        Args:\n            request: The incoming request object.\n\n        Returns:\n            tuple: A tuple containing the delta time and basis for the query range.\n        \"\"\"\n        try:\n            range_str = request.GET[\"range\"]\n        except KeyError:\n            # default\n            range_str = \"7d\"\n\n        return parse_humanized_range(range_str)\n</code></pre>"},{"location":"GreedyBear/Api-docs/#docs.Submodules.GreedyBear.api.views.StatisticsViewSet.__aggregation_response_static_ioc","title":"<code>__aggregation_response_static_ioc(annotations)</code>","text":"<p>Helper method to generate IOC response based on annotations.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>dict</code> <p>Dictionary containing the annotations for the query.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>A JSON response containing the aggregated IOC data.</p> Source code in <code>docs/Submodules/GreedyBear/api/views.py</code> <pre><code>def __aggregation_response_static_ioc(self, annotations: dict) -&gt; Response:\n    \"\"\"\n    Helper method to generate IOC response based on annotations.\n\n    Args:\n        annotations (dict): Dictionary containing the annotations for the query.\n\n    Returns:\n        Response: A JSON response containing the aggregated IOC data.\n    \"\"\"\n    delta, basis = self.__parse_range(self.request)\n\n    qs = (\n        IOC.objects.filter(last_seen__gte=delta)\n        .exclude(general_honeypot__active=False)\n        .annotate(date=Trunc(\"last_seen\", basis))\n        .values(\"date\")\n        .annotate(**annotations)\n    )\n    return Response(qs)\n</code></pre>"},{"location":"GreedyBear/Api-docs/#docs.Submodules.GreedyBear.api.views.StatisticsViewSet.__aggregation_response_static_statistics","title":"<code>__aggregation_response_static_statistics(annotations)</code>","text":"<p>Helper method to generate statistics response based on annotations.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>dict</code> <p>Dictionary containing the annotations for the query.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>A JSON response containing the aggregated statistics.</p> Source code in <code>docs/Submodules/GreedyBear/api/views.py</code> <pre><code>def __aggregation_response_static_statistics(self, annotations: dict) -&gt; Response:\n    \"\"\"\n    Helper method to generate statistics response based on annotations.\n\n    Args:\n        annotations (dict): Dictionary containing the annotations for the query.\n\n    Returns:\n        Response: A JSON response containing the aggregated statistics.\n    \"\"\"\n    delta, basis = self.__parse_range(self.request)\n    qs = Statistics.objects.filter(request_date__gte=delta).annotate(date=Trunc(\"request_date\", basis)).values(\"date\").annotate(**annotations)\n    return Response(qs)\n</code></pre>"},{"location":"GreedyBear/Api-docs/#docs.Submodules.GreedyBear.api.views.StatisticsViewSet.__parse_range","title":"<code>__parse_range(request)</code>  <code>staticmethod</code>","text":"<p>Parse the range parameter from the request query string to determine the time range for the query.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the delta time and basis for the query range.</p> Source code in <code>docs/Submodules/GreedyBear/api/views.py</code> <pre><code>@staticmethod\ndef __parse_range(request):\n    \"\"\"\n    Parse the range parameter from the request query string to determine the time range for the query.\n\n    Args:\n        request: The incoming request object.\n\n    Returns:\n        tuple: A tuple containing the delta time and basis for the query range.\n    \"\"\"\n    try:\n        range_str = request.GET[\"range\"]\n    except KeyError:\n        # default\n        range_str = \"7d\"\n\n    return parse_humanized_range(range_str)\n</code></pre>"},{"location":"GreedyBear/Api-docs/#docs.Submodules.GreedyBear.api.views.StatisticsViewSet.enrichment","title":"<code>enrichment(request, pk=None)</code>","text":"<p>Retrieve enrichment statistics, including the number of sources and requests.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object.</p> required <code>pk</code> <code>str</code> <p>The type of statistics to retrieve (e.g., \"sources\", \"requests\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>A JSON response containing the requested statistics.</p> Source code in <code>docs/Submodules/GreedyBear/api/views.py</code> <pre><code>@action(detail=True, methods=[\"get\"])\ndef enrichment(self, request, pk=None):\n    \"\"\"\n    Retrieve enrichment statistics, including the number of sources and requests.\n\n    Args:\n        request: The incoming request object.\n        pk (str): The type of statistics to retrieve (e.g., \"sources\", \"requests\").\n\n    Returns:\n        Response: A JSON response containing the requested statistics.\n    \"\"\"\n    if pk == \"sources\":\n        annotations = {\n            \"Sources\": Count(\n                \"source\",\n                distinct=True,\n                filter=Q(view=viewType.ENRICHMENT_VIEW.value),\n            )\n        }\n    elif pk == \"requests\":\n        annotations = {\"Requests\": Count(\"source\", filter=Q(view=viewType.ENRICHMENT_VIEW.value))}\n    else:\n        logger.error(\"this is impossible. check the code\")\n        return HttpResponseServerError()\n    return self.__aggregation_response_static_statistics(annotations)\n</code></pre>"},{"location":"GreedyBear/Api-docs/#docs.Submodules.GreedyBear.api.views.StatisticsViewSet.feeds","title":"<code>feeds(request, pk=None)</code>","text":"<p>Retrieve feed statistics, including the number of sources and downloads.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object.</p> required <code>pk</code> <code>str</code> <p>The type of statistics to retrieve (e.g., \"sources\", \"downloads\").</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>A JSON response containing the requested statistics.</p> Source code in <code>docs/Submodules/GreedyBear/api/views.py</code> <pre><code>@action(detail=True, methods=[\"GET\"])\ndef feeds(self, request, pk=None):\n    \"\"\"\n    Retrieve feed statistics, including the number of sources and downloads.\n\n    Args:\n        request: The incoming request object.\n        pk (str): The type of statistics to retrieve (e.g., \"sources\", \"downloads\").\n\n    Returns:\n        Response: A JSON response containing the requested statistics.\n    \"\"\"\n    if pk == \"sources\":\n        annotations = {\n            \"Sources\": Count(\n                \"source\",\n                distinct=True,\n                filter=Q(view=viewType.FEEDS_VIEW.value),\n            )\n        }\n    elif pk == \"downloads\":\n        annotations = {\"Downloads\": Count(\"source\", filter=Q(view=viewType.FEEDS_VIEW.value))}\n    else:\n        logger.error(\"this is impossible. check the code\")\n        return HttpResponseServerError()\n    return self.__aggregation_response_static_statistics(annotations)\n</code></pre>"},{"location":"GreedyBear/Api-docs/#docs.Submodules.GreedyBear.api.views.StatisticsViewSet.feeds_types","title":"<code>feeds_types(request)</code>","text":"<p>Retrieve statistics for different types of feeds, including Log4j, Cowrie, and general honeypots.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>A JSON response containing the feed type statistics.</p> Source code in <code>docs/Submodules/GreedyBear/api/views.py</code> <pre><code>@action(detail=False, methods=[\"get\"])\ndef feeds_types(self, request):\n    \"\"\"\n    Retrieve statistics for different types of feeds, including Log4j, Cowrie,\n    and general honeypots.\n\n    Args:\n        request: The incoming request object.\n\n    Returns:\n        Response: A JSON response containing the feed type statistics.\n    \"\"\"\n    # FEEDS\n    annotations = {\n        \"Log4j\": Count(\"name\", distinct=True, filter=Q(log4j=True)),\n        \"Cowrie\": Count(\"name\", distinct=True, filter=Q(cowrie=True)),\n    }\n    # feed_type for each general honeypot in the list\n    generalHoneypots = GeneralHoneypot.objects.all().filter(active=True)\n    for hp in generalHoneypots:\n        annotations[hp.name] = Count(\"name\", Q(general_honeypot__name__iexact=hp.name.lower()))\n    return self.__aggregation_response_static_ioc(annotations)\n</code></pre>"},{"location":"GreedyBear/Api-docs/#general_honeypot_list","title":"<code>general_honeypot_list</code>","text":"<p>Retrieve a list of all general honeypots, optionally filtering by active status.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The incoming request object containing query parameters.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>A JSON response containing the list of general honeypots.</p> Source code in <code>docs/Submodules/GreedyBear/api/views.py</code> <pre><code>@api_view([GET])\ndef general_honeypot_list(request):\n    \"\"\"\n    Retrieve a list of all general honeypots, optionally filtering by active status.\n\n    Args:\n        request: The incoming request object containing query parameters.\n\n    Returns:\n        Response: A JSON response containing the list of general honeypots.\n    \"\"\"\n\n    logger.info(f\"Requested general honeypots list from {request.user}.\")\n    active = request.query_params.get(\"onlyActive\")\n    honeypots = []\n    generalHoneypots = GeneralHoneypot.objects.all()\n    if active == \"true\":\n        generalHoneypots = generalHoneypots.filter(active=True)\n        logger.info(\"Requested only active general honeypots\")\n    honeypots.extend([hp.name for hp in generalHoneypots])\n\n    logger.info(f\"General honeypots: {honeypots}\")\n    return Response(honeypots)\n</code></pre>"},{"location":"GreedyBear/Contribute/","title":"Contribute","text":""},{"location":"GreedyBear/Contribute/#general-guidance","title":"General Guidance","text":"<p>Please refer to IntelOwl Documentation for everything missing here.</p>"},{"location":"GreedyBear/Contribute/#rules","title":"Rules","text":"<p>GreedyBear welcomes contributors from anywhere and from any kind of education or skill level. We strive to create a community of developers that is welcoming, friendly and right.</p> <p>For this reason it is important to follow some easy rules based on a simple but important concept: Respect.</p> <ul> <li>Before starting to work on an issue, you need to get the approval of one of the maintainers. Therefore please ask to be assigned to an issue. If you do not that but you still raise a PR for that issue, your PR can be rejected. This is a form of respect for both the maintainers and the other contributors who could have already started to work on the same problem.</li> </ul> <ul> <li>When you ask to be assigned to an issue, it means that you are ready to work on it. When you get assigned, take the lock and then you disappear, you are not respecting the maintainers and the other contributors who could be able to work on that. So, after having been assigned, you have a week of time to deliver your first draft PR. After that time has passed without any notice, you will be unassigned.</li> </ul> <ul> <li>Before asking questions regarding how the project works, please read through all the documentation and install the project on your own local machine to try it and understand how it basically works. This is a form of respect to the maintainers.</li> </ul> <ul> <li>Once you started working on an issue and you have some work to share and discuss with us, please raise a draft PR early with incomplete changes. This way you can continue working on the same and we can track your progress and actively review and help. This is a form of respect to you and to the maintainers.</li> </ul> <ul> <li>When creating a PR, please read through the sections that you will find in the PR template and compile it appropriately. If you do not, your PR can be rejected. This is a form of respect to the maintainers.</li> </ul>"},{"location":"GreedyBear/Contribute/#code-style","title":"Code Style","text":"<p>Keeping to a consistent code style throughout the project makes it easier to contribute and collaborate. We make use of <code>psf/black</code> and isort for code formatting and <code>flake8</code> for style guides.</p>"},{"location":"GreedyBear/Contribute/#how-to-start-setup-project-and-development-instance","title":"How to start (Setup project and development instance)","text":"<p>To start with the development setup, make sure you go through all the steps in Installation Guide and properly installed it.</p> <p>Please create a new branch based on the develop branch that contains the most recent changes. This is mandatory.</p> <p><code>git checkout -b myfeature develop</code></p> <p>Then we strongly suggest to configure pre-commit to force linters on every commits you perform:</p> <pre><code># create virtualenv to host pre-commit installation\npython3 -m venv venv\nsource venv/bin/activate\n# from the project base directory\npip install pre-commit\npre-commit install -c .github/.pre-commit-config.yaml\n</code></pre> <p>Remember that whenever you make changes, you need to rebuild the docker image to see the reflected changes.</p>"},{"location":"GreedyBear/Contribute/#note-about-documentation","title":"NOTE about documentation:","text":"<p>If you made any changes to an existing model/serializer/view, please run the following command to generate a new version of the API schema and docs:</p> <pre><code>docker exec -it greedybear_uwsgi python manage.py spectacular --file docs/source/schema.yml &amp;&amp; make html\n</code></pre>"},{"location":"GreedyBear/Contribute/#frontend","title":"Frontend","text":"<p>To start the frontend in \"develop\" mode, you can execute the startup npm script within the folder <code>frontend</code>:</p> <pre><code>cd frontend/\n# Install\nnpm i\n# Start\nDANGEROUSLY_DISABLE_HOST_CHECK=true npm start\n# See https://create-react-app.dev/docs/proxying-api-requests-in-development/#invalid-host-header-errors-after-configuring-proxy for why we use that flag in development mode\n</code></pre> <p>Most of the time you would need to test the changes you made together with the backend. In that case, you would need to run the backend locally too:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"GreedyBear/Contribute/#certego-ui","title":"Certego-UI","text":"<p>The GreedyBear Frontend is tightly linked to the <code>certego-ui</code> library. Most of the React components are imported from there. Because of this, it may happen that, during development, you would need to work on that library too. To install the <code>certego-ui</code> library, please take a look to npm link and remember to start certego-ui without installing peer dependencies (to avoid conflicts with GreedyBear dependencies):</p> <pre><code>git clone https://github.com/certego/certego-ui.git\n# change directory to the folder where you have the cloned the library\ncd certego-ui/\n# install, without peer deps (to use packages of GreedyBear)\nnpm i --legacy-peer-deps\n# create link to the project (this will globally install this package)\nsudo npm link\n# compile the library\nnpm start\n</code></pre> <p>Then, open another command line tab, create a link in the <code>frontend</code> to the <code>certego-ui</code> and re-install and re-start the frontend application (see previous section):</p> <pre><code>cd frontend/\nnpm link @certego/certego-ui\n</code></pre> <p>This trick will allow you to see reflected every changes you make in the <code>certego-ui</code> directly in the running <code>frontend</code> application.</p>"},{"location":"GreedyBear/Contribute/#example-application","title":"Example application","text":"<p>The <code>certego-ui</code> application comes with an example project that showcases the components that you can re-use and import to other projects, like GreedyBear:</p> <pre><code># To have the Example application working correctly, be sure to have installed `certego-ui` *without* the `--legacy-peer-deps` option and having it started in another command line\ncd certego-ui/\nnpm i\nnpm start\n# go to another tab\ncd certego-ui/example/\nnpm i\nnpm start\n</code></pre>"},{"location":"GreedyBear/Contribute/#create-a-pull-request","title":"Create a pull request","text":""},{"location":"GreedyBear/Contribute/#remember","title":"Remember!!!","text":"<p>Please create pull requests only for the branch develop. That code will be pushed to master only on a new release.</p> <p>Also remember to pull the most recent changes available in the develop branch before submitting your PR. If your PR has merge conflicts caused by this behavior, it won't be accepted.</p>"},{"location":"GreedyBear/Contribute/#tests","title":"Tests","text":""},{"location":"GreedyBear/Contribute/#backend","title":"Backend","text":""},{"location":"GreedyBear/Contribute/#install-testing-requirements","title":"Install testing requirements","text":"<p>You have to install <code>pre-commit</code> to have your code adjusted and fixed with the available linters:</p> <pre><code>pip install pre-commit\npre-commit install -c .github/.pre-commit-config.yaml\n</code></pre> <p>Once done that, you won't have to think about linters anymore.</p>"},{"location":"GreedyBear/Contribute/#run-all-tests","title":"Run all tests","text":"<pre><code>docker exec greedybear_uwsgi python3 manage.py test\n</code></pre>"},{"location":"GreedyBear/Contribute/#frontend_1","title":"Frontend","text":"<p>All the frontend tests must be run from the folder <code>frontend</code>. The tests can contain log messages, you can suppress then with the environment variable <code>SUPPRESS_JEST_LOG=True</code>.</p>"},{"location":"GreedyBear/Contribute/#run-all-tests_1","title":"Run all tests","text":"<pre><code>npm test\n</code></pre>"},{"location":"GreedyBear/Contribute/#run-a-specific-component-tests","title":"Run a specific component tests","text":"<pre><code>npm test -- -t &lt;componentPath&gt;\n// example\nnpm test tests/components/auth/Login.test.jsx\n</code></pre>"},{"location":"GreedyBear/Contribute/#run-a-specific-test","title":"Run a specific test","text":"<pre><code>npm test -- -t '&lt;describeString&gt; &lt;testString&gt;'\n// example\nnpm test -- -t \"Login component User login\"\n</code></pre> <p>if you get any errors, fix them. Once you make sure that everything is working fine, please squash all of our commits into a single one and finally create a pull request.</p>"},{"location":"GreedyBear/Installation/","title":"Installation","text":""},{"location":"GreedyBear/Installation/#requirements","title":"Requirements","text":"<p>For requirements, please refer to IntelOwl requirements which are the same</p> <p>Note that GreedyBear needs a running instance of ElasticSearch of a T-POT to function. In <code>docker/env_file</code>, set the variable <code>ELASTIC_ENDPOINT</code> with the URL of your Elasticsearch T-POT.</p> <p>If you don't have one, you can make the following changes to make GreeyBear spin up it's own ElasticSearch instance. (...Care! This option would require enough RAM to run the additional containers. Suggested is &gt;=16GB):</p> <ol> <li>In <code>docker/env_file</code>, set the variable <code>ELASTIC_ENDPOINT</code> to <code>http://elasticsearch:9200</code>.</li> <li>Add <code>:docker/elasticsearch.yml</code> to the last defined <code>COMPOSE_FILE</code> variable or uncomment the <code># local development with elasticsearch container</code> block in <code>.env</code> file.</li> </ol>"},{"location":"GreedyBear/Installation/#installation-steps","title":"Installation steps","text":"<p>Start by cloning the project</p> <pre><code># clone the Greedybear project repository\ngit clone https://github.com/honeynet/GreedyBear\ncd GreedyBear/\n\n# construct environment files from templates\ncp .env_template .env\ncd docker/\ncp env_file_template env_file\ncp env_file_postgres_template env_file_postgres\n</code></pre> <p>Now you can start by building the image using docker-compose and run the project.</p> <pre><code># build the image locally\ndocker-compose build\n\n# start the app\ndocker-compose up\n\n# now the app is running on http://localhost:80\n\n# shut down the application\ndocker-compose down\n</code></pre> <p>Note: To create a superuser run the following:</p> <pre><code>docker exec -ti greedybear_uwsgi python3 manage.py createsuperuser\n</code></pre> <p>The app administrator can enable/disable the extraction of source IPs for specific honeypots from the Django Admin. This is used for honeypots that are not specifically implemented to extract additional information (so not Log4Pot and Cowrie).</p>"},{"location":"GreedyBear/Installation/#environment-configuration","title":"Environment configuration","text":"<p>In the <code>env_file</code>, configure different variables as explained below.</p> <p>Required variable to set:</p> <ul> <li><code>DEFAULT_FROM_EMAIL</code>: email address used for automated correspondence from the site manager (example: <code>noreply@mydomain.com</code>)</li> <li><code>DEFAULT_EMAIL</code>: email address used for correspondence with users (example: <code>info@mydomain.com</code>)</li> <li><code>EMAIL_HOST</code>: the host to use for sending email with SMTP</li> <li><code>EMAIL_HOST_USER</code>: username to use for the SMTP server defined in EMAIL_HOST</li> <li><code>EMAIL_HOST_PASSWORD</code>: password to use for the SMTP server defined in EMAIL_HOST. This setting is used in conjunction with EMAIL_HOST_USER when authenticating to the SMTP server.</li> <li><code>EMAIL_PORT</code>: port to use for the SMTP server defined in EMAIL_HOST.</li> <li><code>EMAIL_USE_TLS</code>: whether to use an explicit TLS (secure) connection when talking to the SMTP server, generally used on port 587.</li> <li><code>EMAIL_USE_SSL</code>: whether to use an implicit TLS (secure) connection when talking to the SMTP server, generally used on port 465.</li> </ul> <p>Optional configuration:</p> <ul> <li><code>SLACK_TOKEN</code>: Slack token of your Slack application that will be used to send/receive notifications</li> <li><code>DEFAULT_SLACK_CHANNEL</code>: ID of the Slack channel you want to post the message to</li> </ul>"},{"location":"GreedyBear/Installation/#elasticsearch-compatibility","title":"ElasticSearch compatibility.","text":"<p>Greedybear leverages a python client for interacting with ElasticSearch which requires to be at the exact major version of the related T-POT ElasticSearch instance. This means that there could problems if those versions do not match.</p> <p>The actual version of the client installed is the 8.15.0 which allows to run TPOT version from 22.04.0 to 24.04.0 without any problems (and some later ones...we regularly check T-POT releases but we could miss one or two here.)</p> <p>If you want to have compatibility with previous versions, you need to change the <code>elasticsearch-dsl</code> version here and re-build locally the project.</p>"},{"location":"GreedyBear/Installation/#update-and-re-build","title":"Update and Re-build","text":""},{"location":"GreedyBear/Installation/#rebuilding-the-project-creating-custom-docker-build","title":"Rebuilding the project / Creating custom docker build","text":"<p>If you make some code changes and you like to rebuild the project, follow these steps:</p> <ol> <li>Be sure that your <code>.env</code> file has a <code>COMPOSE_FILE</code> variable which mounts the <code>docker/local.override.yml</code> compose file.</li> <li><code>docker-compose build</code> to build the new docker image.</li> <li>Start the containers with <code>docker-compose up</code>.</li> </ol>"},{"location":"GreedyBear/Installation/#update-to-the-most-recent-version","title":"Update to the most recent version","text":"<p>To update the project with the most recent available code you have to follow these steps:</p> <pre><code>$ cd &lt;your_greedy_bear_directory&gt; # go into the project directory\n$ git pull # pull new repository changes\n$ docker pull intelowlproject/greedybear:prod # pull new docker images\n$ docker-compose down # stop and destroy the currently running GreedyBear containers\n$ docker-compose up # restart the GreedyBear application\n</code></pre>"},{"location":"GreedyBear/Introduction/","title":"Introduction","text":"<p> GreedyBear Repository</p>"},{"location":"GreedyBear/Introduction/#introduction","title":"Introduction","text":"<p>The project goal is to extract data of the attacks detected by a TPOT or a cluster of them and to generate some feeds that can be used to prevent and detect attacks.</p> <p>Official announcement here.</p>"},{"location":"GreedyBear/Introduction/#public-feeds","title":"Public feeds","text":"<p>There are public feeds provided by The Honeynet Project in this site: greedybear.honeynet.org. Example</p> <p>To check all the available feeds, Please refer to our usage guide</p> <p>Please do not perform too many requests to extract feeds or you will be banned.</p> <p>If you want to be updated regularly, please download the feeds only once every 10 minutes (this is the time between each internal update).</p>"},{"location":"GreedyBear/Usage/","title":"Usage","text":""},{"location":"GreedyBear/Usage/#user-management","title":"User management","text":""},{"location":"GreedyBear/Usage/#registration","title":"Registration","text":"<p>Since Greedybear v1.1.0 we added a Registration Page that can be used to manage Registration requests when providing GreedyBear as a Service.</p> <p>After an user registration, an email is sent to the user to verify their email address. If necessary, there are buttons on the login page to resend the verification email and to reset the password.</p> <p>Once the user has verified their email, they would be manually vetted before being allowed to use the GreedyBear platform. The registration requests would be handled in the Django Admin page by admins. If you have GreedyBear deployed on an AWS instance you can use the SES service.</p> <p>In a development environment the emails that would be sent are written to the standard output.</p>"},{"location":"GreedyBear/Usage/#amazon-ses","title":"Amazon SES","text":"<p>If you like, you could use Amazon SES for sending automated emails.</p> <p>First, you need to configure the environment variable <code>AWS_SES</code> to <code>True</code> to enable it. Then you have to add some credentials for AWS: if you have GreedyBear deployed on the AWS infrastructure, you can use IAM credentials: to allow that just set <code>AWS_IAM_ACCESS</code> to <code>True</code>. If that is not the case, you have to set both <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>.</p> <p>Additionally, if you are not using the default AWS region of us-east-1, you need to specify your <code>AWS_REGION</code>. You can customize the AWS Region location of you services by changing the environment variable <code>AWS_REGION</code>. Default is <code>eu-central-1</code>.</p>"},{"location":"GreedyBear/Usage/#feeds","title":"Feeds","text":"<p>GreedyBear is created with the aim to collect the information from the TPOTs and generate some actionable feeds, so that they can be easily accessible and act as valuable information to prevent and detect attacks.</p> <p>The feeds are reachable through the following URL:</p> <pre><code>https://&lt;greedybear_site&gt;/api/feeds/&lt;feed_type&gt;/&lt;attack_type&gt;/&lt;age&gt;.&lt;format&gt;\n</code></pre> <p>The available feed_type are:</p> <ul> <li><code>log4j</code>: attacks detected from the Log4pot.</li> <li><code>cowrie</code>: attacks detected from the Cowrie Honeypot.</li> <li><code>all</code>: get all types at once</li> <li>The following honeypot feeds exist (for extraction of (only) the source IPs):<ul> <li><code>heralding</code></li> <li><code>ciscoasa</code></li> <li><code>honeytrap</code></li> <li><code>dionaea</code></li> <li><code>conpot</code></li> <li><code>adbhoney</code></li> <li><code>tanner</code></li> <li><code>citrixhoneypot</code></li> <li><code>mailoney</code></li> <li><code>ipphoney</code></li> <li><code>ddospot</code></li> <li><code>elasticpot</code></li> <li><code>dicompot</code></li> <li><code>redishoneypot</code></li> <li><code>sentrypeer</code></li> <li><code>glutton</code></li> </ul> </li> </ul> <p>The available attack_type are:</p> <ul> <li><code>scanner</code>: IP addresses captured by the honeypots while performing attacks</li> <li><code>payload_request</code>: IP addresses and domains extracted from payloads that would have been executed after a speficic attack would have been successful</li> <li><code>all</code>: get all types at once</li> </ul> <p>The available age are:</p> <ul> <li><code>recent</code>: most recent IOCs seen in the last 3 days</li> <li><code>persistent</code>: these IOCs are the ones that were seen regularly by the honeypots. This feeds will start empty once no prior data was collected and will become bigger over time.</li> </ul> <p>The available formats are:</p> <ul> <li><code>txt</code>: plain text (just one line for each IOC)</li> <li><code>csv</code>: CSV-like file (just one line for each IOC)</li> <li><code>json</code>: JSON file with additional information regarding the IOCs</li> </ul> <p>Check the API specification or the to get all the details about how to use the available APIs.</p>"},{"location":"GreedyBear/Usage/#enrichment","title":"Enrichment","text":"<p>GreedyBear provides an easy-to-query API to get the information available in GB regarding the queried observable (domain or IP address).</p> <pre><code>https://&lt;greedybear_site&gt;/api/enrichment?query=&lt;observable&gt;\n</code></pre> <p>This \"Enrichment\" API is protected through authentication. Please reach out Matteo Lodi or another member of The Honeynet Project if you are interested in gain access to this API.</p> <p>If you would like to leverage this API without the need of writing even a line of code and together with a lot of other awesome tools, consider using IntelOwl.</p>"},{"location":"IntelOwl/","title":"Index","text":"<p>This is a Documentation for IntelOwl.</p>"},{"location":"IntelOwl/advanced_configuration/","title":"Advanced Configuration","text":"<p>This page includes details about some advanced features that Intel Owl provides which can be optionally configured by the administrator.</p>"},{"location":"IntelOwl/advanced_configuration/#elasticsearch","title":"ElasticSearch","text":"<p>Available for version &gt; 6.1.0</p> <p>Right now only ElasticSearch v8 is supported.</p>"},{"location":"IntelOwl/advanced_configuration/#configuration","title":"Configuration","text":"<p>In the <code>env_file_app_template</code>, you'd see various elasticsearch related environment variables. The user should spin their own Elastic Search instance and configure these variables.</p> <ul> <li>ELASTICSEARCH_DSL_ENABLED: Enable the ElasticSearch integration to perform advanced searches.</li> <li>ELASTICSEARCH_DSL_HOST: URL of the Elasticsearch instance.</li> <li>ELASTICSEARCH_DSL_PASSWORD: (optional) Password of the \"elastic\" user. This can be empty in case of external services with credentials in the url.</li> <li>ELASTICSEARCH_BI_ENABLED: Use the Business Intelligence feature.</li> <li>ELASTICSEARCH_BI_HOST: URL of the Elasticsearch instance for the BI.</li> <li>ELASTICSEARCH_BI_INDEX: Base path of the BI index.</li> </ul> <p>In the <code>env_file_elasticsearch_template</code> there is a viarable called <code>ELASTICSEARCH_PASSWORD</code>. This name is forced by elastic to set the password into the container.</p>"},{"location":"IntelOwl/advanced_configuration/#example-configuration","title":"Example Configuration","text":"<ul> <li>Use external instance: In this case it's enough to set the <code>ELASTICSEARCH_DSL_ENABLED</code> to <code>True</code> and <code>ELASTICSEARCH_DSL_HOST</code> with the URL of the external instance.</li> <li>Use docker instance:<ul> <li>Before starting IntelOwl move inside <code>docker</code> folder.</li> <li><code>cp env_file_elasticsearch_template env_file_elasticsearch</code></li> <li>Populate the var <code>ELASTICSEARCH_PASSWORD</code> inside the file <code>env_file_elasticsearch</code>.</li> <li>Populate the var <code>ELASTICSEARCH_DSL_PASSWORD</code> in the file <code>env_file_app</code> with the same value of <code>ELASTICSEARCH_PASSWORD</code>. Populate also <code>ELASTICSEARCH_DSL_HOST</code> with https://elasticsearch:9200.</li> <li>Start the project with <code>--elastic</code> in this way a container based Elasticsearch instance will start.</li> </ul> </li> </ul>"},{"location":"IntelOwl/advanced_configuration/#data-search","title":"Data Search","text":"<p>Thanks to django-elasticsearch-dsl Job results are indexed into elasticsearch. The <code>save</code> and <code>delete</code> operations are auto-synced so you always have the latest data in ES.</p> <p>With elasticsearch-py the AnalyzerReport, ConnectorReport and PivotReport objects are indexed into elasticsearch. In this way is possible to search data inside the report fields and many other via the UI. Each time IntelOwl is restarted the index template is updated and the every 5 minutes a task insert the reports in ElasticSearch. </p>"},{"location":"IntelOwl/advanced_configuration/#business-intelligence","title":"Business Intelligence","text":"<p>IntelOwl stores data that can be used for Business Intelligence purpose. Since plugin reports are deleted periodically, this feature allows to save indefinitely small amount of data to keep track of how analyzers perform and user usage. At the moment, the following information are sent to elastic:</p> <ul> <li>application name</li> <li>timestamp</li> <li>username</li> <li>configuration used</li> <li>process_time</li> <li>status</li> <li>end_time</li> <li>parameters</li> </ul> <p>Documents are saved in the <code>ELEASTICSEARCH_BI_INDEX-%YEAR-%MONTH</code>, allowing to manage the retention accordingly. To activate this feature, it is necessary to set <code>ELASTICSEARCH_BI_ENABLED</code> to <code>True</code> in the <code>env_file_app</code> and <code>ELASTICSEARCH_BI_HOST</code> to <code>elasticsearch:9200</code> or your elasticsearch server.</p> <p>An index template is created after the first bulk submission of reports.</p>"},{"location":"IntelOwl/advanced_configuration/#authentication-options","title":"Authentication options","text":"<p>IntelOwl provides support for some of the most common authentication methods:</p> <ul> <li>Google Oauth2</li> <li>LDAP</li> <li>RADIUS</li> </ul>"},{"location":"IntelOwl/advanced_configuration/#google-oauth2","title":"Google OAuth2","text":"<p>The first step is to create a Google Cloud Platform project, and then create OAuth credentials for it.</p> <p>It is important to add the correct callback in the \"Authorized redirect URIs\" section to allow the application to redirect properly after the successful login. Add this:</p> <pre><code>http://&lt;localhost|yourowndomain&gt;/api/auth/google-callback\n</code></pre> <p>After that, specify the client ID and secret as <code>GOOGLE_CLIENT_ID</code> and <code>GOOGLE_CLIENT_SECRET</code> environment variables and restart IntelOwl to see the applied changes.</p> <p>Note</p> While configuring Google Auth2 you can choose either to enable access to the all users with a Google Account (\"External\" mode) or to enable access to only the users of your organization (\"Internal\" mode). Reference"},{"location":"IntelOwl/advanced_configuration/#ldap","title":"LDAP","text":"<p>IntelOwl leverages Django-auth-ldap to perform authentication via LDAP.</p> <p>How to configure and enable LDAP on Intel Owl?</p> <ol> <li>Change the values with your LDAP configuration inside <code>configuration/ldap_config.py</code>. This file is mounted as a docker volume, so you won't need to rebuild the image.</li> </ol> <p>Note</p> For more details on how to configure this file, check the official documentation of the django-auth-ldap library.  <ol> <li>Once you have done that, you have to set the environment variable <code>LDAP_ENABLED</code> as <code>True</code> in the environment configuration file <code>env_file_app</code>.    Finally, you can restart the application with <code>docker-compose up</code></li> </ol>"},{"location":"IntelOwl/advanced_configuration/#radius-authentication","title":"RADIUS Authentication","text":"<p>IntelOwl leverages Django-radius to perform authentication via RADIUS server.</p> <p>How to configure and enable RADIUS authentication on Intel Owl?</p> <ol> <li>Change the values with your RADIUS auth configuration inside <code>configuration/radius_config.py</code>. This file is mounted as a    docker volume, so you won't need to rebuild the image.</li> </ol> <p>Note</p> For more details on how to configure this file, check the official documentation of the django-radius library.  <ol> <li>Once you have done that, you have to set the environment variable <code>RADIUS_AUTH_ENABLED</code> as <code>True</code> in the environment    configuration file <code>env_file_app</code>. Finally, you can restart the application with <code>docker-compose up</code></li> </ol>"},{"location":"IntelOwl/advanced_configuration/#opencti","title":"OpenCTI","text":"<p>Like many other integrations that we have, we have an Analyzer and a Connector for the OpenCTI platform.</p> <p>This allows the users to leverage these 2 popular open source projects and frameworks together.</p> <p>So why we have a section here? This is because there are various compatibility problems with the official PyCTI library.</p> <p>We found out (see issues in IntelOwl and PyCTI) that, most of the times, it is required that the OpenCTI version of the server you are using and the pycti version installed in IntelOwl must match perfectly.</p> <p>Because of that, we decided to provide to the users the chance to customize the version of PyCTI installed in IntelOwl based on the OpenCTI version that they are using.</p> <p>To do that, you would need to leverage the option <code>--pycti-version</code> provided by the <code>./start</code> helper:</p> <ul> <li>check the default version that would be installed by checking the description of the option <code>--pycti-version</code> with <code>./start -h</code></li> <li>if the default version is different from your OpenCTI server version, you need to rebuild the IntelOwl Image with <code>./start test build --pycti-version &lt;your_version&gt;</code></li> <li>then restart the project <code>./start test up -- --build</code></li> <li>enjoy</li> </ul>"},{"location":"IntelOwl/advanced_configuration/#cloud-support","title":"Cloud Support","text":""},{"location":"IntelOwl/advanced_configuration/#aws-support","title":"AWS support","text":"<p>We have support for several AWS services.</p> <p>You can customize the AWS Region location of you services by changing the environment variable <code>AWS_REGION</code>. Default is <code>eu-central-1</code></p> <p>You have to add some credentials for AWS: if you have IntelOwl deployed on the AWS infrastructure, you can use IAM credentials: to allow that just set <code>AWS_IAM_ACCESS</code> to <code>True</code>. If that is not the case, you have to set both <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code></p>"},{"location":"IntelOwl/advanced_configuration/#s3","title":"S3","text":"<p>If you prefer to use S3 to store the analyzed samples, instead of the local storage, you can do it.</p> <p>First, you need to configure the environment variable <code>LOCAL_STORAGE</code> to <code>False</code> to enable it and set <code>AWS_STORAGE_BUCKET_NAME</code> to the AWS bucket you want to use.</p> <p>Then you need to configure permission access to the chosen S3 bucket.</p>"},{"location":"IntelOwl/advanced_configuration/#message-broker","title":"Message Broker","text":"<p>IntelOwl at the moment supports 3 different message brokers:</p> <ul> <li>Redis (default)</li> <li>RabbitMQ</li> <li>Aws SQS</li> </ul> <p>The default broker, if nothing is specified, is <code>Redis</code>.</p> <p>To use <code>RabbitMQ</code>, you must use the option <code>--rabbitmq</code> when launching IntelOwl with the <code>./start</code> script.</p> <p>To use <code>AWS SQS</code>, you must use the option <code>--sqs</code> when launching IntelOwl with the <code>.start</code> script. In that case, you should create new FIFO SQS queues in AWS called <code>intelowl-&lt;environment&gt;-&lt;queue_name&gt;.fifo</code> and give your instances on AWS the proper permissions to access it. Moreover, you must populate the <code>AWS_USER_NUMBER</code>. This is required to connect in the right way to the selected SQS queues. Only FIFO queues are supported.</p> <p>If you want to use a remote message broker (like an <code>ElasticCache</code> or <code>AmazonMQ</code> instance), you must populate the <code>BROKER_URL</code> environment variable.</p> <p>It is possible to use task priority inside IntelOwl: each User has default priority of 10, and robots users (like the Ingestors) have a priority of 7. You can customize these priorities inside Django Admin, in the <code>Authentication.User Profiles</code> section.</p>"},{"location":"IntelOwl/advanced_configuration/#websockets","title":"Websockets","text":"<p><code>Redis</code> is used for two different functions:</p> <ul> <li>message broker</li> <li>websockets</li> </ul> <p>For this reason, a <code>Redis</code> instance is mandatory. You can personalize IntelOwl in two different way:</p> <ul> <li>with a local <code>Redis</code> instance.</li> </ul> <p>This is the default behaviour.</p> <ul> <li>With a remote <code>Redis</code> instance.</li> </ul> <p>You must use the option <code>--use-external-redis</code> when launching IntelOwl with the <code>.start</code> script. Moreover, you need to populate the <code>WEBSOCKETS_URL</code> environment variable. If you are using <code>Redis</code> as a message broker too, remember to populate the <code>BROKER_URL</code> environment variable</p>"},{"location":"IntelOwl/advanced_configuration/#rds","title":"RDS","text":"<p>If you like, you could use AWS RDS instead of PostgreSQL for your database. In that case, you should change the database required options accordingly: <code>DB_HOST</code>, <code>DB_PORT</code>, <code>DB_USER</code>, <code>DB_PASSWORD</code> and setup your machine to access the service.</p> <p>If you have IntelOwl deployed on the AWS infrastructure, you can use IAM credentials to access the Postgres DB. To allow that just set <code>AWS_RDS_IAM_ROLE</code> to <code>True</code>. In this case <code>DB_PASSWORD</code> is not required anymore.</p> <p>Moreover, to avoid to run PostgreSQL locally, you would need to use the option <code>--use-external-database</code> when launching IntelOwl with the <code>./start</code> script.</p>"},{"location":"IntelOwl/advanced_configuration/#ses","title":"SES","text":"<p>If you like, you could use Amazon SES for sending automated emails (password resets / registration requests, etc).</p> <p>You need to configure the environment variable <code>AWS_SES</code> to <code>True</code> to enable it.</p>"},{"location":"IntelOwl/advanced_configuration/#secrets","title":"Secrets","text":"<p>You can use the \"Secrets Manager\" to store your credentials. In this way your secrets would be better protected.</p> <p>First you need to set the environment variable <code>AWS_SECRETS</code> to <code>True</code> to enable this mode.</p> <p>Then, instead of adding the variables to the environment file, you should just add them with the same name on the AWS Secrets Manager and Intel Owl will fetch them transparently.</p> <p>Beware! Any left environment variable would be prioritized. So, you want to use your secrets in AWS, make sure to have removed the related environment variables locally.</p> <p>Obviously, you should also have created and managed the permissions in AWS in advance and accordingly to your infrastructure requirements.</p>"},{"location":"IntelOwl/advanced_configuration/#nfs","title":"NFS","text":"<p>You can use a <code>Network File System</code> for the shared_files that are downloaded runtime by IntelOwl (for example Yara rules).</p> <p>To use this feature, you would need to add the address of the remote file system inside the <code>.env</code> file, and you would need to use the option <code>--nfs</code> when launching IntelOwl with the <code>./start</code> script.</p>"},{"location":"IntelOwl/advanced_configuration/#google-kubernetes-engine","title":"Google Kubernetes Engine","text":"<p>Right now there is no official support for Kubernetes deployments.</p> <p>But we have an active community. Please refer to the following blog post for an example on how to deploy IntelOwl on Google Kubernetes Engine:</p> <p>Deploying Intel-Owl on GKE by Mayank Malik.</p>"},{"location":"IntelOwl/advanced_configuration/#queues","title":"Queues","text":""},{"location":"IntelOwl/advanced_configuration/#multi-queue","title":"Multi Queue","text":"<p>IntelOwl provides an additional multi-queue.override.yml compose file allowing IntelOwl users to better scale with the performance of their own architecture.</p> <p>If you want to leverage it, you should add the option <code>--multi-queue</code> when starting the project. Example:</p> <pre><code>./start prod up --multi-queue\n</code></pre> <p>This functionality is not enabled by default because this deployment would start 2 more containers so the resource consumption is higher. We suggest to use this option only when leveraging IntelOwl massively.</p>"},{"location":"IntelOwl/advanced_configuration/#queue-customization","title":"Queue Customization","text":"<p>It is possible to define new celery workers: each requires the addition of a new container in the docker-compose file, as shown in the <code>multi-queue.override.yml</code>.</p> <p>Moreover IntelOwl requires that the name of the workers are provided in the <code>docker-compose</code> file. This is done through the environment variable <code>CELERY_QUEUES</code> inside the <code>uwsgi</code> container. Each queue must be separated using the character <code>,</code>, as shown in the example.</p> <p>One can customize what analyzer should use what queue by specifying so in the analyzer entry in the analyzer_config.json configuration file. If no queue(s) are provided, the <code>default</code> queue will be selected.</p>"},{"location":"IntelOwl/advanced_configuration/#queue-monitoring","title":"Queue monitoring","text":"<p>IntelOwl provides an additional flower.override.yml compose file allowing IntelOwl users to use Flower features to monitor and manage queues and tasks</p> <p>If you want to leverage it, you should add the option <code>--flower</code> when starting the project. Example:</p> <pre><code>./start prod up --flower\n</code></pre> <p>The flower interface is available at port 5555: to set the credentials for its access, update the environment variables</p> <pre><code>FLOWER_USER\nFLOWER_PWD\n</code></pre> <p>or change the <code>.htpasswd</code> file that is created in the <code>docker</code> directory in the <code>intelowl_flower</code> container.</p>"},{"location":"IntelOwl/advanced_configuration/#manual-usage","title":"Manual Usage","text":"<p>The <code>./start</code> script essentially acts as a wrapper over Docker Compose, performing additional checks. IntelOwl can still be started by using the standard <code>docker compose</code> command, but all the dependencies have to be manually installed by the user.</p>"},{"location":"IntelOwl/advanced_configuration/#options","title":"Options","text":"<p>The <code>--project-directory</code> and <code>-p</code> options are required to run the project. Default values set by <code>./start</code> script are \"docker\" and \"intel_owl\", respectively.</p> <p>The startup is based on chaining various Docker Compose YAML files using <code>-f</code> option. All Docker Compose files are stored in <code>docker/</code> directory of the project. The default compose file, named <code>default.yml</code>, requires configuration for an external database and message broker. In their absence, the <code>postgres.override.yml</code> and <code>rabbitmq.override.yml</code> files should be chained to the default one.</p> <p>The command composed, considering what is said above (using <code>sudo</code>), is</p> <pre><code>sudo docker compose --project-directory docker -f docker/default.yml -f docker/postgres.override.yml -f docker/rabbitmq.override.yml -p intel_owl up\n</code></pre> <p>The other most common compose file that can be used is for the testing environment. The equivalent of running <code>./start test up</code> is adding the <code>test.override.yml</code> file, resulting in:</p> <pre><code>sudo docker compose --project-directory docker -f docker/default.yml -f docker/postgres.override.yml -f docker/rabbitmq.override.yml -f docker/test.override.yml -p intel_owl up\n</code></pre> <p>All other options available in the <code>./start</code> script (<code>./start -h</code> to view them) essentially chain other compose file to <code>docker compose</code> command with corresponding filenames.</p>"},{"location":"IntelOwl/advanced_configuration/#optional-analyzer","title":"Optional Analyzer","text":"<p>IntelOwl includes integrations with some analyzer that are not enabled by default. These analyzers, stored under the <code>integrations/</code> directory, are packed within Docker Compose files. The <code>compose.yml</code> file has to be chained to include the analyzer. The additional <code>compose-test.yml</code> file has to be chained for testing environment.</p>"},{"location":"IntelOwl/advanced_usage/","title":"Advanced Usage","text":"<p>This page includes details about some advanced features that Intel Owl provides which can be optionally enabled. Namely,</p>"},{"location":"IntelOwl/advanced_usage/#organizations-and-user-management","title":"Organizations and User management","text":"<p>Starting from IntelOwl v4, a new \"Organization\" section is available on the GUI. This section substitute the previous permission management via Django Admin and aims to provide an easier way to manage users and visibility.</p>"},{"location":"IntelOwl/advanced_usage/#multi-tenancy","title":"Multi Tenancy","text":"<p>Thanks to the \"Organization\" feature, IntelOwl can be used by multiple SOCs, companies, etc...very easily. Right now it works very simply: only users in the same organization can see analysis of one another. An user can belong to an organization only.</p>"},{"location":"IntelOwl/advanced_usage/#manage-organizations","title":"Manage organizations","text":"<p>You can create a new organization by going to the \"Organization\" section, available under the Dropdown menu you cand find under the username.</p> <p>Once you create an organization, you are the unique \"Owner\" of that organization. So you are the only one who can delete the organization and promote/demote/kick users. Another role, which is called \"Admin\", can be set to a user (via the Django Admin interface only for now). Owners and admins share the following powers: they can manage invitations and the organization's plugin configuration.</p>"},{"location":"IntelOwl/advanced_usage/#accept-invites","title":"Accept Invites","text":"<p>Once an invite has sent, the invited user has to login, go to the \"Organization\" section and accept the invite there. Afterwards the Administrator will be able to see the user in his \"Organization\" section.</p> <p></p>"},{"location":"IntelOwl/advanced_usage/#plugins-params-and-secrets","title":"Plugins Params and Secrets","text":"<p>From IntelOwl v4.1.0, Plugin Parameters and Secrets can be defined at the organization level, in the dedicated section. This allows to share configurations between users of the same org while allowing complete multi-tenancy of the application. Only Owners and Admins of the organization can set, change and delete them.</p>"},{"location":"IntelOwl/advanced_usage/#disable-plugins-at-org-level","title":"Disable Plugins at Org level","text":"<p>The org admin can disable a specific plugin for all the users in a specific org. To do that, Org Admins needs to go in the \"Plugins\" section and click the button \"Enabled for organization\" of the plugin that they want to disable.</p> <p></p>"},{"location":"IntelOwl/advanced_usage/#registration","title":"Registration","text":"<p>Since IntelOwl v4.2.0 we added a Registration Page that can be used to manage Registration requests when providing IntelOwl as a Service.</p> <p>After a user registration has been made, an email is sent to the user to verify their email address. If necessary, there are buttons on the login page to resend the verification email and to reset the password.</p> <p>Once the user has verified their email, they would be manually vetted before being allowed to use the IntelOwl platform. The registration requests would be handled in the Django Admin page by admins. If you have IntelOwl deployed on an AWS instance with an IAM role you can use the SES service.</p> <p>To have the \"Registration\" page to work correctly, you must configure some variables before starting IntelOwl. See Optional Environment Configuration</p> <p>In a development environment the emails that would be sent are written to the standard output.</p>"},{"location":"IntelOwl/advanced_usage/#optional-analyzers","title":"Optional Analyzers","text":"<p>Some analyzers which run in their own Docker containers are kept disabled by default. They are disabled by default to prevent accidentally starting too many containers and making your computer unresponsive.</p> Name Analyzers Description Malware Tools Analyzers <ul> <li><code>PEframe_Scan</code></li> <li><code>Capa_Info</code></li> <li><code>Floss</code></li> <li><code>Strings_Info</code></li> <li><code>ClamAV</code></li> <li><code>APKiD</code></li> <li><code>Thug_URL_Info</code>,       <code>Thug_HTML_Info</code></li> <li><code>BoxJS</code></li> <li><code>Qiling_Windows</code>,       <code>Qiling_Windows_Shellcode</code>,       <code>Qiling_Linux</code>,       <code>Qiling_Linux_Shellcode</code></li> </ul> <ul> <li>PEFrame performs static analysis on Portable Executable malware and malicious MS Office documents</li> <li>Capa detects capabilities in executable files</li> <li>FLOSS automatically deobfuscate strings from malware binaries</li> <li>String_Info_Classic extracts human-readable strings where as ML version of it ranks them</li> <li>ClamAV antivirus engine scans files for trojans, viruses, malwares using a multi-threaded daemon</li> <li>APKiD identifies many compilers, packers, obfuscators, and other weird stuff from an APK or DEX file.</li> <li>Thug performs hybrid dynamic/static analysis on a URL or HTML page.</li> <li>Box-JS is a tool for studying JavaScript malware</li> <li>Qiling is a tool for emulating the execution of a binary file or a shellcode.      It requires the configuration of its rootfs, and the optional configuration of profiles.      The rootfs can be copied from the  Qiling project: please remember that Windows dll  must be manually added for license reasons.      Qiling provides a  DllCollector to retrieve dlls from your licensed Windows.        Profiles  must be placed in the <code>profiles</code> subfolder      </li> </ul> TOR Analyzers <code>Onionscan</code> Scans TOR .onion domains for privacy leaks and information disclosures. CyberChef <code>CyberChef</code> Run a transformation on a CyberChef server using pre-defined or custom recipes(rules that describe how the input has to be transformed). Check further instructions here PCAP Analyzers <code>Suricata</code> You can upload a PCAP to have it analyzed by Suricata with the open Ruleset. The result will provide a list of the triggered signatures plus a more detailed report with all the raw data generated by Suricata. You can also add your own rules (See paragraph \"Analyzers with special configuration\"). The installation is optimized for scaling so the execution time is really fast. PhoneInfoga <code>PhoneInfoga_scan</code> PhoneInfoga is one of the most advanced tools to scan international phone numbers. It allows you to first gather basic information such as country, area, carrier and line type, then use various techniques to try to find the VoIP provider or identify the owner. It works with a collection of scanners that must be configured in order for the tool to be effective. PhoneInfoga doesn't automate everything, it's just there to help investigating on phone numbers. here Phishing Analyzers <ul> <li><code>Phishing_Extractor</code></li> <li><code>Phishing_Form_Compiler</code></li> </ul> This framework tries to render a potential phishing page and extract useful information from it. Also, if the page contains a form, it tries to submit the form using fake data. The goal is to extract IOCs and check whether the page is real phishing or not. <p>To enable all the optional analyzers you can add the option <code>--all_analyzers</code> when starting the project. Example:</p> <pre><code>./start prod up --all_analyzers\n</code></pre> <p>Otherwise you can enable just one of the cited integration by using the related option. Example:</p> <pre><code>./start prod up --tor_analyzers\n</code></pre>"},{"location":"IntelOwl/advanced_usage/#customize-analyzer-execution","title":"Customize analyzer execution","text":"<p>Some analyzers provide the chance to customize the performed analysis based on parameters that are different for each analyzer.</p>"},{"location":"IntelOwl/advanced_usage/#from-the-gui","title":"from the GUI","text":"<p>You can click on \"Runtime Configuration\"  button in the \"Scan\" page and add the runtime configuration in the form of a dictionary. Example:</p> <pre><code>\"VirusTotal_v3_File\": {\n    \"force_active_scan_if_old\": true\n}\n</code></pre>"},{"location":"IntelOwl/advanced_usage/#from-pyintelowl","title":"from Pyintelowl","text":"<p>While using <code>send_observable_analysis_request</code> or <code>send_file_analysis_request</code> endpoints, you can pass the parameter <code>runtime_configuration</code> with the optional values. Example:</p> <pre><code>runtime_configuration = {\n    \"Doc_Info\": {\n        \"additional_passwords_to_check\": [\"passwd\", \"2020\"]\n    }\n}\npyintelowl_client.send_file_analysis_request(..., runtime_configuration=runtime_configuration)\n</code></pre>"},{"location":"IntelOwl/advanced_usage/#phoneinfoga","title":"PhoneInfoga","text":"<p>PhoneInfoga provides several Scanners to extract as much information as possible from a given phone number. Those scanners may require authentication, so they are automatically skipped when no authentication credentials are found.</p> <p>By default the scanner used is <code>local</code>. Go through this guide to initiate other required API keys related to this analyzer.</p>"},{"location":"IntelOwl/advanced_usage/#cyberchef","title":"CyberChef","text":"<p>You can either use pre-defined recipes or create your own as explained here.</p> <p>To use a pre-defined recipe, set the <code>predefined_recipe_name</code> argument to the name of the recipe as defined here. Else, leave the <code>predefined_recipe_name</code> argument empty and set the <code>custom_recipe</code> argument to the contents of the recipe you want to use.</p> <p>Additionally, you can also (optionally) set the <code>output_type</code> argument.</p>"},{"location":"IntelOwl/advanced_usage/#pre-defined-recipes","title":"Pre-defined recipes","text":"<ul> <li>\"to decimal\": <code>[{\"op\": \"To Decimal\", \"args\": [\"Space\", False]}]</code></li> </ul>"},{"location":"IntelOwl/advanced_usage/#phishing-analyzers","title":"Phishing Analyzers","text":"<p>The framework aims to be extandable and provides two different playbooks connected through a pivot. The first playbook, named <code>PhishingExtractor</code>, is in charge of extracting useful information from the web page rendered with Selenium-based browser. The second playbook is called <code>PhishingAnalysis</code> and its main purposes are to extract useful insights on the page itself and to try to submit forms with fake data to extract other IOCs.</p> <p>XPath syntax is used to find elements in the page. These selectors are customizable via the plugin's config page. The parameter <code>xpath_form_selector</code> controls how the form is retrieved from the page and <code>xpath_js_selector</code> is used to search for JavaScript inside the page.</p> <p>A mapping is used in order to compile the page with fake data. This is due to the fact that most input tags of type \"text\" do not have a specific role in the page, so there must be some degree of approximation. This behaviour is controlled through <code>*-mapping</code> parameters. They are a list that must contain the input tag's name to compile with fake data.</p> <p>Here is an example of what a phishing investigation looks like started from <code>PhishingExtractor</code> playbook:  </p>"},{"location":"IntelOwl/advanced_usage/#infrastructure-diagram","title":"Infrastructure diagram","text":"<p>To better understand how this integration works, here is a diagram showing how the components are arranged (at container level) and how they communicate to reach target website. </p>"},{"location":"IntelOwl/advanced_usage/#analyzers-with-special-configuration","title":"Analyzers with special configuration","text":"<p>Some analyzers could require a special configuration:</p> <ul> <li><code>GoogleWebRisk</code>: this analyzer needs a service account key with the Google Cloud credentials to work properly.   You should follow the official guide for creating the key.   Then you can populate the secret <code>service_account_json</code> for that analyzer with the JSON of the service account file.</li> </ul> <ul> <li><code>ClamAV</code>: this Docker-based analyzer uses <code>clamd</code> daemon as its scanner and is communicating with <code>clamdscan</code> utility to scan files. The daemon requires 2 different configuration files: <code>clamd.conf</code>(daemon's config) and <code>freshclam.conf</code> (virus database updater's config). These files are mounted as docker volumes in <code>/integrations/malware_tools_analyzers/clamav</code> and hence, can be edited by the user as per needs, without restarting the application. Moreover ClamAV is integrated with unofficial open source signatures extracted with Fangfrisch. The configuration file <code>fangfrisch.conf</code> is mounted in the same directory and can be customized on your wish. For instance, you should change it if you want to integrate open source signatures from SecuriteInfo</li> </ul> <ul> <li> <p><code>Suricata</code>: you can customize the behavior of Suricata:</p> <ul> <li><code>/integrations/pcap_analyzers/config/suricata/rules</code>: here there are Suricata rules. You can change the <code>custom.rules</code> files to add your own rules at any time. Once you made this change, you need to either restart IntelOwl or (this is faster) run a new analysis with the Suricata analyzer and set the parameter <code>reload_rules</code> to <code>true</code>.</li> <li><code>/integrations/pcap_analyzers/config/suricata/etc</code>: here there are Suricata configuration files. Change it based on your wish. Restart IntelOwl to see the changes applied.</li> </ul> </li> </ul> <ul> <li><code>Yara</code>:<ul> <li>You can customize both the <code>repositories</code> parameter and <code>private_repositories</code> secret to download and use different rules from the default that IntelOwl currently support.<ul> <li>The <code>repositories</code> values is what will be used to actually run the analysis: if you have added private repositories, remember to add the url in <code>repositories</code> too!</li> </ul> </li> <li>You can add local rules inside the directory at <code>/opt/deploy/files_required/yara/YOUR_USERNAME/custom_rules/</code>. Please remember that these rules are not synced in a cluster deploy: for this reason is advised to upload them on GitHub and use the <code>repositories</code> or <code>private_repositories</code> attributes.</li> </ul> </li> </ul> <ul> <li><code>NERD</code> :<ul> <li>The <code>nerd_analysis</code> parameter allows you to customize the level of detail in the analysis response. Available options are:<ul> <li><code>basic</code> (default): Provides a simplified response from the database.</li> <li><code>full</code>: Includes all available information about the IP from the database.</li> <li><code>fmp</code>: Returns only the FMP (Future Misbehavior Probability) score.</li> <li><code>rep</code>: Returns only the reputation score of the IP.</li> </ul> </li> </ul> </li> </ul>"},{"location":"IntelOwl/advanced_usage/#notifications","title":"Notifications","text":"<p>Since v4, IntelOwl integrated the notification system from the <code>certego_saas</code> package, allowing the admins to create notification that every user will be able to see.</p> <p>The user would find the Notifications button on the top right of the page:</p> <p></p> <p>There the user can read notifications provided by either the administrators or the IntelOwl Maintainers.</p> <p>As an Admin, if you want to add a notification to have it sent to all the users, you have to login to the Django Admin interface, go to the \"Notifications\" section and add it there. While adding a new notification, in the <code>body</code> section it is possible to even use HTML syntax, allowing to embed images, links, etc; in the <code>app_name field</code>, please remember to use <code>intelowl</code> as the app name.</p> <p>Everytime a new release is installed, once the backend goes up it will automatically create a new notification, having as content the latest changes described in the CHANGELOG.md, allowing the users to keep track of the changes inside intelowl itself.</p>"},{"location":"IntelOwl/api_docs/","title":"API Documentation","text":""},{"location":"IntelOwl/api_docs/#global-functions","title":"Global Functions","text":""},{"location":"IntelOwl/api_docs/#ask_analysis_availability","title":"<code>ask_analysis_availability</code>","text":"<p>API endpoint to check for existing analysis based on an MD5 hash.</p> <p>This endpoint helps avoid redundant analysis by checking if there is already an analysis in progress or completed with status \"running\" or \"reported_without_fails\" for the provided MD5 hash. The analyzers that need to be executed should be specified to ensure expected results.</p> <p>Deprecated: This endpoint will be deprecated after 01-07-2023.</p> <p>Parameters: - request (POST): Contains the MD5 hash and analyzer details.</p> <p>Returns: - 200: JSON response with the analysis status, job ID, and analyzers to be executed.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"\n    This is useful to avoid repeating the same analysis multiple times.\n    By default this API checks if there are existing analysis related to the md5 in\n    status \"running\" or \"reported_without_fails\"\n    Also, you need to specify the analyzers needed because, otherwise, it is\n    highly probable that you won't get all the results that you expect\"\"\",\n    request=JobAvailabilitySerializer,\n    responses={\n        200: inline_serializer(\n            name=\"AskAnalysisAvailabilitySuccessResponse\",\n            fields={\n                \"status\": rfs.StringRelatedField(),\n                \"job_id\": rfs.StringRelatedField(),\n                \"analyzers_to_execute\": OpenApiTypes.OBJECT,\n            },\n        ),\n    },\n)\n@deprecated_endpoint(deprecation_date=\"01-07-2023\")\n@api_view([\"POST\"])\ndef ask_analysis_availability(request):\n    \"\"\"\n    API endpoint to check for existing analysis based on an MD5 hash.\n\n    This endpoint helps avoid redundant analysis by checking if there is already an analysis\n    in progress or completed with status \"running\" or \"reported_without_fails\" for the provided MD5 hash.\n    The analyzers that need to be executed should be specified to ensure expected results.\n\n    Deprecated: This endpoint will be deprecated after 01-07-2023.\n\n    Parameters:\n    - request (POST): Contains the MD5 hash and analyzer details.\n\n    Returns:\n    - 200: JSON response with the analysis status, job ID, and analyzers to be executed.\n    \"\"\"\n    serializer = JobAvailabilitySerializer(\n        data=request.data, context={\"request\": request}\n    )\n    serializer.is_valid(raise_exception=True)\n    try:\n        job = serializer.save()\n    except Job.DoesNotExist:\n        result = None\n    else:\n        result = job\n    return Response(\n        JobResponseSerializer(result).data,\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#ask_multi_analysis_availability","title":"<code>ask_multi_analysis_availability</code>","text":"<p>API endpoint to check for existing analysis for multiple MD5 hashes.</p> <p>Similar to <code>ask_analysis_availability</code>, this endpoint checks for existing analysis for multiple MD5 hashes. It prevents redundant analysis by verifying if there are any jobs in progress or completed with status \"running\" or \"reported_without_fails\". The analyzers required should be specified to ensure accurate results.</p> <p>Parameters: - request (POST): Contains multiple MD5 hashes and analyzer details.</p> <p>Returns: - 200: JSON response with the analysis status, job IDs, and analyzers to be executed for each MD5 hash.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"\n    This is useful to avoid repeating the same analysis multiple times.\n    By default this API checks if there are existing analysis related to the md5 in\n    status \"running\" or \"reported_without_fails\"\n    Also, you need to specify the analyzers needed because, otherwise, it is\n    highly probable that you won't get all the results that you expect.\n    NOTE: This API is similar to ask_analysis_availability, but it allows multiple\n    md5s to be checked at the same time.\"\"\",\n    responses={200: JobAvailabilitySerializer(many=True)},\n)\n@api_view([\"POST\"])\ndef ask_multi_analysis_availability(request):\n    \"\"\"\n    API endpoint to check for existing analysis for multiple MD5 hashes.\n\n    Similar to `ask_analysis_availability`, this endpoint checks for existing analysis for multiple MD5 hashes.\n    It prevents redundant analysis by verifying if there are any jobs in progress or completed with status\n    \"running\" or \"reported_without_fails\". The analyzers required should be specified to ensure accurate results.\n\n    Parameters:\n    - request (POST): Contains multiple MD5 hashes and analyzer details.\n\n    Returns:\n    - 200: JSON response with the analysis status, job IDs, and analyzers to be executed for each MD5 hash.\n    \"\"\"\n    logger.info(f\"received ask_multi_analysis_availability from user {request.user}\")\n    serializer = JobAvailabilitySerializer(\n        data=request.data, context={\"request\": request}, many=True\n    )\n    serializer.is_valid(raise_exception=True)\n    try:\n        jobs = serializer.save()\n    except Job.DoesNotExist:\n        result = []\n    else:\n        result = jobs\n    jrs = JobResponseSerializer(result, many=True).data\n    logger.info(f\"finished ask_multi_analysis_availability from user {request.user}\")\n    return Response(\n        jrs,\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#analyze_file","title":"<code>analyze_file</code>","text":"<p>API endpoint to start an analysis job for a single file.</p> <p>This endpoint initiates an analysis job for a single file and sends it to the specified analyzers. The file-related information and analyzers should be provided in the request data.</p> <p>Parameters: - request (POST): Contains file data and analyzer details.</p> <p>Returns: - 200: JSON response with the job details after initiating the analysis.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"This endpoint allows to start a Job related for a single File.\"\n    \" Retained for retro-compatibility\",\n    request=FileJobSerializer,\n    responses={200: JobResponseSerializer(many=True)},\n)\n@api_view([\"POST\"])\ndef analyze_file(request):\n    \"\"\"\n    API endpoint to start an analysis job for a single file.\n\n    This endpoint initiates an analysis job for a single file and sends it to the\n    specified analyzers. The file-related information and analyzers should be provided\n    in the request data.\n\n    Parameters:\n    - request (POST): Contains file data and analyzer details.\n\n    Returns:\n    - 200: JSON response with the job details after initiating the analysis.\n    \"\"\"\n    logger.info(f\"received analyze_file from user {request.user}\")\n    fas = FileJobSerializer(data=request.data, context={\"request\": request})\n    fas.is_valid(raise_exception=True)\n    job = fas.save(send_task=True)\n    jrs = JobResponseSerializer(job).data\n    logger.info(f\"finished analyze_file from user {request.user}\")\n    return Response(\n        jrs,\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#analyze_multiple_files","title":"<code>analyze_multiple_files</code>","text":"<p>API endpoint to start analysis jobs for multiple files.</p> <p>This endpoint initiates analysis jobs for multiple files and sends them to the specified analyzers. The file-related information and analyzers should be provided in the request data.</p> <p>Parameters: - request (POST): Contains multiple file data and analyzer details.</p> <p>Returns: - 200: JSON response with the job details for each initiated analysis.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"This endpoint allows to start Jobs related to multiple Files\",\n    # It should be better to link the doc to the related MultipleFileAnalysisSerializer.\n    # It is not straightforward because you can't just add a class\n    # which extends a ListSerializer.\n    # Follow this doc to try to find a fix:\n    # https://drf-spectacular.readthedocs.io/en/latest/customization.html#declare-serializer-magic-with\n    # -openapiserializerextension\n    request=inline_serializer(\n        name=\"MultipleFilesSerializer\",\n        fields={\n            \"files\": rfs.ListField(child=rfs.FileField()),\n            \"file_names\": rfs.ListField(child=rfs.CharField()),\n            \"file_mimetypes\": rfs.ListField(child=rfs.CharField()),\n        },\n    ),\n    responses={200: JobResponseSerializer},\n)\n@api_view([\"POST\"])\ndef analyze_multiple_files(request):\n    \"\"\"\n    API endpoint to start analysis jobs for multiple files.\n\n    This endpoint initiates analysis jobs for multiple files and sends them to the specified analyzers.\n    The file-related information and analyzers should be provided in the request data.\n\n    Parameters:\n    - request (POST): Contains multiple file data and analyzer details.\n\n    Returns:\n    - 200: JSON response with the job details for each initiated analysis.\n    \"\"\"\n    logger.info(f\"received analyze_multiple_files from user {request.user}\")\n    fas = FileJobSerializer(data=request.data, context={\"request\": request}, many=True)\n    fas.is_valid(raise_exception=True)\n    parent_job = fas.validated_data[0].get(\"parent_job\", None)\n    jobs = fas.save(send_task=True, parent=parent_job)\n    jrs = JobResponseSerializer(jobs, many=True).data\n    logger.info(f\"finished analyze_multiple_files from user {request.user}\")\n    return Response(\n        jrs,\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#analyze_observable","title":"<code>analyze_observable</code>","text":"<p>API endpoint to start an analysis job for a single observable.</p> <p>This endpoint initiates an analysis job for a single observable (e.g., domain, IP, URL, etc.) and sends it to the specified analyzers. The observable-related information and analyzers should be provided in the request data.</p> <p>Parameters: - request (POST): Contains observable data and analyzer details.</p> <p>Returns: - 200: JSON response with the job details after initiating the analysis.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"This endpoint allows to start a Job related to an observable. \"\n    \"Retained for retro-compatibility\",\n    request=ObservableAnalysisSerializer,\n    responses={200: JobResponseSerializer},\n)\n@api_view([\"POST\"])\ndef analyze_observable(request):\n    \"\"\"\n    API endpoint to start an analysis job for a single observable.\n\n    This endpoint initiates an analysis job for a single observable (e.g., domain, IP, URL, etc.)\n    and sends it to the specified analyzers. The observable-related information and analyzers should be\n    provided in the request data.\n\n    Parameters:\n    - request (POST): Contains observable data and analyzer details.\n\n    Returns:\n    - 200: JSON response with the job details after initiating the analysis.\n    \"\"\"\n    logger.info(f\"received analyze_observable from user {request.user}\")\n    oas = ObservableAnalysisSerializer(data=request.data, context={\"request\": request})\n    oas.is_valid(raise_exception=True)\n    job = oas.save(send_task=True)\n    jrs = JobResponseSerializer(job).data\n    logger.info(f\"finished analyze_observable from user {request.user}\")\n    return Response(\n        jrs,\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#analyze_multiple_observables","title":"<code>analyze_multiple_observables</code>","text":"<p>API endpoint to start analysis jobs for multiple observables.</p> <p>This endpoint initiates analysis jobs for multiple observables (e.g., domain, IP, URL, etc.) and sends them to the specified analyzers. The observables and analyzer details should be provided in the request data.</p> <p>Parameters: - request (POST): Contains multiple observable data and analyzer details.</p> <p>Returns: - 200: JSON response with the job details for each initiated analysis.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"This endpoint allows to start Jobs related to multiple observables.\n                 Observable parameter must be composed like this:\n                 [(&lt;observable_classification&gt;, &lt;observable_name&gt;), ...]\"\"\",\n    request=inline_serializer(\n        name=\"MultipleObservableSerializer\",\n        fields={\n            \"observables\": rfs.ListField(\n                child=rfs.ListField(max_length=2, min_length=2)\n            )\n        },\n    ),\n    responses={200: JobResponseSerializer},\n)\n@api_view([\"POST\"])\ndef analyze_multiple_observables(request):\n    \"\"\"\n    API endpoint to start analysis jobs for multiple observables.\n\n    This endpoint initiates analysis jobs for multiple observables (e.g., domain, IP, URL, etc.)\n    and sends them to the specified analyzers. The observables and analyzer details should\n    be provided in the request data.\n\n    Parameters:\n    - request (POST): Contains multiple observable data and analyzer details.\n\n    Returns:\n    - 200: JSON response with the job details for each initiated analysis.\n    \"\"\"\n    logger.info(f\"received analyze_multiple_observables from user {request.user}\")\n    oas = ObservableAnalysisSerializer(\n        data=request.data, many=True, context={\"request\": request}\n    )\n    oas.is_valid(raise_exception=True)\n    parent_job = oas.validated_data[0].get(\"parent_job\", None)\n    jobs = oas.save(send_task=True, parent=parent_job)\n    jrs = JobResponseSerializer(jobs, many=True).data\n    logger.info(f\"finished analyze_multiple_observables from user {request.user}\")\n    return Response(\n        jrs,\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#classes","title":"Classes","text":""},{"location":"IntelOwl/api_docs/#commentviewset","title":"<code>CommentViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> <p>CommentViewSet provides the following actions:</p> <ul> <li>list: Retrieve a list of comments associated with jobs visible to the authenticated user.</li> <li>retrieve: Retrieve a specific comment by ID, accessible to the comment's owner or anyone in the same organization.</li> <li>destroy: Delete a comment by ID, allowed only for the comment's owner.</li> <li>update: Update a comment by ID, allowed only for the comment's owner.</li> <li>partial_update: Partially update a comment by ID, allowed only for the comment's owner.</li> </ul> <p>Permissions: - IsAuthenticated: Requires authentication for all actions. - IsObjectUserPermission: Allows only the comment owner to update or delete the comment. - IsObjectUserOrSameOrgPermission: Allows the comment owner or anyone in the same organization to retrieve the comment.</p> <p>Queryset: - Filters comments to include only those associated with jobs visible to the authenticated user.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"\n    REST endpoint to fetch list of job comments or\n    retrieve/delete a job comment with job comment ID.\n    Requires authentication.\n    \"\"\"\n)\nclass CommentViewSet(ModelViewSet):\n    \"\"\"\n    CommentViewSet provides the following actions:\n\n    - **list**: Retrieve a list of comments associated with jobs visible to the authenticated user.\n    - **retrieve**: Retrieve a specific comment by ID, accessible to the comment's owner or anyone in the same organization.\n    - **destroy**: Delete a comment by ID, allowed only for the comment's owner.\n    - **update**: Update a comment by ID, allowed only for the comment's owner.\n    - **partial_update**: Partially update a comment by ID, allowed only for the comment's owner.\n\n    Permissions:\n    - **IsAuthenticated**: Requires authentication for all actions.\n    - **IsObjectUserPermission**: Allows only the comment owner to update or delete the comment.\n    - **IsObjectUserOrSameOrgPermission**: Allows the comment owner or anyone in the same organization to retrieve the comment.\n\n    Queryset:\n    - Filters comments to include only those associated with jobs visible to the authenticated user.\n    \"\"\"\n\n    queryset = Comment.objects.all()\n    serializer_class = CommentSerializer\n    permission_classes = [IsAuthenticated]\n\n    def get_permissions(self):\n        \"\"\"\n        Customizes permissions based on the action being performed.\n\n        - For `destroy`, `update`, and `partial_update` actions, adds `IsObjectUserPermission` to ensure that only\n          the comment owner can perform these actions.\n        - For the `retrieve` action, adds `IsObjectUserOrSameOrgPermission` to allow the comment owner or anyone in the same\n          organization to retrieve the comment.\n\n        Returns:\n        - List of applicable permissions.\n        \"\"\"\n        permissions = super().get_permissions()\n\n        # only the owner of the comment can update or delete the comment\n        if self.action in [\"destroy\", \"update\", \"partial_update\"]:\n            permissions.append(IsObjectUserPermission())\n        # the owner and anyone in the org can read the comment\n        if self.action in [\"retrieve\"]:\n            permissions.append(IsObjectUserOrSameOrgPermission())\n\n        return permissions\n\n    def get_queryset(self):\n        \"\"\"\n        Filters the queryset to include only comments related to jobs visible to the authenticated user.\n\n        - Fetches job IDs that are visible to the user.\n        - Filters the comment queryset to include only comments associated with these jobs.\n\n        Returns:\n        - Filtered queryset of comments.\n        \"\"\"\n        queryset = super().get_queryset()\n        jobs = Job.objects.visible_for_user(self.request.user).values_list(\n            \"pk\", flat=True\n        )\n        return queryset.filter(job__id__in=jobs)\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.CommentViewSet.get_permissions","title":"<code>get_permissions()</code>","text":"<p>Customizes permissions based on the action being performed.</p> <ul> <li>For <code>destroy</code>, <code>update</code>, and <code>partial_update</code> actions, adds <code>IsObjectUserPermission</code> to ensure that only   the comment owner can perform these actions.</li> <li>For the <code>retrieve</code> action, adds <code>IsObjectUserOrSameOrgPermission</code> to allow the comment owner or anyone in the same   organization to retrieve the comment.</li> </ul> <p>Returns: - List of applicable permissions.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>def get_permissions(self):\n    \"\"\"\n    Customizes permissions based on the action being performed.\n\n    - For `destroy`, `update`, and `partial_update` actions, adds `IsObjectUserPermission` to ensure that only\n      the comment owner can perform these actions.\n    - For the `retrieve` action, adds `IsObjectUserOrSameOrgPermission` to allow the comment owner or anyone in the same\n      organization to retrieve the comment.\n\n    Returns:\n    - List of applicable permissions.\n    \"\"\"\n    permissions = super().get_permissions()\n\n    # only the owner of the comment can update or delete the comment\n    if self.action in [\"destroy\", \"update\", \"partial_update\"]:\n        permissions.append(IsObjectUserPermission())\n    # the owner and anyone in the org can read the comment\n    if self.action in [\"retrieve\"]:\n        permissions.append(IsObjectUserOrSameOrgPermission())\n\n    return permissions\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.CommentViewSet.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Filters the queryset to include only comments related to jobs visible to the authenticated user.</p> <ul> <li>Fetches job IDs that are visible to the user.</li> <li>Filters the comment queryset to include only comments associated with these jobs.</li> </ul> <p>Returns: - Filtered queryset of comments.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>def get_queryset(self):\n    \"\"\"\n    Filters the queryset to include only comments related to jobs visible to the authenticated user.\n\n    - Fetches job IDs that are visible to the user.\n    - Filters the comment queryset to include only comments associated with these jobs.\n\n    Returns:\n    - Filtered queryset of comments.\n    \"\"\"\n    queryset = super().get_queryset()\n    jobs = Job.objects.visible_for_user(self.request.user).values_list(\n        \"pk\", flat=True\n    )\n    return queryset.filter(job__id__in=jobs)\n</code></pre>"},{"location":"IntelOwl/api_docs/#jobviewset","title":"<code>JobViewSet</code>","text":"<p>               Bases: <code>ReadAndDeleteOnlyViewSet</code>, <code>SerializerActionMixin</code></p> <p>JobViewSet provides the following actions:</p> <ul> <li>list: Retrieve a list of jobs visible to the authenticated user, ordered by request time.</li> <li>retrieve: Retrieve a specific job by ID.</li> <li>destroy: Delete a job by ID, allowed only for the job owner or anyone in the same organization.</li> <li>recent_scans: Retrieve recent jobs based on an MD5 hash, limited by a maximum temporal distance.</li> <li>recent_scans_user: Retrieve recent jobs for the authenticated user, filtered by sample status.</li> <li>retry: Retry a job if its status is in a final state.</li> <li>kill: Kill a running job by closing celery tasks and marking it as killed.</li> <li>download_sample: Download a file/sample associated with a job.</li> <li>pivot: Perform a pivot operation from a job's reports.</li> <li>aggregate_status: Aggregate jobs by their status over a specified time range.</li> <li>aggregate_type: Aggregate jobs by type (file or observable) over a specified time range.</li> <li>aggregate_observable_classification: Aggregate jobs by observable classification over a specified time range.</li> <li>aggregate_file_mimetype: Aggregate jobs by file MIME type over a specified time range.</li> <li>aggregate_observable_name: Aggregate jobs by observable name over a specified time range.</li> <li>aggregate_md5: Aggregate jobs by MD5 hash over a specified time range.</li> </ul> <p>Permissions: - IsAuthenticated: Requires authentication for all actions. - IsObjectUserOrSameOrgPermission: Allows job deletion or killing only by the job owner or anyone in the same organization.</p> <p>Queryset: - Prefetches related tags and orders jobs by request time, filtered to include only jobs visible to the authenticated user.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"\n    REST endpoint to fetch list of jobs or retrieve/delete a job with job ID.\n    Requires authentication.\n    \"\"\"\n)\nclass JobViewSet(ReadAndDeleteOnlyViewSet, SerializerActionMixin):\n    \"\"\"\n    JobViewSet provides the following actions:\n\n    - **list**: Retrieve a list of jobs visible to the authenticated user, ordered by request time.\n    - **retrieve**: Retrieve a specific job by ID.\n    - **destroy**: Delete a job by ID, allowed only for the job owner or anyone in the same organization.\n    - **recent_scans**: Retrieve recent jobs based on an MD5 hash, limited by a maximum temporal distance.\n    - **recent_scans_user**: Retrieve recent jobs for the authenticated user, filtered by sample status.\n    - **retry**: Retry a job if its status is in a final state.\n    - **kill**: Kill a running job by closing celery tasks and marking it as killed.\n    - **download_sample**: Download a file/sample associated with a job.\n    - **pivot**: Perform a pivot operation from a job's reports.\n    - **aggregate_status**: Aggregate jobs by their status over a specified time range.\n    - **aggregate_type**: Aggregate jobs by type (file or observable) over a specified time range.\n    - **aggregate_observable_classification**: Aggregate jobs by observable classification over a specified time range.\n    - **aggregate_file_mimetype**: Aggregate jobs by file MIME type over a specified time range.\n    - **aggregate_observable_name**: Aggregate jobs by observable name over a specified time range.\n    - **aggregate_md5**: Aggregate jobs by MD5 hash over a specified time range.\n\n    Permissions:\n    - **IsAuthenticated**: Requires authentication for all actions.\n    - **IsObjectUserOrSameOrgPermission**: Allows job deletion or killing only by the job owner or anyone in the same organization.\n\n    Queryset:\n    - Prefetches related tags and orders jobs by request time, filtered to include only jobs visible to the authenticated user.\n    \"\"\"\n\n    queryset = (\n        Job.objects.prefetch_related(\"tags\").order_by(\"-received_request_time\").all()\n    )\n    serializer_class = RestJobSerializer\n    serializer_action_classes = {\n        \"retrieve\": RestJobSerializer,\n        \"list\": JobListSerializer,\n    }\n    filterset_class = JobFilter\n    ordering_fields = [\n        \"received_request_time\",\n        \"finished_analysis_time\",\n        \"process_time\",\n    ]\n\n    def get_permissions(self):\n        \"\"\"\n        Customizes permissions based on the action being performed.\n\n        - For `destroy` and `kill` actions, adds `IsObjectUserOrSameOrgPermission` to ensure that only\n          the job owner or anyone in the same organization can perform these actions.\n\n        Returns:\n        - List of applicable permissions.\n        \"\"\"\n        permissions = super().get_permissions()\n        if self.action in [\"destroy\", \"kill\"]:\n            permissions.append(IsObjectUserOrSameOrgPermission())\n        return permissions\n\n    def get_queryset(self):\n        \"\"\"\n        Filters the queryset to include only jobs visible to the authenticated user, ordered by request time.\n\n        Logs the request parameters and returns the filtered queryset.\n\n        Returns:\n        - Filtered queryset of jobs.\n        \"\"\"\n        user = self.request.user\n        logger.info(\n            f\"user: {user} request the jobs with params: {self.request.query_params}\"\n        )\n        return Job.objects.visible_for_user(user).order_by(\"-received_request_time\")\n\n    @action(detail=False, methods=[\"post\"])\n    def recent_scans(self, request):\n        \"\"\"\n        Retrieve recent jobs based on an MD5 hash, filtered by a maximum temporal distance.\n\n        Expects the following parameters in the request data:\n        - `md5`: The MD5 hash to filter jobs by.\n        - `max_temporal_distance`: The maximum number of days to look back for recent jobs (default is 14 days).\n\n        Returns:\n        - List of recent jobs matching the MD5 hash.\n        \"\"\"\n        if \"md5\" not in request.data:\n            raise ValidationError({\"detail\": \"md5 is required\"})\n        max_temporal_distance = request.data.get(\"max_temporal_distance\", 14)\n        jobs = (\n            Job.objects.filter(md5=request.data[\"md5\"])\n            .visible_for_user(self.request.user)\n            .filter(\n                finished_analysis_time__gte=now()\n                - datetime.timedelta(days=max_temporal_distance)\n            )\n            .annotate_importance(request.user)\n            .order_by(\"-importance\", \"-finished_analysis_time\")\n        )\n        return Response(\n            JobRecentScanSerializer(jobs, many=True).data, status=status.HTTP_200_OK\n        )\n\n    @action(detail=False, methods=[\"post\"])\n    def recent_scans_user(self, request):\n        \"\"\"\n        Retrieve recent jobs for the authenticated user, filtered by sample status.\n\n        Expects the following parameters in the request data:\n        - `is_sample`: Whether to filter jobs by sample status (required).\n        - `limit`: The maximum number of recent jobs to return (default is 5).\n\n        Returns:\n        - List of recent jobs for the user.\n        \"\"\"\n        limit = request.data.get(\"limit\", 5)\n        if \"is_sample\" not in request.data:\n            raise ValidationError({\"detail\": \"is_sample is required\"})\n        jobs = (\n            Job.objects.filter(user__pk=request.user.pk)\n            .filter(is_sample=request.data[\"is_sample\"])\n            .annotate_importance(request.user)\n            .order_by(\"-importance\", \"-finished_analysis_time\")[:limit]\n        )\n        return Response(\n            JobRecentScanSerializer(jobs, many=True).data, status=status.HTTP_200_OK\n        )\n\n    @action(detail=True, methods=[\"patch\"])\n    def retry(self, request, pk=None):\n        \"\"\"\n        Retry a job if its status is in a final state.\n\n        If the job is currently running, raises a validation error.\n\n        Returns:\n        - No content (204) if the job is successfully retried.\n        \"\"\"\n        job = self.get_object()\n        if job.status not in Job.Status.final_statuses():\n            raise ValidationError({\"detail\": \"Job is running\"})\n        job.retry()\n        return Response(status=status.HTTP_204_NO_CONTENT)\n\n    @add_docs(\n        description=\"Kill running job by closing celery tasks and marking as killed\",\n        request=None,\n        responses={\n            204: None,\n        },\n    )\n    @action(detail=True, methods=[\"patch\"])\n    def kill(self, request, pk=None):\n        \"\"\"\n        Kill a running job by closing celery tasks and marking the job as killed.\n\n        If the job is not running, raises a validation error.\n\n        Returns:\n        - No content (204) if the job is successfully killed.\n        \"\"\"\n        # get job object or raise 404\n        job = self.get_object()\n\n        # check if job running\n        if job.status in Job.Status.final_statuses():\n            raise ValidationError({\"detail\": \"Job is not running\"})\n        # close celery tasks and mark reports as killed\n        job.kill_if_ongoing()\n        return Response(status=status.HTTP_204_NO_CONTENT)\n\n    @add_docs(\n        description=\"Download file/sample associated with a job\",\n        request=None,\n        responses={200: OpenApiTypes.BINARY, 400: None},\n    )\n    @action(detail=True, methods=[\"get\"])\n    def download_sample(self, request, pk=None):\n        \"\"\"\n        Download a sample associated with a job.\n\n        If the job does not have a sample, raises a validation error.\n\n        Returns:\n        - The file associated with the job as an attachment.\n\n        :param url: pk (job_id)\n        :returns: bytes\n        \"\"\"\n        # get job object\n        job = self.get_object()\n\n        # make sure it is a sample\n        if not job.is_sample:\n            raise ValidationError(\n                {\"detail\": \"Requested job does not have a sample associated with it.\"}\n            )\n        return FileResponse(\n            job.file,\n            filename=job.file_name,\n            content_type=job.file_mimetype,\n            as_attachment=True,\n        )\n\n    @add_docs(description=\"Pivot a job\")\n    @action(\n        detail=True, methods=[\"post\"]\n    )  # , url_path=\"pivot-(?P&lt;pivot_config_pk&gt;\\d+)\")\n    def pivot(self, request, pk=None, pivot_config_pk=None):\n        \"\"\"\n        Perform a pivot operation from a job's reports based on a specified pivot configuration.\n\n        Expects the following parameters:\n        - `pivot_config_pk`: The primary key of the pivot configuration to use.\n\n        Returns:\n        - List of job IDs created as a result of the pivot.\n        \"\"\"\n        starting_job = self.get_object()\n        try:\n            pivot_config: PivotConfig = PivotConfig.objects.get(pk=pivot_config_pk)\n        except PivotConfig.DoesNotExist:\n            raise ValidationError({\"detail\": \"Requested pivot config does not exist.\"})\n        else:\n            try:\n                pivots = pivot_config.pivot_job(starting_job.reports)\n            except KeyError:\n                msg = (\n                    f\"Unable to retrieve value at {self.field}\"\n                    f\" from job {starting_job.pk}\"\n                )\n                logger.error(msg)\n                raise ValidationError({\"detail\": msg})\n            except Exception as e:\n                logger.exception(e)\n                raise ValidationError(\n                    {\"detail\": f\"Unable to start pivot from job {starting_job.pk}\"}\n                )\n            else:\n                return Response(\n                    [pivot.ending_job.pk for pivot in pivots],\n                    status=status.HTTP_201_CREATED,\n                )\n\n    @action(\n        url_path=\"aggregate/status\",\n        detail=False,\n        methods=[\"GET\"],\n    )\n    @cache_action_response(timeout=60 * 5)\n    def aggregate_status(self, request):\n        \"\"\"\n        Aggregate jobs by their status.\n\n        Returns:\n        - Aggregated count of jobs for each status.\n        \"\"\"\n        annotations = {\n            key.lower(): Count(\"status\", filter=Q(status=key))\n            for key in Job.Status.values\n        }\n        return self.__aggregation_response_static(\n            annotations, users=self.get_org_members(request)\n        )\n\n    @action(\n        url_path=\"aggregate/type\",\n        detail=False,\n        methods=[\"GET\"],\n    )\n    @cache_action_response(timeout=60 * 5)\n    def aggregate_type(self, request):\n        \"\"\"\n        Aggregate jobs by type (file or observable).\n\n        Returns:\n        - Aggregated count of jobs for each type.\n        \"\"\"\n        annotations = {\n            \"file\": Count(\"is_sample\", filter=Q(is_sample=True)),\n            \"observable\": Count(\"is_sample\", filter=Q(is_sample=False)),\n        }\n        return self.__aggregation_response_static(\n            annotations, users=self.get_org_members(request)\n        )\n\n    @action(\n        url_path=\"aggregate/observable_classification\",\n        detail=False,\n        methods=[\"GET\"],\n    )\n    @cache_action_response(timeout=60 * 5)\n    def aggregate_observable_classification(self, request):\n        \"\"\"\n        Aggregate jobs by observable classification.\n\n        Returns:\n        - Aggregated count of jobs for each observable classification.\n        \"\"\"\n        annotations = {\n            oc.lower(): Count(\n                \"observable_classification\", filter=Q(observable_classification=oc)\n            )\n            for oc in ObservableTypes.values\n        }\n        return self.__aggregation_response_static(\n            annotations, users=self.get_org_members(request)\n        )\n\n    @action(\n        url_path=\"aggregate/file_mimetype\",\n        detail=False,\n        methods=[\"GET\"],\n    )\n    @cache_action_response(timeout=60 * 5)\n    def aggregate_file_mimetype(self, request):\n        \"\"\"\n        Aggregate jobs by file MIME type.\n\n        Returns:\n        - Aggregated count of jobs for each MIME type.\n        \"\"\"\n        return self.__aggregation_response_dynamic(\n            \"file_mimetype\", users=self.get_org_members(request)\n        )\n\n    @action(\n        url_path=\"aggregate/observable_name\",\n        detail=False,\n        methods=[\"GET\"],\n    )\n    @cache_action_response(timeout=60 * 5)\n    def aggregate_observable_name(self, request):\n        \"\"\"\n        Aggregate jobs by observable name.\n\n        Returns:\n        - Aggregated count of jobs for each observable name.\n        \"\"\"\n        return self.__aggregation_response_dynamic(\n            \"observable_name\", False, users=self.get_org_members(request)\n        )\n\n    @action(\n        url_path=\"aggregate/md5\",\n        detail=False,\n        methods=[\"GET\"],\n    )\n    @cache_action_response(timeout=60 * 5)\n    def aggregate_md5(self, request):\n        \"\"\"\n        Aggregate jobs by MD5 hash.\n\n        Returns:\n        - Aggregated count of jobs for each MD5 hash.\n        \"\"\"\n        # this is for file\n        return self.__aggregation_response_dynamic(\n            \"md5\", False, users=self.get_org_members(request)\n        )\n\n    @staticmethod\n    def get_org_members(request):\n        \"\"\"\n        Retrieve members of the organization associated with the authenticated user.\n\n        If the 'org' query parameter is set to 'true', this method returns all\n        users who are members of the authenticated user's organization.\n\n        Args:\n            request: The HTTP request object containing user information and query parameters.\n\n        Returns:\n            list or None: A list of users who are members of the user's organization\n            if the 'org' query parameter is 'true', otherwise None.\n        \"\"\"\n        user = request.user\n        org_param = request.GET.get(\"org\", \"\").lower() == \"true\"\n        users_of_organization = None\n        if org_param:\n            organization = user.membership.organization\n            users_of_organization = [\n                membership.user for membership in organization.members.all()\n            ]\n        return users_of_organization\n\n    def __aggregation_response_static(self, annotations: dict, users=None) -&gt; Response:\n        \"\"\"\n        Generate a static aggregation of Job objects filtered by a time range.\n\n        This method applies the provided annotations to aggregate Job objects\n        within the specified time range. Optionally, it filters the results by\n        the given list of users.\n\n        Args:\n            annotations (dict): Annotations to apply for the aggregation.\n            users (list, optional): A list of users to filter the Job objects by.\n\n        Returns:\n            Response: A Django REST framework Response object containing the aggregated data.\n        \"\"\"\n        delta, basis = self.__parse_range(self.request)\n        filter_kwargs = {\"received_request_time__gte\": delta}\n        if users:\n            filter_kwargs[\"user__in\"] = users\n        qs = (\n            Job.objects.filter(**filter_kwargs)\n            .annotate(date=Trunc(\"received_request_time\", basis))\n            .values(\"date\")\n            .annotate(**annotations)\n        )\n        return Response(qs)\n\n    def __aggregation_response_dynamic(\n        self,\n        field_name: str,\n        group_by_date: bool = True,\n        limit: int = 5,\n        users=None,\n    ) -&gt; Response:\n        \"\"\"\n        Dynamically aggregate Job objects based on a specified field and time range.\n\n        This method identifies the most frequent values of a given field within\n        a specified time range and aggregates the Job objects accordingly.\n        Optionally, it can group the results by date and limit the number of\n        most frequent values.\n\n        Args:\n            field_name (str): The name of the field to aggregate by.\n            group_by_date (bool, optional): Whether to group the results by date. Defaults to True.\n            limit (int, optional): The maximum number of most frequent values to retrieve. Defaults to 5.\n            users (list, optional): A list of users to filter the Job objects by.\n\n        Returns:\n            Response: A Django REST framework Response object containing the most frequent values\n            and the aggregated data.\n        \"\"\"\n        delta, basis = self.__parse_range(self.request)\n        filter_kwargs = {\"received_request_time__gte\": delta}\n        if users:\n            filter_kwargs[\"user__in\"] = users\n        if field_name == \"md5\":\n            filter_kwargs[\"is_sample\"] = True\n\n        most_frequent_values = (\n            Job.objects.filter(**filter_kwargs)\n            .exclude(**{f\"{field_name}__isnull\": True})\n            .exclude(**{f\"{field_name}__exact\": \"\"})\n            # excluding those because they could lead to SQL query errors\n            .exclude(\n                observable_classification__in=[\n                    ObservableClassification.URL,\n                    ObservableClassification.GENERIC,\n                ]\n            )\n            .annotate(count=Count(field_name))\n            .distinct()\n            .order_by(\"-count\")[:limit]\n            .values_list(field_name, flat=True)\n        )\n\n        logger.info(\n            f\"request: {field_name} found most_frequent_values: {most_frequent_values}\"\n        )\n\n        if len(most_frequent_values):\n            annotations = {\n                val: Count(field_name, filter=Q(**{field_name: val}))\n                for val in most_frequent_values\n            }\n            logger.debug(f\"request: {field_name} annotations: {annotations}\")\n            if group_by_date:\n                aggregation = (\n                    Job.objects.filter(**filter_kwargs)\n                    .annotate(date=Trunc(\"received_request_time\", basis))\n                    .values(\"date\")\n                    .annotate(**annotations)\n                )\n            else:\n                aggregation = Job.objects.filter(**filter_kwargs).aggregate(\n                    **annotations\n                )\n        else:\n            aggregation = {}\n\n        return Response(\n            {\n                \"values\": most_frequent_values,\n                \"aggregation\": aggregation,\n            }\n        )\n\n    @staticmethod\n    def __parse_range(request):\n        \"\"\"\n        Parse the time range from the request query parameters.\n\n        This method attempts to extract the 'range' query parameter from the\n        request. If the parameter is not provided, it defaults to '7d' (7 days).\n\n        Args:\n            request: The HTTP request object containing query parameters.\n\n        Returns:\n            tuple: A tuple containing the parsed time delta and the basis for date truncation.\n        \"\"\"\n        try:\n            range_str = request.GET[\"range\"]\n        except KeyError:\n            # default\n            range_str = \"7d\"\n\n        return parse_humanized_range(range_str)\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.__aggregation_response_dynamic","title":"<code>__aggregation_response_dynamic(field_name, group_by_date=True, limit=5, users=None)</code>","text":"<p>Dynamically aggregate Job objects based on a specified field and time range.</p> <p>This method identifies the most frequent values of a given field within a specified time range and aggregates the Job objects accordingly. Optionally, it can group the results by date and limit the number of most frequent values.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>The name of the field to aggregate by.</p> required <code>group_by_date</code> <code>bool</code> <p>Whether to group the results by date. Defaults to True.</p> <code>True</code> <code>limit</code> <code>int</code> <p>The maximum number of most frequent values to retrieve. Defaults to 5.</p> <code>5</code> <code>users</code> <code>list</code> <p>A list of users to filter the Job objects by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>A Django REST framework Response object containing the most frequent values</p> <code>Response</code> <p>and the aggregated data.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>def __aggregation_response_dynamic(\n    self,\n    field_name: str,\n    group_by_date: bool = True,\n    limit: int = 5,\n    users=None,\n) -&gt; Response:\n    \"\"\"\n    Dynamically aggregate Job objects based on a specified field and time range.\n\n    This method identifies the most frequent values of a given field within\n    a specified time range and aggregates the Job objects accordingly.\n    Optionally, it can group the results by date and limit the number of\n    most frequent values.\n\n    Args:\n        field_name (str): The name of the field to aggregate by.\n        group_by_date (bool, optional): Whether to group the results by date. Defaults to True.\n        limit (int, optional): The maximum number of most frequent values to retrieve. Defaults to 5.\n        users (list, optional): A list of users to filter the Job objects by.\n\n    Returns:\n        Response: A Django REST framework Response object containing the most frequent values\n        and the aggregated data.\n    \"\"\"\n    delta, basis = self.__parse_range(self.request)\n    filter_kwargs = {\"received_request_time__gte\": delta}\n    if users:\n        filter_kwargs[\"user__in\"] = users\n    if field_name == \"md5\":\n        filter_kwargs[\"is_sample\"] = True\n\n    most_frequent_values = (\n        Job.objects.filter(**filter_kwargs)\n        .exclude(**{f\"{field_name}__isnull\": True})\n        .exclude(**{f\"{field_name}__exact\": \"\"})\n        # excluding those because they could lead to SQL query errors\n        .exclude(\n            observable_classification__in=[\n                ObservableClassification.URL,\n                ObservableClassification.GENERIC,\n            ]\n        )\n        .annotate(count=Count(field_name))\n        .distinct()\n        .order_by(\"-count\")[:limit]\n        .values_list(field_name, flat=True)\n    )\n\n    logger.info(\n        f\"request: {field_name} found most_frequent_values: {most_frequent_values}\"\n    )\n\n    if len(most_frequent_values):\n        annotations = {\n            val: Count(field_name, filter=Q(**{field_name: val}))\n            for val in most_frequent_values\n        }\n        logger.debug(f\"request: {field_name} annotations: {annotations}\")\n        if group_by_date:\n            aggregation = (\n                Job.objects.filter(**filter_kwargs)\n                .annotate(date=Trunc(\"received_request_time\", basis))\n                .values(\"date\")\n                .annotate(**annotations)\n            )\n        else:\n            aggregation = Job.objects.filter(**filter_kwargs).aggregate(\n                **annotations\n            )\n    else:\n        aggregation = {}\n\n    return Response(\n        {\n            \"values\": most_frequent_values,\n            \"aggregation\": aggregation,\n        }\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.__aggregation_response_static","title":"<code>__aggregation_response_static(annotations, users=None)</code>","text":"<p>Generate a static aggregation of Job objects filtered by a time range.</p> <p>This method applies the provided annotations to aggregate Job objects within the specified time range. Optionally, it filters the results by the given list of users.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>dict</code> <p>Annotations to apply for the aggregation.</p> required <code>users</code> <code>list</code> <p>A list of users to filter the Job objects by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>A Django REST framework Response object containing the aggregated data.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>def __aggregation_response_static(self, annotations: dict, users=None) -&gt; Response:\n    \"\"\"\n    Generate a static aggregation of Job objects filtered by a time range.\n\n    This method applies the provided annotations to aggregate Job objects\n    within the specified time range. Optionally, it filters the results by\n    the given list of users.\n\n    Args:\n        annotations (dict): Annotations to apply for the aggregation.\n        users (list, optional): A list of users to filter the Job objects by.\n\n    Returns:\n        Response: A Django REST framework Response object containing the aggregated data.\n    \"\"\"\n    delta, basis = self.__parse_range(self.request)\n    filter_kwargs = {\"received_request_time__gte\": delta}\n    if users:\n        filter_kwargs[\"user__in\"] = users\n    qs = (\n        Job.objects.filter(**filter_kwargs)\n        .annotate(date=Trunc(\"received_request_time\", basis))\n        .values(\"date\")\n        .annotate(**annotations)\n    )\n    return Response(qs)\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.__parse_range","title":"<code>__parse_range(request)</code>  <code>staticmethod</code>","text":"<p>Parse the time range from the request query parameters.</p> <p>This method attempts to extract the 'range' query parameter from the request. If the parameter is not provided, it defaults to '7d' (7 days).</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The HTTP request object containing query parameters.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the parsed time delta and the basis for date truncation.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@staticmethod\ndef __parse_range(request):\n    \"\"\"\n    Parse the time range from the request query parameters.\n\n    This method attempts to extract the 'range' query parameter from the\n    request. If the parameter is not provided, it defaults to '7d' (7 days).\n\n    Args:\n        request: The HTTP request object containing query parameters.\n\n    Returns:\n        tuple: A tuple containing the parsed time delta and the basis for date truncation.\n    \"\"\"\n    try:\n        range_str = request.GET[\"range\"]\n    except KeyError:\n        # default\n        range_str = \"7d\"\n\n    return parse_humanized_range(range_str)\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.aggregate_file_mimetype","title":"<code>aggregate_file_mimetype(request)</code>","text":"<p>Aggregate jobs by file MIME type.</p> <p>Returns: - Aggregated count of jobs for each MIME type.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@action(\n    url_path=\"aggregate/file_mimetype\",\n    detail=False,\n    methods=[\"GET\"],\n)\n@cache_action_response(timeout=60 * 5)\ndef aggregate_file_mimetype(self, request):\n    \"\"\"\n    Aggregate jobs by file MIME type.\n\n    Returns:\n    - Aggregated count of jobs for each MIME type.\n    \"\"\"\n    return self.__aggregation_response_dynamic(\n        \"file_mimetype\", users=self.get_org_members(request)\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.aggregate_md5","title":"<code>aggregate_md5(request)</code>","text":"<p>Aggregate jobs by MD5 hash.</p> <p>Returns: - Aggregated count of jobs for each MD5 hash.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@action(\n    url_path=\"aggregate/md5\",\n    detail=False,\n    methods=[\"GET\"],\n)\n@cache_action_response(timeout=60 * 5)\ndef aggregate_md5(self, request):\n    \"\"\"\n    Aggregate jobs by MD5 hash.\n\n    Returns:\n    - Aggregated count of jobs for each MD5 hash.\n    \"\"\"\n    # this is for file\n    return self.__aggregation_response_dynamic(\n        \"md5\", False, users=self.get_org_members(request)\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.aggregate_observable_classification","title":"<code>aggregate_observable_classification(request)</code>","text":"<p>Aggregate jobs by observable classification.</p> <p>Returns: - Aggregated count of jobs for each observable classification.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@action(\n    url_path=\"aggregate/observable_classification\",\n    detail=False,\n    methods=[\"GET\"],\n)\n@cache_action_response(timeout=60 * 5)\ndef aggregate_observable_classification(self, request):\n    \"\"\"\n    Aggregate jobs by observable classification.\n\n    Returns:\n    - Aggregated count of jobs for each observable classification.\n    \"\"\"\n    annotations = {\n        oc.lower(): Count(\n            \"observable_classification\", filter=Q(observable_classification=oc)\n        )\n        for oc in ObservableTypes.values\n    }\n    return self.__aggregation_response_static(\n        annotations, users=self.get_org_members(request)\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.aggregate_observable_name","title":"<code>aggregate_observable_name(request)</code>","text":"<p>Aggregate jobs by observable name.</p> <p>Returns: - Aggregated count of jobs for each observable name.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@action(\n    url_path=\"aggregate/observable_name\",\n    detail=False,\n    methods=[\"GET\"],\n)\n@cache_action_response(timeout=60 * 5)\ndef aggregate_observable_name(self, request):\n    \"\"\"\n    Aggregate jobs by observable name.\n\n    Returns:\n    - Aggregated count of jobs for each observable name.\n    \"\"\"\n    return self.__aggregation_response_dynamic(\n        \"observable_name\", False, users=self.get_org_members(request)\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.aggregate_status","title":"<code>aggregate_status(request)</code>","text":"<p>Aggregate jobs by their status.</p> <p>Returns: - Aggregated count of jobs for each status.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@action(\n    url_path=\"aggregate/status\",\n    detail=False,\n    methods=[\"GET\"],\n)\n@cache_action_response(timeout=60 * 5)\ndef aggregate_status(self, request):\n    \"\"\"\n    Aggregate jobs by their status.\n\n    Returns:\n    - Aggregated count of jobs for each status.\n    \"\"\"\n    annotations = {\n        key.lower(): Count(\"status\", filter=Q(status=key))\n        for key in Job.Status.values\n    }\n    return self.__aggregation_response_static(\n        annotations, users=self.get_org_members(request)\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.aggregate_type","title":"<code>aggregate_type(request)</code>","text":"<p>Aggregate jobs by type (file or observable).</p> <p>Returns: - Aggregated count of jobs for each type.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@action(\n    url_path=\"aggregate/type\",\n    detail=False,\n    methods=[\"GET\"],\n)\n@cache_action_response(timeout=60 * 5)\ndef aggregate_type(self, request):\n    \"\"\"\n    Aggregate jobs by type (file or observable).\n\n    Returns:\n    - Aggregated count of jobs for each type.\n    \"\"\"\n    annotations = {\n        \"file\": Count(\"is_sample\", filter=Q(is_sample=True)),\n        \"observable\": Count(\"is_sample\", filter=Q(is_sample=False)),\n    }\n    return self.__aggregation_response_static(\n        annotations, users=self.get_org_members(request)\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.download_sample","title":"<code>download_sample(request, pk=None)</code>","text":"<p>Download a sample associated with a job.</p> <p>If the job does not have a sample, raises a validation error.</p> <p>Returns: - The file associated with the job as an attachment.</p> <p>:param url: pk (job_id) :returns: bytes</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"Download file/sample associated with a job\",\n    request=None,\n    responses={200: OpenApiTypes.BINARY, 400: None},\n)\n@action(detail=True, methods=[\"get\"])\ndef download_sample(self, request, pk=None):\n    \"\"\"\n    Download a sample associated with a job.\n\n    If the job does not have a sample, raises a validation error.\n\n    Returns:\n    - The file associated with the job as an attachment.\n\n    :param url: pk (job_id)\n    :returns: bytes\n    \"\"\"\n    # get job object\n    job = self.get_object()\n\n    # make sure it is a sample\n    if not job.is_sample:\n        raise ValidationError(\n            {\"detail\": \"Requested job does not have a sample associated with it.\"}\n        )\n    return FileResponse(\n        job.file,\n        filename=job.file_name,\n        content_type=job.file_mimetype,\n        as_attachment=True,\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.get_org_members","title":"<code>get_org_members(request)</code>  <code>staticmethod</code>","text":"<p>Retrieve members of the organization associated with the authenticated user.</p> <p>If the 'org' query parameter is set to 'true', this method returns all users who are members of the authenticated user's organization.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <p>The HTTP request object containing user information and query parameters.</p> required <p>Returns:</p> Type Description <p>list or None: A list of users who are members of the user's organization</p> <p>if the 'org' query parameter is 'true', otherwise None.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@staticmethod\ndef get_org_members(request):\n    \"\"\"\n    Retrieve members of the organization associated with the authenticated user.\n\n    If the 'org' query parameter is set to 'true', this method returns all\n    users who are members of the authenticated user's organization.\n\n    Args:\n        request: The HTTP request object containing user information and query parameters.\n\n    Returns:\n        list or None: A list of users who are members of the user's organization\n        if the 'org' query parameter is 'true', otherwise None.\n    \"\"\"\n    user = request.user\n    org_param = request.GET.get(\"org\", \"\").lower() == \"true\"\n    users_of_organization = None\n    if org_param:\n        organization = user.membership.organization\n        users_of_organization = [\n            membership.user for membership in organization.members.all()\n        ]\n    return users_of_organization\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.get_permissions","title":"<code>get_permissions()</code>","text":"<p>Customizes permissions based on the action being performed.</p> <ul> <li>For <code>destroy</code> and <code>kill</code> actions, adds <code>IsObjectUserOrSameOrgPermission</code> to ensure that only   the job owner or anyone in the same organization can perform these actions.</li> </ul> <p>Returns: - List of applicable permissions.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>def get_permissions(self):\n    \"\"\"\n    Customizes permissions based on the action being performed.\n\n    - For `destroy` and `kill` actions, adds `IsObjectUserOrSameOrgPermission` to ensure that only\n      the job owner or anyone in the same organization can perform these actions.\n\n    Returns:\n    - List of applicable permissions.\n    \"\"\"\n    permissions = super().get_permissions()\n    if self.action in [\"destroy\", \"kill\"]:\n        permissions.append(IsObjectUserOrSameOrgPermission())\n    return permissions\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Filters the queryset to include only jobs visible to the authenticated user, ordered by request time.</p> <p>Logs the request parameters and returns the filtered queryset.</p> <p>Returns: - Filtered queryset of jobs.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>def get_queryset(self):\n    \"\"\"\n    Filters the queryset to include only jobs visible to the authenticated user, ordered by request time.\n\n    Logs the request parameters and returns the filtered queryset.\n\n    Returns:\n    - Filtered queryset of jobs.\n    \"\"\"\n    user = self.request.user\n    logger.info(\n        f\"user: {user} request the jobs with params: {self.request.query_params}\"\n    )\n    return Job.objects.visible_for_user(user).order_by(\"-received_request_time\")\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.kill","title":"<code>kill(request, pk=None)</code>","text":"<p>Kill a running job by closing celery tasks and marking the job as killed.</p> <p>If the job is not running, raises a validation error.</p> <p>Returns: - No content (204) if the job is successfully killed.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"Kill running job by closing celery tasks and marking as killed\",\n    request=None,\n    responses={\n        204: None,\n    },\n)\n@action(detail=True, methods=[\"patch\"])\ndef kill(self, request, pk=None):\n    \"\"\"\n    Kill a running job by closing celery tasks and marking the job as killed.\n\n    If the job is not running, raises a validation error.\n\n    Returns:\n    - No content (204) if the job is successfully killed.\n    \"\"\"\n    # get job object or raise 404\n    job = self.get_object()\n\n    # check if job running\n    if job.status in Job.Status.final_statuses():\n        raise ValidationError({\"detail\": \"Job is not running\"})\n    # close celery tasks and mark reports as killed\n    job.kill_if_ongoing()\n    return Response(status=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.pivot","title":"<code>pivot(request, pk=None, pivot_config_pk=None)</code>","text":"<p>Perform a pivot operation from a job's reports based on a specified pivot configuration.</p> <p>Expects the following parameters: - <code>pivot_config_pk</code>: The primary key of the pivot configuration to use.</p> <p>Returns: - List of job IDs created as a result of the pivot.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(description=\"Pivot a job\")\n@action(\n    detail=True, methods=[\"post\"]\n)  # , url_path=\"pivot-(?P&lt;pivot_config_pk&gt;\\d+)\")\ndef pivot(self, request, pk=None, pivot_config_pk=None):\n    \"\"\"\n    Perform a pivot operation from a job's reports based on a specified pivot configuration.\n\n    Expects the following parameters:\n    - `pivot_config_pk`: The primary key of the pivot configuration to use.\n\n    Returns:\n    - List of job IDs created as a result of the pivot.\n    \"\"\"\n    starting_job = self.get_object()\n    try:\n        pivot_config: PivotConfig = PivotConfig.objects.get(pk=pivot_config_pk)\n    except PivotConfig.DoesNotExist:\n        raise ValidationError({\"detail\": \"Requested pivot config does not exist.\"})\n    else:\n        try:\n            pivots = pivot_config.pivot_job(starting_job.reports)\n        except KeyError:\n            msg = (\n                f\"Unable to retrieve value at {self.field}\"\n                f\" from job {starting_job.pk}\"\n            )\n            logger.error(msg)\n            raise ValidationError({\"detail\": msg})\n        except Exception as e:\n            logger.exception(e)\n            raise ValidationError(\n                {\"detail\": f\"Unable to start pivot from job {starting_job.pk}\"}\n            )\n        else:\n            return Response(\n                [pivot.ending_job.pk for pivot in pivots],\n                status=status.HTTP_201_CREATED,\n            )\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.recent_scans","title":"<code>recent_scans(request)</code>","text":"<p>Retrieve recent jobs based on an MD5 hash, filtered by a maximum temporal distance.</p> <p>Expects the following parameters in the request data: - <code>md5</code>: The MD5 hash to filter jobs by. - <code>max_temporal_distance</code>: The maximum number of days to look back for recent jobs (default is 14 days).</p> <p>Returns: - List of recent jobs matching the MD5 hash.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@action(detail=False, methods=[\"post\"])\ndef recent_scans(self, request):\n    \"\"\"\n    Retrieve recent jobs based on an MD5 hash, filtered by a maximum temporal distance.\n\n    Expects the following parameters in the request data:\n    - `md5`: The MD5 hash to filter jobs by.\n    - `max_temporal_distance`: The maximum number of days to look back for recent jobs (default is 14 days).\n\n    Returns:\n    - List of recent jobs matching the MD5 hash.\n    \"\"\"\n    if \"md5\" not in request.data:\n        raise ValidationError({\"detail\": \"md5 is required\"})\n    max_temporal_distance = request.data.get(\"max_temporal_distance\", 14)\n    jobs = (\n        Job.objects.filter(md5=request.data[\"md5\"])\n        .visible_for_user(self.request.user)\n        .filter(\n            finished_analysis_time__gte=now()\n            - datetime.timedelta(days=max_temporal_distance)\n        )\n        .annotate_importance(request.user)\n        .order_by(\"-importance\", \"-finished_analysis_time\")\n    )\n    return Response(\n        JobRecentScanSerializer(jobs, many=True).data, status=status.HTTP_200_OK\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.recent_scans_user","title":"<code>recent_scans_user(request)</code>","text":"<p>Retrieve recent jobs for the authenticated user, filtered by sample status.</p> <p>Expects the following parameters in the request data: - <code>is_sample</code>: Whether to filter jobs by sample status (required). - <code>limit</code>: The maximum number of recent jobs to return (default is 5).</p> <p>Returns: - List of recent jobs for the user.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@action(detail=False, methods=[\"post\"])\ndef recent_scans_user(self, request):\n    \"\"\"\n    Retrieve recent jobs for the authenticated user, filtered by sample status.\n\n    Expects the following parameters in the request data:\n    - `is_sample`: Whether to filter jobs by sample status (required).\n    - `limit`: The maximum number of recent jobs to return (default is 5).\n\n    Returns:\n    - List of recent jobs for the user.\n    \"\"\"\n    limit = request.data.get(\"limit\", 5)\n    if \"is_sample\" not in request.data:\n        raise ValidationError({\"detail\": \"is_sample is required\"})\n    jobs = (\n        Job.objects.filter(user__pk=request.user.pk)\n        .filter(is_sample=request.data[\"is_sample\"])\n        .annotate_importance(request.user)\n        .order_by(\"-importance\", \"-finished_analysis_time\")[:limit]\n    )\n    return Response(\n        JobRecentScanSerializer(jobs, many=True).data, status=status.HTTP_200_OK\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.JobViewSet.retry","title":"<code>retry(request, pk=None)</code>","text":"<p>Retry a job if its status is in a final state.</p> <p>If the job is currently running, raises a validation error.</p> <p>Returns: - No content (204) if the job is successfully retried.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@action(detail=True, methods=[\"patch\"])\ndef retry(self, request, pk=None):\n    \"\"\"\n    Retry a job if its status is in a final state.\n\n    If the job is currently running, raises a validation error.\n\n    Returns:\n    - No content (204) if the job is successfully retried.\n    \"\"\"\n    job = self.get_object()\n    if job.status not in Job.Status.final_statuses():\n        raise ValidationError({\"detail\": \"Job is running\"})\n    job.retry()\n    return Response(status=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"IntelOwl/api_docs/#tagviewset","title":"<code>TagViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> <p>A viewset that provides CRUD (Create, Read, Update, Delete) operations for the <code>Tag</code> model.</p> <p>This viewset leverages Django REST framework's <code>ModelViewSet</code> to handle requests for the <code>Tag</code> model. It includes the default implementations for <code>list</code>, <code>retrieve</code>, <code>create</code>, <code>update</code>, <code>partial_update</code>, and <code>destroy</code> actions.</p> <p>Attributes:</p> Name Type Description <code>queryset</code> <code>QuerySet</code> <p>The queryset that retrieves all Tag objects from the database.</p> <code>serializer_class</code> <code>Serializer</code> <p>The serializer class used to convert Tag model instances to JSON and vice versa.</p> <code>pagination_class</code> <p>Pagination is disabled for this viewset.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"\n    REST endpoint to perform CRUD operations on ``Tag`` model.\n    Requires authentication.\n    \"\"\"\n)\nclass TagViewSet(viewsets.ModelViewSet):\n    \"\"\"\n    A viewset that provides CRUD (Create, Read, Update, Delete) operations\n    for the ``Tag`` model.\n\n    This viewset leverages Django REST framework's `ModelViewSet` to handle\n    requests for the `Tag` model. It includes the default implementations\n    for `list`, `retrieve`, `create`, `update`, `partial_update`, and `destroy` actions.\n\n    Attributes:\n        queryset (QuerySet): The queryset that retrieves all Tag objects from the database.\n        serializer_class (Serializer): The serializer class used to convert Tag model instances to JSON and vice versa.\n        pagination_class: Pagination is disabled for this viewset.\n    \"\"\"\n\n    queryset = Tag.objects.all()\n    serializer_class = TagSerializer\n    pagination_class = None\n</code></pre>"},{"location":"IntelOwl/api_docs/#modelwithownershipviewset","title":"<code>ModelWithOwnershipViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> <p>A viewset that enforces ownership-based access control for models.</p> <p>This class extends the functionality of <code>ModelViewSet</code> to restrict access to objects based on ownership. It modifies the queryset for the <code>list</code> action to only include objects visible to the requesting user, and adds custom permission checks for <code>destroy</code> and <code>update</code> actions.</p> <p>Methods:</p> Name Description <code>get_queryset</code> <p>Returns the queryset of the model, filtered for visibility             to the requesting user during the <code>list</code> action.</p> <code>get_permissions</code> <p>Returns the permissions required for the current action,                with additional checks for ownership during <code>destroy</code>                and <code>update</code> actions. Raises <code>PermissionDenied</code> for <code>PUT</code> requests.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>class ModelWithOwnershipViewSet(viewsets.ModelViewSet):\n    \"\"\"\n    A viewset that enforces ownership-based access control for models.\n\n    This class extends the functionality of `ModelViewSet` to restrict access to\n    objects based on ownership. It modifies the queryset for the `list` action\n    to only include objects visible to the requesting user, and adds custom\n    permission checks for `destroy` and `update` actions.\n\n    Methods:\n        get_queryset(): Returns the queryset of the model, filtered for visibility\n                        to the requesting user during the `list` action.\n        get_permissions(): Returns the permissions required for the current action,\n                           with additional checks for ownership during `destroy`\n                           and `update` actions. Raises `PermissionDenied` for `PUT` requests.\n    \"\"\"\n\n    def get_queryset(self):\n        \"\"\"\n        Retrieves the queryset for the viewset, modifying it for the `list` action\n        to only include objects visible to the requesting user.\n\n        Returns:\n            QuerySet: The queryset of the model, possibly filtered for visibility.\n        \"\"\"\n        qs = super().get_queryset()\n        if self.action == \"list\":\n            return qs.visible_for_user(self.request.user)\n        return qs\n\n    def get_permissions(self):\n        \"\"\"\n        Retrieves the permissions required for the current action.\n\n        For the `destroy` and `update` actions, additional checks are performed to\n        ensure that only object owners or admins can perform these actions. Raises\n        a `PermissionDenied` exception for `PUT` requests.\n\n        Returns:\n            list: A list of permission instances.\n        \"\"\"\n        permissions = super().get_permissions()\n        if self.action in [\"destroy\", \"update\"]:\n            if self.request.method == \"PUT\":\n                raise PermissionDenied()\n            # code quality checker marks this as error, but it works correctly\n            permissions.append(\n                (  # skipcq: PYL-E1102\n                    IsObjectAdminPermission | IsObjectOwnerPermission\n                )()\n            )\n\n        return permissions\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.ModelWithOwnershipViewSet.get_permissions","title":"<code>get_permissions()</code>","text":"<p>Retrieves the permissions required for the current action.</p> <p>For the <code>destroy</code> and <code>update</code> actions, additional checks are performed to ensure that only object owners or admins can perform these actions. Raises a <code>PermissionDenied</code> exception for <code>PUT</code> requests.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of permission instances.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>def get_permissions(self):\n    \"\"\"\n    Retrieves the permissions required for the current action.\n\n    For the `destroy` and `update` actions, additional checks are performed to\n    ensure that only object owners or admins can perform these actions. Raises\n    a `PermissionDenied` exception for `PUT` requests.\n\n    Returns:\n        list: A list of permission instances.\n    \"\"\"\n    permissions = super().get_permissions()\n    if self.action in [\"destroy\", \"update\"]:\n        if self.request.method == \"PUT\":\n            raise PermissionDenied()\n        # code quality checker marks this as error, but it works correctly\n        permissions.append(\n            (  # skipcq: PYL-E1102\n                IsObjectAdminPermission | IsObjectOwnerPermission\n            )()\n        )\n\n    return permissions\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.ModelWithOwnershipViewSet.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Retrieves the queryset for the viewset, modifying it for the <code>list</code> action to only include objects visible to the requesting user.</p> <p>Returns:</p> Name Type Description <code>QuerySet</code> <p>The queryset of the model, possibly filtered for visibility.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>def get_queryset(self):\n    \"\"\"\n    Retrieves the queryset for the viewset, modifying it for the `list` action\n    to only include objects visible to the requesting user.\n\n    Returns:\n        QuerySet: The queryset of the model, possibly filtered for visibility.\n    \"\"\"\n    qs = super().get_queryset()\n    if self.action == \"list\":\n        return qs.visible_for_user(self.request.user)\n    return qs\n</code></pre>"},{"location":"IntelOwl/api_docs/#pluginconfigviewset","title":"<code>PluginConfigViewSet</code>","text":"<p>               Bases: <code>ModelWithOwnershipViewSet</code></p> <p>A viewset for managing <code>PluginConfig</code> objects with ownership-based access control.</p> <p>This viewset extends <code>ModelWithOwnershipViewSet</code> to handle <code>PluginConfig</code> objects, allowing users to list, retrieve, and delete configurations while ensuring that only authorized configurations are accessible. It customizes the queryset to exclude default values and orders the configurations by ID.</p> <p>Attributes:</p> Name Type Description <code>serializer_class</code> <code>class</code> <p>The serializer class used for <code>PluginConfig</code> objects.</p> <code>pagination_class</code> <code>class</code> <p>Specifies that pagination is not applied.</p> <code>queryset</code> <code>QuerySet</code> <p>The queryset for <code>PluginConfig</code> objects, initially set to all objects.</p> <p>Methods:</p> Name Description <code>get_queryset</code> <p>Returns the queryset for <code>PluginConfig</code> objects, excluding default values             (where the owner is <code>NULL</code>) and ordering the remaining objects by ID.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"\n    REST endpoint to fetch list of PluginConfig or retrieve/delete a CustomConfig.\n    Requires authentication. Allows access to only authorized CustomConfigs.\n    \"\"\"\n)\nclass PluginConfigViewSet(ModelWithOwnershipViewSet):\n    \"\"\"\n    A viewset for managing `PluginConfig` objects with ownership-based access control.\n\n    This viewset extends `ModelWithOwnershipViewSet` to handle `PluginConfig` objects,\n    allowing users to list, retrieve, and delete configurations while ensuring that only\n    authorized configurations are accessible. It customizes the queryset to exclude default\n    values and orders the configurations by ID.\n\n    Attributes:\n        serializer_class (class): The serializer class used for `PluginConfig` objects.\n        pagination_class (class): Specifies that pagination is not applied.\n        queryset (QuerySet): The queryset for `PluginConfig` objects, initially set to all objects.\n\n    Methods:\n        get_queryset(): Returns the queryset for `PluginConfig` objects, excluding default values\n                        (where the owner is `NULL`) and ordering the remaining objects by ID.\n    \"\"\"\n\n    serializer_class = PluginConfigSerializer\n    pagination_class = None\n    queryset = PluginConfig.objects.all()\n\n    def get_queryset(self):\n        \"\"\"\n        Retrieves the queryset for `PluginConfig` objects, excluding those with default values\n        (where the owner is `NULL`) and ordering the remaining objects by ID.\n\n        Returns:\n            QuerySet: The filtered and ordered queryset of `PluginConfig` objects.\n        \"\"\"\n        # the .exclude is to remove the default values\n        return super().get_queryset().exclude(owner__isnull=True).order_by(\"id\")\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.PluginConfigViewSet.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Retrieves the queryset for <code>PluginConfig</code> objects, excluding those with default values (where the owner is <code>NULL</code>) and ordering the remaining objects by ID.</p> <p>Returns:</p> Name Type Description <code>QuerySet</code> <p>The filtered and ordered queryset of <code>PluginConfig</code> objects.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>def get_queryset(self):\n    \"\"\"\n    Retrieves the queryset for `PluginConfig` objects, excluding those with default values\n    (where the owner is `NULL`) and ordering the remaining objects by ID.\n\n    Returns:\n        QuerySet: The filtered and ordered queryset of `PluginConfig` objects.\n    \"\"\"\n    # the .exclude is to remove the default values\n    return super().get_queryset().exclude(owner__isnull=True).order_by(\"id\")\n</code></pre>"},{"location":"IntelOwl/api_docs/#pythonreportactionviewset","title":"<code>PythonReportActionViewSet</code>","text":"<p>               Bases: <code>GenericViewSet</code></p> <p>A base view set for handling actions related to plugin reports.</p> <p>This view set provides methods for killing and retrying plugin reports, and requires users to have appropriate permissions based on the <code>IsObjectUserOrSameOrgPermission</code>.</p> <p>Attributes:</p> Name Type Description <code>permission_classes</code> <code>list</code> <p>List of permission classes to apply.</p> <p>Methods: get_queryset: Returns the queryset of reports based on the model class. get_object: Retrieves a specific report object by job_id and report_id. perform_kill: Kills a running plugin by terminating its Celery task and marking it as killed. perform_retry: Retries a failed or killed plugin run. kill: Handles the endpoint to kill a specific report. retry: Handles the endpoint to retry a specific report.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>class PythonReportActionViewSet(viewsets.GenericViewSet, metaclass=ABCMeta):\n    \"\"\"\n    A base view set for handling actions related to plugin reports.\n\n    This view set provides methods for killing and retrying plugin reports,\n    and requires users to have appropriate permissions based on the\n    `IsObjectUserOrSameOrgPermission`.\n\n    Attributes:\n        permission_classes (list): List of permission classes to apply.\n\n    Methods:\n    get_queryset: Returns the queryset of reports based on the model class.\n    get_object: Retrieves a specific report object by job_id and report_id.\n    perform_kill: Kills a running plugin by terminating its Celery task and marking it as killed.\n    perform_retry: Retries a failed or killed plugin run.\n    kill: Handles the endpoint to kill a specific report.\n    retry: Handles the endpoint to retry a specific report.\n\n    \"\"\"\n\n    permission_classes = [\n        IsObjectUserOrSameOrgPermission,\n    ]\n\n    @classmethod\n    @property\n    @abstractmethod\n    def report_model(cls):\n        \"\"\"\n        Abstract property that should return the model class for the report.\n\n        Subclasses must implement this property to specify the model\n        class for the reports being handled by this view set.\n\n        Returns:\n            Type[AbstractReport]: The model class for the report.\n\n        Raises:\n            NotImplementedError: If not overridden by a subclass.\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_queryset(self):\n        \"\"\"\n        Returns the queryset of reports based on the model class.\n\n        Filters the queryset to return all instances of the report model.\n\n        Returns:\n            QuerySet: A queryset of all report instances.\n        \"\"\"\n        return self.report_model.objects.all()\n\n    def get_object(self, job_id: int, report_id: int) -&gt; AbstractReport:\n        \"\"\"\n        Retrieves a specific report object by job_id and report_id.\n\n        Overrides the drf's default `get_object` method to fetch a report object\n        based on job_id and report_id, and checks the permissions for the object.\n\n        Args:\n            job_id (int): The ID of the job associated with the report.\n            report_id (int): The ID of the report.\n\n        Returns:\n            AbstractReport: The report object.\n\n        Raises:\n            NotFound: If the report does not exist.\n        \"\"\"\n        try:\n            obj = self.report_model.objects.get(\n                job_id=job_id,\n                pk=report_id,\n            )\n        except self.report_model.DoesNotExist:\n            raise NotFound()\n        else:\n            self.check_object_permissions(self.request, obj)\n            return obj\n\n    @staticmethod\n    def perform_kill(report: AbstractReport):\n        \"\"\"\n        Kills a running plugin by terminating its Celery task and marking it as killed.\n\n        This method is a callback for performing additional actions after a\n        kill operation, including updating the report status and cleaning up\n        the associated job.\n\n        Args:\n            report (AbstractReport): The report to be killed.\n        \"\"\"\n        # kill celery task\n        celery_app.control.revoke(report.task_id, terminate=True)\n        # update report\n        report.status = AbstractReport.Status.KILLED\n        report.save(update_fields=[\"status\"])\n        # clean up job\n\n        job = Job.objects.get(pk=report.job.pk)\n        job.set_final_status()\n        JobConsumer.serialize_and_send_job(job)\n\n    @staticmethod\n    def perform_retry(report: AbstractReport):\n        \"\"\"\n        Retries a failed or killed plugin run.\n\n        This method clears the errors and re-runs the plugin with the same arguments.\n        It fetches the appropriate task signature and schedules the job again.\n\n        Args:\n            report (AbstractReport): The report to be retried.\n\n        Raises:\n            RuntimeError: If unable to find a valid task signature for the report.\n        \"\"\"\n        report.errors.clear()\n        report.save(update_fields=[\"errors\"])\n        try:\n            signature = next(\n                report.config.__class__.objects.filter(pk=report.config.pk)\n                .annotate_runnable(report.job.user)\n                .get_signatures(\n                    report.job,\n                )\n            )\n        except StopIteration:\n            raise RuntimeError(f\"Unable to find signature for report {report.pk}\")\n        runner = signature | tasks.job_set_final_status.signature(\n            args=[report.job.id],\n            kwargs={},\n            queue=report.config.queue,\n            immutable=True,\n            MessageGroupId=str(uuid.uuid4()),\n            priority=report.job.priority,\n        )\n        runner()\n\n    @add_docs(\n        description=\"Kill running plugin by closing celery task and marking as killed\",\n        request=None,\n        responses={\n            204: None,\n        },\n    )\n    @action(detail=False, methods=[\"patch\"])\n    def kill(self, request, job_id, report_id):\n        \"\"\"\n        Kills a specific report by terminating its Celery task and marking it as killed.\n\n        This endpoint handles the patch request to kill a report if its status is\n        running or pending.\n\n        Args:\n            request (HttpRequest): The request object containing the HTTP PATCH request.\n            job_id (int): The ID of the job associated with the report.\n            report_id (int): The ID of the report.\n\n        Returns:\n            Response: HTTP 204 No Content if successful.\n\n        Raises:\n            ValidationError: If the report is not in a valid state for killing.\n        \"\"\"\n        logger.info(\n            f\"kill request from user {request.user}\"\n            f\" for job_id {job_id}, pk {report_id}\"\n        )\n        # get report object or raise 404\n        report = self.get_object(job_id, report_id)\n        if report.status not in [\n            AbstractReport.Status.RUNNING,\n            AbstractReport.Status.PENDING,\n        ]:\n            raise ValidationError({\"detail\": \"Plugin is not running or pending\"})\n\n        self.perform_kill(report)\n        return Response(status=status.HTTP_204_NO_CONTENT)\n\n    @add_docs(\n        description=\"Retry a plugin run if it failed/was killed previously\",\n        request=None,\n        responses={\n            204: None,\n        },\n    )\n    @action(detail=False, methods=[\"patch\"])\n    def retry(self, request, job_id, report_id):\n        \"\"\"\n        Retries a failed or killed plugin run.\n\n        This method clears the errors and re-runs the plugin with the same arguments.\n        It fetches the appropriate task signature and schedules the job again.\n\n        Args:\n            report (AbstractReport): The report to be retried.\n\n        Raises:\n            RuntimeError: If unable to find a valid task signature for the report.\n        \"\"\"\n        logger.info(\n            f\"retry request from user {request.user}\"\n            f\" for job_id {job_id}, report_id {report_id}\"\n        )\n        # get report object or raise 404\n        report = self.get_object(job_id, report_id)\n        if report.status not in [\n            AbstractReport.Status.FAILED,\n            AbstractReport.Status.KILLED,\n        ]:\n            raise ValidationError(\n                {\"detail\": \"Plugin status should be failed or killed\"}\n            )\n\n        # retry with the same arguments\n        try:\n            self.perform_retry(report)\n        except StopIteration:\n            logger.exception(f\"Unable to find signature for report {report.pk}\")\n            return Response(status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n        return Response(status=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.PythonReportActionViewSet.report_model","title":"<code>report_model</code>  <code>abstractmethod</code> <code>classmethod</code> <code>property</code>","text":"<p>Abstract property that should return the model class for the report.</p> <p>Subclasses must implement this property to specify the model class for the reports being handled by this view set.</p> <p>Returns:</p> Type Description <p>Type[AbstractReport]: The model class for the report.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not overridden by a subclass.</p>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.PythonReportActionViewSet.get_object","title":"<code>get_object(job_id, report_id)</code>","text":"<p>Retrieves a specific report object by job_id and report_id.</p> <p>Overrides the drf's default <code>get_object</code> method to fetch a report object based on job_id and report_id, and checks the permissions for the object.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>The ID of the job associated with the report.</p> required <code>report_id</code> <code>int</code> <p>The ID of the report.</p> required <p>Returns:</p> Name Type Description <code>AbstractReport</code> <code>AbstractReport</code> <p>The report object.</p> <p>Raises:</p> Type Description <code>NotFound</code> <p>If the report does not exist.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>def get_object(self, job_id: int, report_id: int) -&gt; AbstractReport:\n    \"\"\"\n    Retrieves a specific report object by job_id and report_id.\n\n    Overrides the drf's default `get_object` method to fetch a report object\n    based on job_id and report_id, and checks the permissions for the object.\n\n    Args:\n        job_id (int): The ID of the job associated with the report.\n        report_id (int): The ID of the report.\n\n    Returns:\n        AbstractReport: The report object.\n\n    Raises:\n        NotFound: If the report does not exist.\n    \"\"\"\n    try:\n        obj = self.report_model.objects.get(\n            job_id=job_id,\n            pk=report_id,\n        )\n    except self.report_model.DoesNotExist:\n        raise NotFound()\n    else:\n        self.check_object_permissions(self.request, obj)\n        return obj\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.PythonReportActionViewSet.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Returns the queryset of reports based on the model class.</p> <p>Filters the queryset to return all instances of the report model.</p> <p>Returns:</p> Name Type Description <code>QuerySet</code> <p>A queryset of all report instances.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>def get_queryset(self):\n    \"\"\"\n    Returns the queryset of reports based on the model class.\n\n    Filters the queryset to return all instances of the report model.\n\n    Returns:\n        QuerySet: A queryset of all report instances.\n    \"\"\"\n    return self.report_model.objects.all()\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.PythonReportActionViewSet.kill","title":"<code>kill(request, job_id, report_id)</code>","text":"<p>Kills a specific report by terminating its Celery task and marking it as killed.</p> <p>This endpoint handles the patch request to kill a report if its status is running or pending.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>HttpRequest</code> <p>The request object containing the HTTP PATCH request.</p> required <code>job_id</code> <code>int</code> <p>The ID of the job associated with the report.</p> required <code>report_id</code> <code>int</code> <p>The ID of the report.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>HTTP 204 No Content if successful.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the report is not in a valid state for killing.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"Kill running plugin by closing celery task and marking as killed\",\n    request=None,\n    responses={\n        204: None,\n    },\n)\n@action(detail=False, methods=[\"patch\"])\ndef kill(self, request, job_id, report_id):\n    \"\"\"\n    Kills a specific report by terminating its Celery task and marking it as killed.\n\n    This endpoint handles the patch request to kill a report if its status is\n    running or pending.\n\n    Args:\n        request (HttpRequest): The request object containing the HTTP PATCH request.\n        job_id (int): The ID of the job associated with the report.\n        report_id (int): The ID of the report.\n\n    Returns:\n        Response: HTTP 204 No Content if successful.\n\n    Raises:\n        ValidationError: If the report is not in a valid state for killing.\n    \"\"\"\n    logger.info(\n        f\"kill request from user {request.user}\"\n        f\" for job_id {job_id}, pk {report_id}\"\n    )\n    # get report object or raise 404\n    report = self.get_object(job_id, report_id)\n    if report.status not in [\n        AbstractReport.Status.RUNNING,\n        AbstractReport.Status.PENDING,\n    ]:\n        raise ValidationError({\"detail\": \"Plugin is not running or pending\"})\n\n    self.perform_kill(report)\n    return Response(status=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.PythonReportActionViewSet.perform_kill","title":"<code>perform_kill(report)</code>  <code>staticmethod</code>","text":"<p>Kills a running plugin by terminating its Celery task and marking it as killed.</p> <p>This method is a callback for performing additional actions after a kill operation, including updating the report status and cleaning up the associated job.</p> <p>Parameters:</p> Name Type Description Default <code>report</code> <code>AbstractReport</code> <p>The report to be killed.</p> required Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@staticmethod\ndef perform_kill(report: AbstractReport):\n    \"\"\"\n    Kills a running plugin by terminating its Celery task and marking it as killed.\n\n    This method is a callback for performing additional actions after a\n    kill operation, including updating the report status and cleaning up\n    the associated job.\n\n    Args:\n        report (AbstractReport): The report to be killed.\n    \"\"\"\n    # kill celery task\n    celery_app.control.revoke(report.task_id, terminate=True)\n    # update report\n    report.status = AbstractReport.Status.KILLED\n    report.save(update_fields=[\"status\"])\n    # clean up job\n\n    job = Job.objects.get(pk=report.job.pk)\n    job.set_final_status()\n    JobConsumer.serialize_and_send_job(job)\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.PythonReportActionViewSet.perform_retry","title":"<code>perform_retry(report)</code>  <code>staticmethod</code>","text":"<p>Retries a failed or killed plugin run.</p> <p>This method clears the errors and re-runs the plugin with the same arguments. It fetches the appropriate task signature and schedules the job again.</p> <p>Parameters:</p> Name Type Description Default <code>report</code> <code>AbstractReport</code> <p>The report to be retried.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to find a valid task signature for the report.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@staticmethod\ndef perform_retry(report: AbstractReport):\n    \"\"\"\n    Retries a failed or killed plugin run.\n\n    This method clears the errors and re-runs the plugin with the same arguments.\n    It fetches the appropriate task signature and schedules the job again.\n\n    Args:\n        report (AbstractReport): The report to be retried.\n\n    Raises:\n        RuntimeError: If unable to find a valid task signature for the report.\n    \"\"\"\n    report.errors.clear()\n    report.save(update_fields=[\"errors\"])\n    try:\n        signature = next(\n            report.config.__class__.objects.filter(pk=report.config.pk)\n            .annotate_runnable(report.job.user)\n            .get_signatures(\n                report.job,\n            )\n        )\n    except StopIteration:\n        raise RuntimeError(f\"Unable to find signature for report {report.pk}\")\n    runner = signature | tasks.job_set_final_status.signature(\n        args=[report.job.id],\n        kwargs={},\n        queue=report.config.queue,\n        immutable=True,\n        MessageGroupId=str(uuid.uuid4()),\n        priority=report.job.priority,\n    )\n    runner()\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.PythonReportActionViewSet.retry","title":"<code>retry(request, job_id, report_id)</code>","text":"<p>Retries a failed or killed plugin run.</p> <p>This method clears the errors and re-runs the plugin with the same arguments. It fetches the appropriate task signature and schedules the job again.</p> <p>Parameters:</p> Name Type Description Default <code>report</code> <code>AbstractReport</code> <p>The report to be retried.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If unable to find a valid task signature for the report.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"Retry a plugin run if it failed/was killed previously\",\n    request=None,\n    responses={\n        204: None,\n    },\n)\n@action(detail=False, methods=[\"patch\"])\ndef retry(self, request, job_id, report_id):\n    \"\"\"\n    Retries a failed or killed plugin run.\n\n    This method clears the errors and re-runs the plugin with the same arguments.\n    It fetches the appropriate task signature and schedules the job again.\n\n    Args:\n        report (AbstractReport): The report to be retried.\n\n    Raises:\n        RuntimeError: If unable to find a valid task signature for the report.\n    \"\"\"\n    logger.info(\n        f\"retry request from user {request.user}\"\n        f\" for job_id {job_id}, report_id {report_id}\"\n    )\n    # get report object or raise 404\n    report = self.get_object(job_id, report_id)\n    if report.status not in [\n        AbstractReport.Status.FAILED,\n        AbstractReport.Status.KILLED,\n    ]:\n        raise ValidationError(\n            {\"detail\": \"Plugin status should be failed or killed\"}\n        )\n\n    # retry with the same arguments\n    try:\n        self.perform_retry(report)\n    except StopIteration:\n        logger.exception(f\"Unable to find signature for report {report.pk}\")\n        return Response(status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n    return Response(status=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"IntelOwl/api_docs/#abstractconfigviewset","title":"<code>AbstractConfigViewSet</code>","text":"<p>               Bases: <code>PaginationMixin</code>, <code>ReadOnlyModelViewSet</code></p> <p>A base view set for handling plugin configuration actions.</p> <p>This view set provides methods for enabling and disabling plugins within an organization. It requires users to be authenticated and to have appropriate permissions.</p> <p>Attributes:</p> Name Type Description <code>permission_classes</code> <code>list</code> <p>List of permission classes to apply.</p> <code>ordering</code> <code>list</code> <p>Default ordering for the queryset.</p> <code>lookup_field</code> <code>str</code> <p>Field to look up in the URL.</p> <p>Methods:</p> Name Description <code>disable_in_org</code> <p>Disables the plugin for the organization of the authenticated user.</p> <code>enable_in_org</code> <p>Enables the plugin for the organization of the authenticated user.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>class AbstractConfigViewSet(\n    PaginationMixin, viewsets.ReadOnlyModelViewSet, metaclass=ABCMeta\n):\n    \"\"\"\n    A base view set for handling plugin configuration actions.\n\n    This view set provides methods for enabling and disabling plugins\n    within an organization. It requires users to be authenticated and\n    to have appropriate permissions.\n\n    Attributes:\n        permission_classes (list): List of permission classes to apply.\n        ordering (list): Default ordering for the queryset.\n        lookup_field (str): Field to look up in the URL.\n\n    Methods:\n        disable_in_org(request, name=None):\n            Disables the plugin for the organization of the authenticated user.\n        enable_in_org(request, name=None):\n            Enables the plugin for the organization of the authenticated user.\n    \"\"\"\n\n    permission_classes = [IsAuthenticated]\n    ordering = [\"name\"]\n    lookup_field = \"name\"\n\n    @add_docs(\n        description=\"Disable/Enable plugin for your organization\",\n        request=None,\n        responses={201: {}, 202: {}},\n    )\n    @action(\n        methods=[\"post\"],\n        detail=True,\n        url_path=\"organization\",\n    )\n    def disable_in_org(self, request, name=None):\n        \"\"\"\n        Disables the plugin for the organization of the authenticated user.\n\n        Only organization admins can disable the plugin. If the plugin is\n        already disabled, a validation error is raised.\n\n        Args:\n            request (Request): The HTTP request object.\n            name (str, optional): The name of the plugin. Defaults to None.\n\n        Returns:\n            Response: HTTP response indicating the success or failure of the operation.\n        \"\"\"\n        logger.info(f\"get disable_in_org from user {request.user}, name {name}\")\n        obj: AbstractConfig = self.get_object()\n        if request.user.has_membership():\n            if not request.user.membership.is_admin:\n                raise PermissionDenied()\n        else:\n            raise PermissionDenied()\n        organization = request.user.membership.organization\n        org_configuration = obj.get_or_create_org_configuration(organization)\n        if org_configuration.disabled:\n            raise ValidationError({\"detail\": f\"Plugin {obj.name} already disabled\"})\n        org_configuration.disable_manually(request.user)\n        return Response(status=status.HTTP_201_CREATED)\n\n    @disable_in_org.mapping.delete\n    def enable_in_org(self, request, name=None):\n        \"\"\"\n        Enables the plugin for the organization of the authenticated user.\n\n        Only organization admins can enable the plugin. If the plugin is\n        already enabled, a validation error is raised.\n\n        Args:\n            request (Request): The HTTP request object.\n            name (str, optional): The name of the plugin. Defaults to None.\n\n        Returns:\n            Response: HTTP response indicating the success or failure of the operation.\n        \"\"\"\n        logger.info(f\"get enable_in_org from user {request.user}, name {name}\")\n        obj: AbstractConfig = self.get_object()\n        if request.user.has_membership():\n            if not request.user.membership.is_admin:\n                raise PermissionDenied()\n        else:\n            raise PermissionDenied()\n        organization = request.user.membership.organization\n        org_configuration = obj.get_or_create_org_configuration(organization)\n        if not org_configuration.disabled:\n            raise ValidationError({\"detail\": f\"Plugin {obj.name} already enabled\"})\n        org_configuration.enable_manually(request.user)\n        return Response(status=status.HTTP_202_ACCEPTED)\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.AbstractConfigViewSet.disable_in_org","title":"<code>disable_in_org(request, name=None)</code>","text":"<p>Disables the plugin for the organization of the authenticated user.</p> <p>Only organization admins can disable the plugin. If the plugin is already disabled, a validation error is raised.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The HTTP request object.</p> required <code>name</code> <code>str</code> <p>The name of the plugin. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>HTTP response indicating the success or failure of the operation.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"Disable/Enable plugin for your organization\",\n    request=None,\n    responses={201: {}, 202: {}},\n)\n@action(\n    methods=[\"post\"],\n    detail=True,\n    url_path=\"organization\",\n)\ndef disable_in_org(self, request, name=None):\n    \"\"\"\n    Disables the plugin for the organization of the authenticated user.\n\n    Only organization admins can disable the plugin. If the plugin is\n    already disabled, a validation error is raised.\n\n    Args:\n        request (Request): The HTTP request object.\n        name (str, optional): The name of the plugin. Defaults to None.\n\n    Returns:\n        Response: HTTP response indicating the success or failure of the operation.\n    \"\"\"\n    logger.info(f\"get disable_in_org from user {request.user}, name {name}\")\n    obj: AbstractConfig = self.get_object()\n    if request.user.has_membership():\n        if not request.user.membership.is_admin:\n            raise PermissionDenied()\n    else:\n        raise PermissionDenied()\n    organization = request.user.membership.organization\n    org_configuration = obj.get_or_create_org_configuration(organization)\n    if org_configuration.disabled:\n        raise ValidationError({\"detail\": f\"Plugin {obj.name} already disabled\"})\n    org_configuration.disable_manually(request.user)\n    return Response(status=status.HTTP_201_CREATED)\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.AbstractConfigViewSet.enable_in_org","title":"<code>enable_in_org(request, name=None)</code>","text":"<p>Enables the plugin for the organization of the authenticated user.</p> <p>Only organization admins can enable the plugin. If the plugin is already enabled, a validation error is raised.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The HTTP request object.</p> required <code>name</code> <code>str</code> <p>The name of the plugin. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>HTTP response indicating the success or failure of the operation.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@disable_in_org.mapping.delete\ndef enable_in_org(self, request, name=None):\n    \"\"\"\n    Enables the plugin for the organization of the authenticated user.\n\n    Only organization admins can enable the plugin. If the plugin is\n    already enabled, a validation error is raised.\n\n    Args:\n        request (Request): The HTTP request object.\n        name (str, optional): The name of the plugin. Defaults to None.\n\n    Returns:\n        Response: HTTP response indicating the success or failure of the operation.\n    \"\"\"\n    logger.info(f\"get enable_in_org from user {request.user}, name {name}\")\n    obj: AbstractConfig = self.get_object()\n    if request.user.has_membership():\n        if not request.user.membership.is_admin:\n            raise PermissionDenied()\n    else:\n        raise PermissionDenied()\n    organization = request.user.membership.organization\n    org_configuration = obj.get_or_create_org_configuration(organization)\n    if not org_configuration.disabled:\n        raise ValidationError({\"detail\": f\"Plugin {obj.name} already enabled\"})\n    org_configuration.enable_manually(request.user)\n    return Response(status=status.HTTP_202_ACCEPTED)\n</code></pre>"},{"location":"IntelOwl/api_docs/#pythonconfigviewset","title":"<code>PythonConfigViewSet</code>","text":"<p>               Bases: <code>AbstractConfigViewSet</code></p> <p>A view set for handling actions related to Python plugin configurations.</p> <p>This view set provides methods to perform health checks and pull updates for Python-based plugins. It inherits from <code>AbstractConfigViewSet</code> and requires users to be authenticated.</p> <p>Attributes:</p> Name Type Description <code>serializer_class</code> <code>class</code> <p>Serializer class for the view set.</p> <p>Methods:</p> Name Description <code>health_check</code> <p>Checks if the server instance associated with the plugin is up.</p> <code>pull</code> <p>Pulls updates for the plugin.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>class PythonConfigViewSet(AbstractConfigViewSet):\n    \"\"\"\n    A view set for handling actions related to Python plugin configurations.\n\n    This view set provides methods to perform health checks and pull updates\n    for Python-based plugins. It inherits from `AbstractConfigViewSet` and\n    requires users to be authenticated.\n\n    Attributes:\n        serializer_class (class): Serializer class for the view set.\n\n    Methods:\n        health_check(request, name=None):\n            Checks if the server instance associated with the plugin is up.\n        pull(request, name=None):\n            Pulls updates for the plugin.\n    \"\"\"\n\n    serializer_class = PythonConfigSerializer\n\n    def get_queryset(self):\n        \"\"\"\n        Returns a queryset of all PythonConfig instances with related\n        python_module parameters pre-fetched.\n\n        Returns:\n            QuerySet: A queryset of PythonConfig instances.\n        \"\"\"\n        return self.serializer_class.Meta.model.objects.all().prefetch_related(\n            \"python_module__parameters\"\n        )\n\n    @add_docs(\n        description=\"Health Check: \"\n        \"if server instance associated with plugin is up or not\",\n        request=None,\n        responses={\n            200: inline_serializer(\n                name=\"PluginHealthCheckSuccessResponse\",\n                fields={\n                    \"status\": rfs.BooleanField(allow_null=True),\n                },\n            ),\n        },\n    )\n    @action(\n        methods=[\"get\"],\n        detail=True,\n        url_path=\"health_check\",\n    )\n    def health_check(self, request, name=None):\n        \"\"\"\n        Checks the health of the server instance associated with the plugin.\n\n        This method attempts to check if the plugin's server instance is\n        up and running. It uses the `health_check` method of the plugin's\n        Python class.\n\n        Args:\n            request (Request): The HTTP request object.\n            name (str, optional): The name of the plugin. Defaults to None.\n\n        Returns:\n            Response: HTTP response with the health status of the plugin.\n\n        Raises:\n            ValidationError: If no health check is implemented or if an\n                             unexpected exception occurs.\n        \"\"\"\n        logger.info(f\"get healthcheck from user {request.user}, name {name}\")\n        config: PythonConfig = self.get_object()\n        python_obj = config.python_module.python_class(config)\n        try:\n            health_status = python_obj.health_check(request.user)\n        except NotImplementedError as e:\n            logger.info(f\"NotImplementedError {e}, user {request.user}, name {name}\")\n            raise ValidationError({\"detail\": \"No healthcheck implemented\"})\n        except Exception as e:\n            logger.exception(e)\n            raise ValidationError(\n                {\"detail\": \"Unexpected exception raised. Check the code.\"}\n            )\n        else:\n            return Response(data={\"status\": health_status}, status=status.HTTP_200_OK)\n\n    @action(\n        methods=[\"post\"],\n        detail=True,\n        url_path=\"pull\",\n    )\n    def pull(self, request, name=None):\n        \"\"\"\n        Pulls updates for the plugin.\n\n        This method attempts to pull updates for the plugin by calling\n        the `update` method of the plugin's Python class. It also handles\n        any exceptions that occur during this process.\n\n        Args:\n            request (Request): The HTTP request object.\n            name (str, optional): The name of the plugin. Defaults to None.\n\n        Returns:\n            Response: HTTP response with the update status of the plugin.\n\n        Raises:\n            ValidationError: If the update is not implemented or if an\n                             unexpected exception occurs.\n        \"\"\"\n        logger.info(f\"post pull from user {request.user}, name {name}\")\n        obj: PythonConfig = self.get_object()\n        python_obj = obj.python_module.python_class(obj)\n        try:\n            update_status = python_obj.update()\n        except NotImplementedError as e:\n            raise ValidationError({\"detail\": str(e)})\n        except Exception as e:\n            logger.exception(e)\n            raise ValidationError(\n                {\"detail\": \"Unexpected exception raised. Check the code.\"}\n            )\n        else:\n            if update_status is None:\n                raise ValidationError(\n                    {\"detail\": \"This Plugin has no Update implemented\"}\n                )\n            return Response(data={\"status\": update_status}, status=status.HTTP_200_OK)\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.PythonConfigViewSet.get_queryset","title":"<code>get_queryset()</code>","text":"<p>Returns a queryset of all PythonConfig instances with related python_module parameters pre-fetched.</p> <p>Returns:</p> Name Type Description <code>QuerySet</code> <p>A queryset of PythonConfig instances.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>def get_queryset(self):\n    \"\"\"\n    Returns a queryset of all PythonConfig instances with related\n    python_module parameters pre-fetched.\n\n    Returns:\n        QuerySet: A queryset of PythonConfig instances.\n    \"\"\"\n    return self.serializer_class.Meta.model.objects.all().prefetch_related(\n        \"python_module__parameters\"\n    )\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.PythonConfigViewSet.health_check","title":"<code>health_check(request, name=None)</code>","text":"<p>Checks the health of the server instance associated with the plugin.</p> <p>This method attempts to check if the plugin's server instance is up and running. It uses the <code>health_check</code> method of the plugin's Python class.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The HTTP request object.</p> required <code>name</code> <code>str</code> <p>The name of the plugin. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>HTTP response with the health status of the plugin.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If no health check is implemented or if an              unexpected exception occurs.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"Health Check: \"\n    \"if server instance associated with plugin is up or not\",\n    request=None,\n    responses={\n        200: inline_serializer(\n            name=\"PluginHealthCheckSuccessResponse\",\n            fields={\n                \"status\": rfs.BooleanField(allow_null=True),\n            },\n        ),\n    },\n)\n@action(\n    methods=[\"get\"],\n    detail=True,\n    url_path=\"health_check\",\n)\ndef health_check(self, request, name=None):\n    \"\"\"\n    Checks the health of the server instance associated with the plugin.\n\n    This method attempts to check if the plugin's server instance is\n    up and running. It uses the `health_check` method of the plugin's\n    Python class.\n\n    Args:\n        request (Request): The HTTP request object.\n        name (str, optional): The name of the plugin. Defaults to None.\n\n    Returns:\n        Response: HTTP response with the health status of the plugin.\n\n    Raises:\n        ValidationError: If no health check is implemented or if an\n                         unexpected exception occurs.\n    \"\"\"\n    logger.info(f\"get healthcheck from user {request.user}, name {name}\")\n    config: PythonConfig = self.get_object()\n    python_obj = config.python_module.python_class(config)\n    try:\n        health_status = python_obj.health_check(request.user)\n    except NotImplementedError as e:\n        logger.info(f\"NotImplementedError {e}, user {request.user}, name {name}\")\n        raise ValidationError({\"detail\": \"No healthcheck implemented\"})\n    except Exception as e:\n        logger.exception(e)\n        raise ValidationError(\n            {\"detail\": \"Unexpected exception raised. Check the code.\"}\n        )\n    else:\n        return Response(data={\"status\": health_status}, status=status.HTTP_200_OK)\n</code></pre>"},{"location":"IntelOwl/api_docs/#docs.Submodules.IntelOwl.api_app.views.PythonConfigViewSet.pull","title":"<code>pull(request, name=None)</code>","text":"<p>Pulls updates for the plugin.</p> <p>This method attempts to pull updates for the plugin by calling the <code>update</code> method of the plugin's Python class. It also handles any exceptions that occur during this process.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The HTTP request object.</p> required <code>name</code> <code>str</code> <p>The name of the plugin. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Response</code> <p>HTTP response with the update status of the plugin.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the update is not implemented or if an              unexpected exception occurs.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@action(\n    methods=[\"post\"],\n    detail=True,\n    url_path=\"pull\",\n)\ndef pull(self, request, name=None):\n    \"\"\"\n    Pulls updates for the plugin.\n\n    This method attempts to pull updates for the plugin by calling\n    the `update` method of the plugin's Python class. It also handles\n    any exceptions that occur during this process.\n\n    Args:\n        request (Request): The HTTP request object.\n        name (str, optional): The name of the plugin. Defaults to None.\n\n    Returns:\n        Response: HTTP response with the update status of the plugin.\n\n    Raises:\n        ValidationError: If the update is not implemented or if an\n                         unexpected exception occurs.\n    \"\"\"\n    logger.info(f\"post pull from user {request.user}, name {name}\")\n    obj: PythonConfig = self.get_object()\n    python_obj = obj.python_module.python_class(obj)\n    try:\n        update_status = python_obj.update()\n    except NotImplementedError as e:\n        raise ValidationError({\"detail\": str(e)})\n    except Exception as e:\n        logger.exception(e)\n        raise ValidationError(\n            {\"detail\": \"Unexpected exception raised. Check the code.\"}\n        )\n    else:\n        if update_status is None:\n            raise ValidationError(\n                {\"detail\": \"This Plugin has no Update implemented\"}\n            )\n        return Response(data={\"status\": update_status}, status=status.HTTP_200_OK)\n</code></pre>"},{"location":"IntelOwl/api_docs/#functions","title":"Functions","text":""},{"location":"IntelOwl/api_docs/#plugin_state_viewer","title":"<code>plugin_state_viewer</code>","text":"<p>View to retrieve the state of plugin configurations for the requesting user\u2019s organization.</p> <p>This endpoint is accessible only to users with an active membership in an organization. It returns a JSON response with the state of each plugin configuration, specifically indicating whether each plugin is disabled.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>HttpRequest</code> <p>The request object containing the HTTP GET request.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <p>A JSON response with the state of each plugin configuration,       indicating whether it is disabled or not.</p> <p>Raises:</p> Type Description <code>PermissionDenied</code> <p>If the requesting user does not belong to any organization.</p> Source code in <code>docs/Submodules/IntelOwl/api_app/views.py</code> <pre><code>@add_docs(\n    description=\"\"\"This endpoint allows organization owners\n    and members to view plugin state.\"\"\",\n    responses={\n        200: inline_serializer(\n            name=\"PluginStateViewerResponseSerializer\",\n            fields={\n                \"data\": rfs.JSONField(),\n            },\n        ),\n    },\n)\n@api_view([\"GET\"])\ndef plugin_state_viewer(request):\n    \"\"\"\n    View to retrieve the state of plugin configurations for the requesting user\u2019s organization.\n\n    This endpoint is accessible only to users with an active membership in an organization.\n    It returns a JSON response with the state of each plugin configuration, specifically\n    indicating whether each plugin is disabled.\n\n    Args:\n        request (HttpRequest): The request object containing the HTTP GET request.\n\n    Returns:\n        Response: A JSON response with the state of each plugin configuration,\n                  indicating whether it is disabled or not.\n\n    Raises:\n        PermissionDenied: If the requesting user does not belong to any organization.\n    \"\"\"\n    if not request.user.has_membership():\n        raise PermissionDenied()\n\n    result = {\"data\": {}}\n    for opc in OrganizationPluginConfiguration.objects.filter(disabled=True):\n        result[\"data\"][opc.config.name] = {\n            \"disabled\": True,\n        }\n    return Response(result)\n</code></pre>"},{"location":"IntelOwl/contribute/","title":"Contribute","text":"<p>There are a lot of different ways you could choose to contribute to the IntelOwl Project:</p> <ul> <li>main repository: IntelOwl</li> </ul> <ul> <li>official Python client: pyintelowl.</li> </ul> <ul> <li>official GO client: go-intelowl.</li> </ul> <ul> <li>official IntelOwl Site: intelowlproject.github.io.</li> </ul> <ul> <li>honeypots project: Greedybear</li> </ul>"},{"location":"IntelOwl/contribute/#rules","title":"Rules","text":"<p>Intel Owl welcomes contributors from anywhere and from any kind of education or skill level. We strive to create a community of developers that is welcoming, friendly and right.</p> <p>For this reason it is important to follow some easy rules based on a simple but important concept: Respect.</p> <ul> <li>Before asking any questions regarding how the project works, please read through all the documentation and install the project on your own local machine to try it and understand how it basically works. This is a form of respect to the maintainers.</li> </ul> <ul> <li>DO NOT contact the maintainers with direct messages unless it is an urgent request. We don't have much time and cannot just answer to all the questions that we receive like \"Guide me please! Help me understand how the project work\". There is plenty of documentation and a lot of people in the community that can help you and would benefit from your questions. Share your problems and your knowledge. Please ask your questions in open channels (Github and Slack). This is a form of respect to the maintainers and to the community.</li> </ul> <ul> <li>Before starting to work on an issue, you need to get the approval of one of the maintainers. Therefore please ask to be assigned to an issue. If you do not that but you still raise a PR for that issue, your PR can be rejected. This is a form of respect for both the maintainers and the other contributors who could have already started to work on the same problem.</li> </ul> <ul> <li>When you ask to be assigned to an issue, it means that you are ready to work on it. When you get assigned, take the lock and then you disappear, you are not respecting the maintainers and the other contributors who could be able to work on that. So, after having been assigned, you have a week of time to deliver your first draft PR. After that time has passed without any notice, you will be unassigned.</li> </ul> <ul> <li>Once you started working on an issue and you have some work to share and discuss with us, please raise a draft PR early with incomplete changes. This way you can continue working on the same and we can track your progress and actively review and help. This is a form of respect to you and to the maintainers.</li> </ul> <ul> <li>When creating a PR, please read through the sections that you will find in the PR template and compile it appropriately. If you do not, your PR can be rejected. This is a form of respect to the maintainers.</li> </ul>"},{"location":"IntelOwl/contribute/#code-style","title":"Code Style","text":"<p>Keeping to a consistent code style throughout the project makes it easier to contribute and collaborate. We make use of <code>psf/black</code> and isort for code formatting and <code>flake8</code> for style guides.</p>"},{"location":"IntelOwl/contribute/#how-to-start-setup-project-and-development-instance","title":"How to start (Setup project and development instance)","text":"<p>This guide assumes that you have already performed the steps required to install the project. If not, please do it (Installation Guide).</p> <p>Create a personal fork of the project on Github. Then, please create a new branch based on the develop branch that contains the most recent changes. This is mandatory.</p> <p><code>git checkout -b myfeature develop</code></p> <p>Then we strongly suggest to configure pre-commit to force linters on every commits you perform</p> <pre><code># From the project directory\npython3 -m venv venv\nsource venv/bin/activate\n# from the project base directory\npip install pre-commit\npre-commit install\n\n# create .env file for controlling repo_downloader.sh\n# (to speed up image builds during development: it avoid downloading some repos)\ncp docker/.env.start.test.template docker/.env.start.test\n\n# set STAGE env variable to \"local\"\nsed -i \"s/STAGE=\\\"production\\\"/STAGE=\\\"local\\\"/g\" docker/env_file_app\n</code></pre>"},{"location":"IntelOwl/contribute/#backend","title":"Backend","text":"<p>Now, you can execute IntelOwl in development mode by selecting the mode <code>test</code> while launching the startup script:</p> <pre><code>./start test up\n</code></pre> <p>Every time you perform a change, you should perform an operation to reflect the changes into the application:</p> <ul> <li>if you changed the python requirements, restart the application and re-build the images. This is the slowest process. You can always choose this way but it would waste a lot of time.</li> </ul> <pre><code>./start test down &amp;&amp; ./start test up -- --build\n</code></pre> <ul> <li>if you changed either analyzers, connectors, playbooks or anything that is executed asynchronously by the \"celery\" containers, you just need to restart the application because we leverage Docker bind volumes that will reflect the changes to the containers. This saves the time of the build</li> </ul> <pre><code>./start test down &amp;&amp; ./start test up\n</code></pre> <ul> <li>if you made changes to either the API or anything that is executed only by the application server, changes will be instantly reflected and you don't need to do anything. This is thanks to the Django Development server that is executed instead of <code>uwsgi</code> while using the <code>test</code> mode</li> </ul>"},{"location":"IntelOwl/contribute/#note-about-documentation","title":"NOTE about documentation:","text":"<p>If you made any changes to an existing model/serializer/view, please run the following command to generate a new version of the API schema and docs:</p> <pre><code>docker exec -it intelowl_uwsgi python manage.py spectacular --file docs/source/schema.yml &amp;&amp; make html\n</code></pre>"},{"location":"IntelOwl/contribute/#frontend","title":"Frontend","text":"<p>To start the frontend in \"develop\" mode, you can execute the startup npm script within the folder <code>frontend</code>:</p> <pre><code>cd frontend/\n# Install\nnpm i\n# Start\nDANGEROUSLY_DISABLE_HOST_CHECK=true npm start\n# See https://create-react-app.dev/docs/proxying-api-requests-in-development/#invalid-host-header-errors-after-configuring-proxy for why we use that flag in development mode\n</code></pre> <p>Most of the time you would need to test the changes you made together with the backend. In that case, you would need to run the backend locally too:</p> <pre><code>./start prod up\n</code></pre> <p>Note</p> <ul> <li>Running <code>prod</code> would be faster because you would leverage the official images and you won't need to build the backend locally. In case you would need to test backend changes too at the same time, please use <code>test</code> and refer to the previous section of the documentation.</li> <li>This works thanks to the directive <code>proxy</code> in the <code>frontend/package.json</code> configuration</li> <li>It may happen that the backend build does not work due to incompatibility between the frontend version you are testing with the current complete IntelOwl version you are running. In those cases, considering that you don't need to build the frontend together with the backend because you are already testing it separately, we suggest to remove the first build step (the frontend part) from the main Dockerfile temporarily and build IntelOwl with only the backend. In this way there won't be conflict issues.</li> </ul>"},{"location":"IntelOwl/contribute/#certego-ui","title":"Certego-UI","text":"<p>The IntelOwl Frontend is tightly linked to the <code>certego-ui</code> library. Most of the React components are imported from there. Because of this, it may happen that, during development, you would need to work on that library too. To install the <code>certego-ui</code> library, please take a look to npm link and remember to start certego-ui without installing peer dependencies (to avoid conflicts with IntelOwl dependencies):</p> <pre><code>git clone https://github.com/certego/certego-ui.git\n# change directory to the folder where you have the cloned the library\ncd certego-ui/\n# install, without peer deps (to use packages of IntelOwl)\nnpm i --legacy-peer-deps\n# create link to the project (this will globally install this package)\nsudo npm link\n# compile the library\nnpm start\n</code></pre> <p>Then, open another command line tab, create a link in the <code>frontend</code> to the <code>certego-ui</code> and re-install and re-start the frontend application (see previous section):</p> <pre><code>cd frontend/\nnpm link @certego/certego-ui\n</code></pre> <p>This trick will allow you to see reflected every changes you make in the <code>certego-ui</code> directly in the running <code>frontend</code> application.</p>"},{"location":"IntelOwl/contribute/#example-application","title":"Example application","text":"<p>The <code>certego-ui</code> application comes with an example project that showcases the components that you can re-use and import to other projects, like IntelOwl:</p> <pre><code># To have the Example application working correctly, be sure to have installed `certego-ui` *without* the `--legacy-peer-deps` option and having it started in another command line\ncd certego-ui/\nnpm i\nnpm start\n# go to another tab\ncd certego-ui/example/\nnpm i\nnpm start\n</code></pre>"},{"location":"IntelOwl/contribute/#how-to-add-a-new-plugin","title":"How to add a new Plugin","text":"<p>IntelOwl was designed to ease the addition of new plugins. With a simple python script you can integrate your own engine or integrate an external service in a short time.</p> <p>There are two possible cases:</p> <ol> <li>You are creating an entirely new Plugin, meaning that you actually wrote python code</li> <li>You are creating a new Configuration for some code that already exists.</li> </ol> <p>If you are doing the step number <code>2</code>, you can skip this paragraph.</p> <p>First, you need to create the python code that will be actually executed. You can easily take other plugins as example to write this. Then, you have to create a <code>Python Module</code> model. You can do this in the <code>Django Admin</code> page: You have to specify which type of Plugin you wrote, and its python module. Again, you can use as an example an already configured <code>Python Module</code>.</p> <p>Some <code>Python Module</code> requires to update some part of its code in a schedule way: for example <code>Yara</code> requires to update the rule repositories, <code>QuarkEngine</code> to update its database and so on. If the <code>Python Module</code> that you define need this type of behaviour, you have to configure two things:</p> <ul> <li>In the python code, you have to override a method called <code>update</code> and put the updating logic (see other plugins for examples) there.</li> <li>In the model class, you have to add the <code>update_schedule</code> (crontab syntax) that define when the update should be executed.</li> </ul> <p>Some <code>Python Module</code> requires further check to see if the service provider is able to answer requests; for example if you have done too many requests, or the website is currently down for maintenance and so on. If the <code>Python Module</code> that you define need this type of behaviour, you have to configure two things:</p> <ul> <li>In the python code, you can override a method called <code>health_check</code> and put there the custom health check logic. As default, plugins will try to make an HTTP <code>HEAD</code> request to the configured url (the Plugin must have a <code>url</code> attribute).</li> <li>In the model class, you have to add the <code>health_check_schedule</code> (crontab syntax) that define when the health check should be executed.</li> </ul> <p>Press <code>Save and continue editing</code> to, at the moment, manually ad the <code>Parameters</code> that the python code requires (the class attributes that you needed):</p> <ol> <li>*name: Name of the parameter that will be dynamically added to the python class (if is a secret, in the python code a <code>_</code> wil be prepended to the name)</li> <li>*type: data type, <code>string</code>, <code>list</code>, <code>dict</code>, <code>integer</code>, <code>boolean</code>, <code>float</code></li> <li>*description</li> <li>*required: <code>true</code> or <code>false</code>, meaning that a value is necessary to allow the run of the analyzer</li> <li>*is_secret: <code>true</code> or <code>false</code></li> </ol> <p>At this point, you can follow the specific guide for each plugin</p>"},{"location":"IntelOwl/contribute/#how-to-add-a-new-analyzer","title":"How to add a new Analyzer","text":"<p>You may want to look at a few existing examples to start to build a new one, such as:</p> <ul> <li>shodan.py, if you are creating an observable analyzer</li> <li>malpedia_scan.py, if you are creating a file analyzer</li> <li>peframe.py, if you are creating a docker based analyzer</li> <li>Please note: If the new analyzer that you are adding is free for the user to use, please add it in the <code>FREE_TO_USE_ANALYZERS</code> playbook. To do this you have to make a migration file; you can use <code>0026_add_mmdb_analyzer_free_to_use</code> as a template.</li> </ul> <p>After having written the new python module, you have to remember to:</p> <ol> <li>Put the module in the <code>file_analyzers</code> or <code>observable_analyzers</code> directory based on what it can analyze</li> <li>Remember to use <code>_monkeypatch()</code> in its class to create automated tests for the new analyzer. This is a trick to have tests in the same class of its analyzer.</li> <li>Create the configuration inside django admin in <code>Analyzers_manager/AnalyzerConfigs</code> (* = mandatory, ~ = mandatory on conditions)<ol> <li>*Name: specific name of the configuration</li> <li>*Python module: . <li>*Description: description of the configuration</li> <li>*Routing key: celery queue that will be used</li> <li>*Soft_time_limit: maximum time for the task execution</li> <li>*Type: <code>observable</code> or <code>file</code></li> <li>*Docker based: if the analyzer run through a docker instance</li> <li>*Maximum tlp: maximum tlp to allow the run on the connector</li> <li>~Observable supported: required if <code>type</code> is <code>observable</code></li> <li>~Supported filetypes: required if <code>type</code> is <code>file</code> and <code>not supported filetypes</code> is empty</li> <li>Run hash: if the analyzer supports hash as inputs</li> <li>~Run hash type: required if <code>run hash</code> is <code>True</code></li> <li>~Not supported filetypes: required if <code>type</code> is <code>file</code> and <code>supported filetypes</code> is empty</li>"},{"location":"IntelOwl/contribute/#integrating-a-docker-based-analyzer","title":"Integrating a docker based analyzer","text":"<p>If the analyzer you wish to integrate doesn't exist as a public API or python package, it should be integrated with its own docker image which can be queried from the main Django app.</p> <ul> <li>It should follow the same design principle as the other such existing integrations, unless there's very good reason not to.</li> <li>The dockerfile should be placed at <code>./integrations/&lt;analyzer_name&gt;/Dockerfile</code>.</li> <li>Two docker-compose files <code>compose.yml</code> for production and <code>compose-tests.yml</code> for testing should be placed under <code>./integrations/&lt;analyzer_name&gt;</code>.</li> <li>If your docker-image uses any environment variables, add them in the <code>docker/env_file_integrations_template</code>.</li> <li>Rest of the steps remain same as given under \"How to add a new analyzer\".</li> </ul>"},{"location":"IntelOwl/contribute/#how-to-add-a-new-connector","title":"How to add a new Connector","text":"<p>You may want to look at a few existing examples to start to build a new one:</p> <ul> <li>misp.py</li> <li>opencti.py</li> </ul> <p>After having written the new python module, you have to remember to:</p> <ol> <li>Put the module in the <code>connectors</code> directory</li> <li>Remember to use <code>_monkeypatch()</code> in its class to create automated tests for the new connector. This is a trick to have tests in the same class of its connector.</li> <li>Create the configuration inside django admin in <code>Connectors_manager/ConnectorConfigs</code> (* = mandatory, ~ = mandatory on conditions)<ol> <li>*Name: specific name of the configuration</li> <li>*Python module: . <li>*Description: description of the configuration</li> <li>*Routing key: celery queue that will be used</li> <li>*Soft_time_limit: maximum time for the task execution</li> <li>*Maximum tlp: maximum tlp to allow the run on the connector</li> <li>*Run on failure: if the connector should be run even if the job fails</li>"},{"location":"IntelOwl/contribute/#how-to-add-a-new-ingestor","title":"How to add a new Ingestor","text":"<ol> <li>Put the module in the <code>ingestors</code> directory</li> <li>Remember to use <code>_monkeypatch()</code> in its class to create automated tests for the new ingestor. This is a trick to have tests in the same class of its ingestor.</li> <li>Create the configuration inside django admin in <code>Ingestors_manager/IngestorConfigs</code> (* = mandatory, ~ = mandatory on conditions)<ol> <li>*Name: specific name of the configuration</li> <li>*Python module: . <li>*Description: description of the configuration</li> <li>*Routing key: celery queue that will be used</li> <li>*Soft_time_limit: maximum time for the task execution</li> <li>*Playbook to Execute: Playbook that will be executed on every IOC retrieved</li> <li>*Schedule: Crontab object that describes the schedule of the ingestor. You are able to create a new clicking the <code>plus</code> symbol.</li>"},{"location":"IntelOwl/contribute/#how-to-add-a-new-pivot","title":"How to add a new Pivot","text":"<ol> <li>Put the module in the <code>pivots</code> directory</li> <li>Remember to use <code>_monkeypatch()</code> in its class to create automated tests for the new pivot. This is a trick to have tests in the same class of its pivot.</li> <li>Create the configuration inside django admin in <code>Pivots_manager/PivotConfigs</code> (* = mandatory, ~ = mandatory on conditions)<ol> <li>*Name: specific name of the configuration</li> <li>*Python module: . <li>*Description: description of the configuration</li> <li>*Routing key: celery queue that will be used</li> <li>*Soft_time_limit: maximum time for the task execution</li> <li>*Playbook to Execute: Playbook that will be executed in the Job generated by the Pivot</li> <p>Most of the times you don't need to create a new Pivot Module. There are already some base modules that can be extended. The most important ones are the following 2: - 1.<code>AnyCompare</code>: use this module if you want to create a custom Pivot from a specific value extracted from the results of the analyzers/connectors. How? you should populate the parameter <code>field_to_compare</code> with the dotted path to the field you would like to extract the value from. - 2.<code>SelfAnalyzable</code>: use this module if you want to create a custom Pivot that would analyze again the same observable/file.</p>"},{"location":"IntelOwl/contribute/#how-to-add-a-new-visualizer","title":"How to add a new Visualizer","text":""},{"location":"IntelOwl/contribute/#configuration","title":"Configuration","text":"<ol> <li>Put the module in the <code>visualizers</code> directory</li> <li>Remember to use <code>_monkeypatch()</code> in its class to create automated tests for the new visualizer. This is a trick to have tests in the same class of its visualizer.</li> <li>Create the configuration inside django admin in <code>Visualizers_manager/VisualizerConfigs</code> (* = mandatory, ~ = mandatory on conditions)<ol> <li>*Name: specific name of the configuration</li> <li>*Python module: . <li>*Description: description of the configuration</li> <li>*Config:     1. *Queue: celery queue that will be used     2. *Soft_time_limit: maximum time for the task execution</li> <li>*Playbook: Playbook that must have run to execute the visualizer</li>"},{"location":"IntelOwl/contribute/#python-class","title":"Python class","text":"<p>The visualizers' python code could be not immediate, so a small digression on how it works is necessary. Visualizers have as goal to create a data structure inside the <code>Report</code> that the frontend is able to parse and correctly visualize on the page. To do so, some utility classes have been made:</p> Class Description Visual representation/example VisualizablePage A single page of the final report, made of different levels. Each page added is represented as a new tab in frontend. VisualizableLevel        Each level corresponds to a line in the final frontend visualizations. Every level is made of a        VisualizableHorizontalList.       The dimension of the level can be customized with the size parameter (1 is the biggest, 6 is the smallest).       VisualizableHorizontalList An horizontal list of visualizable elements. In the example there is an horizontal list of vertical lists. VisualizableVerticalList A vertical list made of a name, a title, and the list of elements. VisualizableTable A table of visualizable elements. In the example there is a table of base and vertical lists. VisualizableBool The representation of a boolean value. It can be enabled or disabled with colors. VisualizableTitle The representation of a tuple, composed of a title and a value. VisualizableDownload A button useful for download a file with custom content. VisualizableBase The representation of a base string. Can have a link attached to it and even an icon. The background color can be changed. The title above is composed by two <code>VisualizableBase</code> <p>Inside a <code>Visualizer</code> you can retrieve the reports of the analyzers and connectors  that have been specified inside configuration of the Visualizer itself using <code>.analyzer_reports()</code> and <code>.connector_reports()</code>. At this point, you can compose these values as you wish wrapping them with the <code>Visualizable</code> classes mentioned before.</p> <p>The best way to create a visualizer is to define several methods, one for each <code>Visualizable</code> you want to show in the UI, in your new visualizer and decore them with <code>visualizable_error_handler_with_params</code>. This decorator handles exceptions: in case there is a bug during the generation of a Visualizable element, it will be show an error instead of this component and all the other Visualizable are safe and will render correctly. Be careful using it because is a function returning a decorator! This means you need to use a syntax like this:</p> <pre><code>@visualizable_error_handler_with_params(error_name=\"custom visualizable\", error_size=VisualizableSize.S_2)\ndef custom_visualizable(self):\n   ...\n</code></pre> <p>instead of the syntax of other decorators that doesn't need the function call.</p> <p>You may want to look at a few existing examples to start to build a new one:</p> <ul> <li>dns.py</li> <li>yara.py</li> </ul>"},{"location":"IntelOwl/contribute/#how-to-share-your-plugin-with-the-community","title":"How to share your plugin with the community","text":"<p>To allow other people to use your configuration, that is now stored in your local database, you have to export it and create a data migration</p> <ol> <li>You can use the django management command <code>dumpplugin</code> to automatically create the migration file for your new analyzer (you will find it under <code>api_app/YOUR_PLUGIN_manager/migrations</code>). The script will create the following models:     1. PythonModule     2. AnalyzerConfig     3. Parameter     4. PluginConfig</li> <li>Example: <code>docker exec -ti intelowl_uwsgi python3 manage.py dumpplugin AnalyzerConfig &lt;new_analyzer_name&gt;</code></li> </ol> <p>Add the new analyzer in the lists in the docs: Usage. Also, if the analyzer provides additional optional configuration, add the available options here: Advanced-Usage</p> <p>In the Pull Request remember to provide some real world examples (screenshots and raw JSON results) of some successful executions of the analyzer to let us understand how it would work.</p>"},{"location":"IntelOwl/contribute/#how-to-add-a-new-playbook","title":"How to add a new Playbook","text":"<ol> <li>Create the configuration inside django admin in <code>Playbooks_manager/PlaybookConfigs</code> (* = mandatory, ~ = mandatory on conditions)<ol> <li>*Name: specific name of the configuration</li> <li>*Description: description of the configuration</li> <li>*Type: list of types that are supported by the playbook</li> <li>*Analyzers: List of analyzers that will be run</li> <li>*Connectors: List of connectors that will be run</li> </ol> </li> </ol>"},{"location":"IntelOwl/contribute/#how-to-share-your-playbook-with-the-community","title":"How to share your playbook with the community","text":"<p>To allow other people to use your configuration, that is now stored in your local database, you have to export it and create a data migration You can use the django management command <code>dumpplugin</code> to automatically create the migration file for your new analyzer (you will find it under <code>api_app/playbook_manager/migrations</code>).</p> <p>Example: <code>docker exec -ti intelowl_uwsgi python3 manage.py dumpplugin PlaybookConfig &lt;new_analyzer_name&gt;</code></p>"},{"location":"IntelOwl/contribute/#how-to-modify-a-plugin","title":"How to modify a plugin","text":"<p>If the changes that you have to make should stay local, you can just change the configuration inside the <code>Django admin</code> page.</p> <p>But if, instead, you want your changes to be usable by every IntelOwl user, you have to create a new migration.</p> <p>To do so, you can use the following snippets as an example:</p> <ol> <li>You have to create a new migration file</li> <li>Add as dependency the previous last migration of the package</li> <li>You have to create a forward and a reverse function</li> <li>You have to make the proper changes of the configuration inside these functions (change parameters, secrets, or even delete the configuration)<ol> <li>If changes are made, you have to validate the instance calling <code>.full_clean()</code> and then you can save the instance with <code>.save()</code></li> </ol> </li> </ol>"},{"location":"IntelOwl/contribute/#example-how-to-add-a-new-parameter-in-the-configuration-with-a-default-value","title":"Example: how to add a new parameter in the configuration with a default value","text":"<pre><code>def migrate(apps, schema_editor):\n   PythonModule = apps.get_model(\"api_app\", \"PythonModule\")\n   Parameter = apps.get_model(\"api_app\", \"Parameter\")\n   PluginConfig = apps.get_model(\"api_app\", \"PluginConfig\")\n   pm = PythonModule.objects.get(module=\"test.Test\", base_path=\"api_app.connectors_manager.connectors\")\n   p = Parameter(name=\"mynewfield\", type=\"str\", description=\"Test field\", is_secret=False, required=True, python_module=pm)\n   p.full_clean()\n   p.save()\n   for connector in pm.connectorconfigs.all():\n    pc = PluginConfig(value=\"test\", connector_config=connector, python_module=pm, for_organization=False, owner=None, parameter=p)\n    pc.full_clean()\n    pc.save()\n</code></pre>"},{"location":"IntelOwl/contribute/#example-how-to-add-a-new-secret-in-the-configuration","title":"Example: how to add a new secret in the configuration","text":"<pre><code>def migrate(apps, schema_editor):\n   PythonModule = apps.get_model(\"api_app\", \"PythonModule\")\n   Parameter = apps.get_model(\"api_app\", \"Parameter\")\n   pm = PythonModule.objects.get(module=\"test.Test\", base_path=\"api_app.connectors_manager.connectors\")\n   p = Parameter(name=\"mynewsecret\", type=\"str\", description=\"Test field\", is_secret=True, required=True, python_module=pm)\n   p.full_clean()\n   p.save()\n</code></pre>"},{"location":"IntelOwl/contribute/#example-how-to-delete-a-parameter","title":"Example: how to delete a parameter","text":"<pre><code>def migrate(apps, schema_editor):\n   PythonModule = apps.get_model(\"api_app\", \"PythonModule\")\n   Parameter = apps.get_model(\"api_app\", \"Parameter\")\n   pm = PythonModule.objects.get(module=\"test.Test\", base_path=\"api_app.connectors_manager.connectors\")\n   Parameter.objects.get(name=\"myoldfield\", python_module=pm).delete()\n</code></pre>"},{"location":"IntelOwl/contribute/#example-how-to-change-the-default-value-of-a-parameter","title":"Example: how to change the default value of a parameter","text":"<pre><code>def migrate(apps, schema_editor):\n   PythonModule = apps.get_model(\"api_app\", \"PythonModule\")\n   Parameter = apps.get_model(\"api_app\", \"Parameter\")\n   PluginConfig = apps.get_model(\"api_app\", \"PluginConfig\")\n   pm = PythonModule.objects.get(module=\"test.Test\", base_path=\"api_app.connectors_manager.connectors\")\n   p = Parameter.objects.get(name=\"myfield\", python_module=pm)\n   PluginConfig.objects.filter(parameter=p, python_module=pm, for_organization=False, owner=None ).update(value=\"newvalue\")\n</code></pre>"},{"location":"IntelOwl/contribute/#modifying-functionalities-of-the-certego-packages","title":"Modifying functionalities of the Certego packages","text":"<p>Since v4, IntelOwl leverages some packages from Certego:</p> <ul> <li>certego-saas that integrates some common reusable Django applications and tools that can be used for generic services.</li> <li>certego-ui that contains reusable React components for the UI.</li> </ul> <p>If you need to modify the behavior or add feature to those packages, please follow the same rules for IntelOwl and request a Pull Request there. The same maintainers of IntelOwl will answer to you.</p> <p>Follow these guides to understand how to start to contribute to them while developing for IntelOwl:</p> <ul> <li>certego-saas: create a fork, commit your changes in your local repo, then change the commit hash to the last one you made in the requirements file. Ultimately re-build the project</li> <li>certego-ui: Frontend doc</li> </ul>"},{"location":"IntelOwl/contribute/#how-to-test-the-application","title":"How to test the application","text":"<p>IntelOwl makes use of the django testing framework and the <code>unittest</code> library for unit testing of the API endpoints and End-to-End testing of the analyzers and connectors.</p>"},{"location":"IntelOwl/contribute/#configuration_1","title":"Configuration","text":"<ul> <li>In the encrypted folder <code>tests/test_files.zip</code> (password: \"intelowl\") there are some files that you can use for testing purposes.</li> </ul> <ul> <li> <p>With the following environment variables you can customize your tests:</p> <ul> <li><code>DISABLE_LOGGING_TEST</code> -&gt; disable logging to get a clear output</li> <li><code>MOCK_CONNECTIONS</code> -&gt; mock connections to external API to test the analyzers without a real connection or a valid API key</li> </ul> </li> </ul> <ul> <li>If you prefer to use custom inputs for tests, you can change the following environment variables in the environment file based on the data you would like to test:<ul> <li><code>TEST_MD5</code></li> <li><code>TEST_URL</code></li> <li><code>TEST_IP</code></li> <li><code>TEST_DOMAIN</code></li> </ul> </li> </ul>"},{"location":"IntelOwl/contribute/#setup-containers","title":"Setup containers","text":"<p>The point here is to launch the code in your environment and not the last official image in Docker Hub. For this, use the <code>test</code> or the <code>ci</code> option when launching the containers with the <code>./start</code> script.</p> <ul> <li>Use the <code>test</code> option to actually execute tests that simulate a real world environment without mocking connections.</li> <li>Use the <code>ci</code> option to execute tests in a CI environment where connections are mocked.</li> </ul> <pre><code>$ ./start test up\n$ # which corresponds to the command: docker-compose -f docker/default.yml -f docker/test.override.yml up\n</code></pre>"},{"location":"IntelOwl/contribute/#launch-tests","title":"Launch tests","text":"<p>Now that the containers are up, we can launch the test suite.</p>"},{"location":"IntelOwl/contribute/#backend_1","title":"Backend","text":""},{"location":"IntelOwl/contribute/#run-all-tests","title":"Run all tests","text":"<p>Examples:</p> <pre><code>$ docker exec intelowl_uwsgi python3 manage.py test\n</code></pre>"},{"location":"IntelOwl/contribute/#run-tests-for-a-particular-plugin","title":"Run tests for a particular plugin","text":"<p>To test a plugin in real environment, i.e. without mocked data, we suggest that you use the GUI of IntelOwl directly. Meaning that you have your plugin configured, you have selected a correct observable/file to analyze, and the final report shown in the GUI of IntelOwl is exactly what you wanted.</p>"},{"location":"IntelOwl/contribute/#run-tests-available-in-a-particular-file","title":"Run tests available in a particular file","text":"<p>Examples:</p> <pre><code>$ docker exec intelowl_uwsgi python3 manage.py test tests.api_app tests.test_crons # dotted paths\n</code></pre>"},{"location":"IntelOwl/contribute/#frontend_1","title":"Frontend","text":"<p>All the frontend tests must be run from the folder <code>frontend</code>. The tests can contain log messages, you can suppress then with the environment variable <code>SUPPRESS_JEST_LOG=True</code>.</p>"},{"location":"IntelOwl/contribute/#run-all-tests_1","title":"Run all tests","text":"<pre><code>npm test\n</code></pre>"},{"location":"IntelOwl/contribute/#run-a-specific-component-tests","title":"Run a specific component tests","text":"<pre><code>npm test -- -t &lt;componentPath&gt;\n// example\nnpm test tests/components/auth/Login.test.jsx\n</code></pre>"},{"location":"IntelOwl/contribute/#run-a-specific-test","title":"Run a specific test","text":"<pre><code>npm test -- -t '&lt;describeString&gt; &lt;testString&gt;'\n// example\nnpm test -- -t \"Login component User login\"\n</code></pre>"},{"location":"IntelOwl/contribute/#create-a-pull-request","title":"Create a pull request","text":""},{"location":"IntelOwl/contribute/#remember","title":"Remember!!!","text":"<p>Please create pull requests only for the branch develop. That code will be pushed to master only on a new release.</p> <p>Also remember to pull the most recent changes available in the develop branch before submitting your PR. If your PR has merge conflicts caused by this behavior, it won't be accepted.</p>"},{"location":"IntelOwl/contribute/#install-testing-requirements","title":"Install testing requirements","text":"<p>Run <code>pip install -r requirements/test-requirements.txt</code> to install the requirements to validate your code.</p>"},{"location":"IntelOwl/contribute/#pass-linting-and-tests","title":"Pass linting and tests","text":"<ol> <li>Run <code>psf/black</code> to lint the files automatically, then <code>flake8</code> to check and <code>isort</code>:</li> </ol> <p>(if you installed <code>pre-commit</code> this is performed automatically at every commit)</p> <pre><code>$ black . --exclude \"migrations|venv\"\n$ flake8 . --show-source --statistics\n$ isort . --profile black --filter-files --skip venv\n</code></pre> <p>if flake8 shows any errors, fix them.</p> <ol> <li>Run the build and start the app using the docker-compose test file. In this way, you would launch the code in your environment and not the last official image in Docker Hub:</li> </ol> <pre><code>$ ./start ci build\n$ ./start ci up\n</code></pre> <ol> <li>Here, we simulate the GitHub CI tests locally by running the following 3 tests:</li> </ol> <pre><code>$ docker exec -ti intelowl_uwsgi unzip -P intelowl tests/test_files.zip -d test_files\n$ docker exec -ti intelowl_uwsgi python manage.py test tests\n</code></pre> <p>Note: IntelOwl has dynamic testing suite. This means that no explicit analyzers/connector tests are required after the addition of a new analyzer or connector.</p> <p>If everything is working, before submitting your pull request, please squash your commits into a single one!</p>"},{"location":"IntelOwl/contribute/#how-to-squash-commits-to-a-single-one","title":"How to squash commits to a single one","text":"<ul> <li>Run <code>git rebase -i HEAD~[NUMBER OF COMMITS]</code></li> <li>You should see a list of commits, each commit starting with the word \"pick\".</li> <li>Make sure the first commit says \"pick\" and change the rest from \"pick\" to \"squash\". -- This will squash each commit into the previous commit, which will continue until every commit is squashed into the first commit.</li> <li>Save and close the editor.</li> <li>It will give you the opportunity to change the commit message.</li> <li>Save and close the editor again.</li> <li>Then you have to force push the final, squashed commit: <code>git push --force-with-lease origin</code>.</li> </ul> <p>Squashing commits can be a tricky process but once you figure it out, it's really helpful and keeps our repo concise and clean.</p>"},{"location":"IntelOwl/contribute/#debug-application-problems","title":"Debug application problems","text":"<p>Keep in mind that, if any errors arise during development, you would need to check the application logs to better understand what is happening so you can easily address the problem.</p> <p>This is the reason why it is important to add tons of logs in the application...if they are not available in time of needs you would cry really a lot.</p> <p>Where are IntelOwl logs? With a default installation of IntelOwl, you would be able to get the application data from the following paths in your OS:</p> <ul> <li><code>/var/lib/docker/volumes/intel_owl_generic_logs/_data/django</code>: Django Application logs</li> <li><code>/var/lib/docker/volumes/intel_owl_generic_logs/_data/uwsgi</code>: Uwsgi application server logs</li> <li><code>/var/lib/docker/volumes/intel_owl_nginx_logs/_data/</code>: Nginx Web Server Logs</li> </ul>"},{"location":"IntelOwl/installation/","title":"Installation","text":""},{"location":"IntelOwl/installation/#requirements","title":"Requirements","text":"<p>The project leverages <code>docker compose</code> with a custom Bash script and you need to have the following packages installed in your machine:</p> <ul> <li>docker - v19.03.0+</li> <li>docker-compose - v2.3.4+</li> </ul> <p>In some systems you could find pre-installed older versions. Please check this and install a supported version before attempting the installation. Otherwise it would fail. Note: We've added a new Bash script <code>initialize.sh</code> that will check compatibility with your system and attempt to install the required dependencies.</p> <p>Note</p> <ul> <li>The project uses public docker images that are available on Docker Hub</li> <li>IntelOwl is tested and supported to work in a Debian distro. More precisely we suggest using Ubuntu. Other Linux-based OS should work but that has not been tested much. It may also run on Windows, but that is not officially supported.</li> <li>IntelOwl does not support ARM at the moment. We'll fix this with the next v6.0.5 release <li>Before installing remember that you must comply with the LICENSE and the Legal Terms</li> <p>Warning</p> The <code>start</code> script requires a `bash` version &gt; 4 to run.  Note that macOS is shipped with an older version of <code>bash</code>. Please ensure to upgrade before running the script."},{"location":"IntelOwl/installation/#tldr","title":"TL;DR","text":"<p>Obviously we strongly suggest reading through all the page to configure IntelOwl in the most appropriate way.</p> <p>However, if you feel lazy, you could just install and test IntelOwl with the following steps. <code>docker</code> will be run with <code>sudo</code> if permissions/roles have not been set.</p> <pre><code># clone the IntelOwl project repository\ngit clone https://github.com/intelowlproject/IntelOwl\ncd IntelOwl/\n\n# run helper script to verify installed dependencies and configure basic stuff\n./initialize.sh\n\n# start the app\n./start prod up\n# now the application is running on http://localhost:80\n\n# create a super user\nsudo docker exec -ti intelowl_uwsgi python3 manage.py createsuperuser\n\n# now you can login with the created user from http://localhost:80/login\n\n# Have fun!\n</code></pre> <p>Warning</p> The first time you start IntelOwl, a lot of database migrations are being applied. This requires some time. If you get 500 status code errors in the GUI, just wait few minutes and then refresh the page."},{"location":"IntelOwl/installation/#infrastructure-requirements","title":"Infrastructure Requirements","text":"<p>These are our recommendations for dedicated deployments of IntelOwl:</p> <ul> <li>Basic Installation in a VM: 2 CPU, 4GB RAM, 20GB Disk</li> <li>Intensive Usage (hundreds of analysis in a hour) in a single VM: 8CPU, 16GB RAM and 80GB Disk.</li> </ul> <p>Please remember that every environment has its own peculiarities so these numbers must not be taken as the holy grail.</p> <p>What should be done is a comprehensive evaluation of the environment where the application will deployed.</p> <p>For more complex environments, a Docker Swarm / Kubernetes cluster is recommended.</p> <p>IntelOwl's maintainers are available to offer paid consultancy and mentorship about that.</p>"},{"location":"IntelOwl/installation/#deployment-components","title":"Deployment Components","text":"<p>IntelOwl is composed of various different technologies, namely:</p> <ul> <li>React: Frontend, using CRA and certego-ui</li> <li>Django: Backend</li> <li>PostgreSQL: Database</li> <li>Redis: Message Broker</li> <li>Celery: Task Queue</li> <li>Nginx: Reverse proxy for the Django API and web asssets.</li> <li>Uwsgi: Application Server</li> <li>Daphne: Asgi Server for WebSockets</li> <li>Elastic Search (optional): Auto-sync indexing of analysis' results.</li> <li>Flower (optional): Celery Management Web Interface</li> </ul> <p>All these components are managed via <code>docker compose</code>.</p>"},{"location":"IntelOwl/installation/#deployment-preparation","title":"Deployment Preparation","text":"<ul> <li>Environment configuration (required)</li> <li>Database configuration (required)</li> <li>Web server configuration (optional)</li> <li>Analyzers configuration (optional)</li> </ul> <p>Open a terminal and execute below commands to construct new environment files from provided templates.</p> <pre><code>./initialize.sh\n</code></pre>"},{"location":"IntelOwl/installation/#environment-configuration-required","title":"Environment configuration (required)","text":"<p>In the <code>docker/env_file_app</code>, configure different variables as explained below.</p> <p>REQUIRED variables to run the image:</p> <ul> <li><code>DB_HOST</code>, <code>DB_PORT</code>, <code>DB_USER</code>, <code>DB_PASSWORD</code>: PostgreSQL configuration (The DB credentals should match the ones in the <code>env_file_postgres</code>). If you like, you can configure the connection to an external PostgreSQL instance in the same variables. Then, to avoid to run PostgreSQL locally, please run IntelOwl with the option <code>--use-external-database</code>. Otherwise, <code>DB_HOST</code> must be <code>postgres</code> to have the app properly communicate with the PostgreSQL container.</li> <li> <p><code>DJANGO_SECRET</code>: random 50 chars key, must be unique. If you do not provide one, Intel Owl will automatically set a secret key and use the same for each run. The key is generated by <code>initialize.sh</code> script.</p> <p>Strongly recommended variable to set:</p> </li> </ul> <ul> <li><code>INTELOWL_WEB_CLIENT_DOMAIN</code> (example: <code>localhost</code>/<code>mywebsite.com</code>): the web domain of your instance, this is used for generating links to analysis results.</li> </ul> <p>Optional configuration:</p> <ul> <li><code>OLD_JOBS_RETENTION_DAYS</code>: Database retention for analysis results (default: 14 days). Change this if you want to keep your old analysis longer in the database.</li> </ul>"},{"location":"IntelOwl/installation/#other-optional-configuration-to-enable-specific-services-features","title":"Other optional configuration to enable specific services / features","text":"<p>Configuration required to enable integration with Slack:</p> <ul> <li><code>SLACK_TOKEN</code>: Slack token of your Slack application that will be used to send/receive notifications</li> <li><code>DEFAULT_SLACK_CHANNEL</code>: ID of the Slack channel you want to post the message to</li> </ul> <p>Configuration required to have InteOwl sending Emails (registration requests, mail verification, password reset/change, etc)</p> <ul> <li><code>DEFAULT_FROM_EMAIL</code>: email address used for automated correspondence from the site manager (example: <code>noreply@mydomain.com</code>)</li> <li><code>DEFAULT_EMAIL</code>: email address used for correspondence with users (example: <code>info@mydomain.com</code>)</li> <li><code>EMAIL_HOST</code>: the host to use for sending email with SMTP</li> <li><code>EMAIL_HOST_USER</code>: username to use for the SMTP server defined in EMAIL_HOST</li> <li><code>EMAIL_HOST_PASSWORD</code>: password to use for the SMTP server defined in EMAIL_HOST. This setting is used in conjunction with EMAIL_HOST_USER when authenticating to the SMTP server.</li> <li><code>EMAIL_PORT</code>: port to use for the SMTP server defined in EMAIL_HOST.</li> <li><code>EMAIL_USE_TLS</code>: whether to use an explicit TLS (secure) connection when talking to the SMTP server, generally used on port 587.</li> <li><code>EMAIL_USE_SSL</code>: whether to use an implicit TLS (secure) connection when talking to the SMTP server, generally used on port 465.</li> </ul>"},{"location":"IntelOwl/installation/#database-configuration-required-if-running-postgresql-locally","title":"Database configuration (required if running PostgreSQL locally)","text":"<p>If you use a local PostgreSQL instance (this is the default), in the <code>env_file_postgres</code> you have to configure different variables as explained below.</p> <p>Required variables:</p> <ul> <li><code>POSTGRES_PASSWORD</code> (same as <code>DB_PASSWORD</code>)</li> <li><code>POSTGRES_USER</code> (same as <code>DB_USER</code>)</li> <li><code>POSTGRES_DB</code> (default: <code>intel_owl_db</code>)</li> </ul>"},{"location":"IntelOwl/installation/#logrotate-configuration-strongly-recommended","title":"Logrotate configuration (strongly recommended)","text":"<p>If you want to have your logs rotated correctly, we suggest you to add the configuration for the system Logrotate. To do that you can leverage the <code>initialize.sh</code> script. Otherwise, if you have skipped that part, you can manually install logrotate by launching the following script:</p> <pre><code>cd ./docker/scripts\n./install_logrotate.sh\n</code></pre> <p>We decided to do not leverage Django Rotation Configuration because it caused problematic concurrency issues, leading to logs that are not rotated correctly and to apps that do not log anymore. Logrotate configuration is more stable.</p>"},{"location":"IntelOwl/installation/#crontab-configuration-recommended-for-advanced-deployments","title":"Crontab configuration (recommended for advanced deployments)","text":"<p>We added few Crontab configurations that could be installed in the host machine at system level to solve some possible edge-case issues:</p> <ul> <li>Memory leaks: Once a week it is suggested to do a full restart of the application to clean-up the memory used by the application. Practical experience suggest us to do that to solve some recurrent memory issues in Celery. A cron called <code>application_restart</code> has been added for this purpose (it uses the absolute path of <code>start</code> script in the container). This cron assumes that you have executed IntelOwl with the parameters <code>--all_analyzers</code>. If you didn't, feel free to change the cron as you wish.</li> </ul> <p>This configuration is optional but strongly recommended for people who want to have a production grade deployment. To install it you need to run the following script in each deployed server:</p> <pre><code>cd ./docker/scripts\n./install_crontab.sh\n</code></pre>"},{"location":"IntelOwl/installation/#web-server-configuration-required-for-enabling-https","title":"Web server configuration (required for enabling HTTPS)","text":"<p>Intel Owl provides basic configuration for:</p> <ul> <li>Nginx (<code>configuration/nginx/http.conf</code>)</li> <li>Uwsgi (<code>configuration/intel_owl.ini</code>)</li> </ul> <p>In case you enable HTTPS, remember to set the environment variable <code>HTTPS_ENABLED</code> as \"enabled\" to increment the security of the application.</p> <p>There are 3 options to execute the web server:</p> <ul> <li> <p>HTTP only (default)</p> <p>The project would use the default deployment configuration and HTTP only.</p> </li> </ul> <ul> <li> <p>HTTPS with your own certificate</p> <p>The project provides a template file to configure Nginx to serve HTTPS: <code>configuration/nginx/https.conf</code>.</p> <p>You should change <code>ssl_certificate</code>, <code>ssl_certificate_key</code> and <code>server_name</code> in that file and put those required files in the specified locations.</p> <p>Then you should call the <code>./start</code> script with the parameter <code>--https</code> to leverage the right Docker Compose file for HTTPS.</p> <p>Plus, if you use Flower, you should change in the <code>docker/flower.override.yml</code> the <code>flower_http.conf</code> with <code>flower_https.conf</code>.</p> </li> </ul> <ul> <li> <p>HTTPS with Let's Encrypt</p> <p>We provide a specific docker-compose file that leverages Traefik to allow fast deployments of public-faced and HTTPS-enabled applications.</p> <p>Before using it, you should configure the configuration file <code>docker/traefik.override.yml</code> by changing the email address and the hostname where the application is served. For a detailed explanation follow the official documentation: Traefix doc.</p> <p>After the configuration is done, you can add the option <code>--traefik</code> while executing <code>./start</code></p> </li> </ul>"},{"location":"IntelOwl/installation/#run","title":"Run","text":"<p>Important Info</p> IntelOwl depends heavily on docker and docker compose so as to hide this complexity from the enduser the project leverages a custom shell script (<code>start</code>) to interface with <code>docker compose</code>.  You may invoke <code>$ ./start --help</code> to get help and usage info.  The CLI provides the primitives to correctly build, run or stop the containers for IntelOwl. Therefore,  <ul> <li>It is possible to attach every optional docker container that IntelOwl has: multi_queue with traefik enabled while every optional docker analyzer is active.</li> <li>It is possible to insert an optional docker argument that the CLI will pass to <code>docker compose</code></li> </ul> <p>Now that you have completed different configurations, starting the containers is as simple as invoking:</p> <pre><code>$ ./start prod up\n</code></pre> <p>You can add the <code>docker</code> options <code>-d</code> to run the application in the background.</p> <p>Important Info</p> All <code>docker</code> and <code>docker compose</code> specific options must be passed at the end of the script, after a <code>--</code> token. This token indicates the end of IntelOwl's options and the beginning of Docker options.  Example:  <pre><code>./start prod up -- -d\n</code></pre> <p>Hint</p> Starting from IntelOwl 4.0.0, with the startup script you can select which version of IntelOwl you want to run (<code>--version</code>). This  can be helpful to keep using old versions in case of retrocompatibility issues. The <code>--version</code> parameter checks out the Git Repository to the Tag of the version that you have chosen. This means that if you checkout to a v3.x.x version, you won't have the <code>--version</code> parameter anymore so you would need to manually checkout back to the <code>master</code> branch to use newer versions.  <p>Warning</p> If, for any reason, the <code>start</code> script does not work in your environment, we suggest to use plain <code>docker compose</code> and configuring manually all the optional containers you need.  The basic replacement of <code>./start prod up</code> would be: <pre><code>docker compose --project-directory docker -f docker/default.yml -f docker/postgres.override.yml -f docker/redis.override.yml -f docker/nginx.override.yml -p intelowl up\n</code></pre>"},{"location":"IntelOwl/installation/#stop","title":"Stop","text":"<p>To stop the application you have to:</p> <ul> <li>if executed without <code>-d</code> parameter: press <code>CTRL+C</code></li> <li>if executed with <code>-d</code> parameter: <code>./start prod down</code></li> </ul>"},{"location":"IntelOwl/installation/#cleanup-of-database-and-application","title":"Cleanup of database and application","text":"<p>This is a destructive operation but can be useful to start again the project from scratch</p> <p><code>./start prod down -- -v</code></p>"},{"location":"IntelOwl/installation/#get-the-experimental-features-in-the-develop-branch","title":"Get the experimental features in the develop branch","text":"<p>If you cannot wait for official releases and you want to leverage the most recent features added in the development branch, you can do it!</p> <p>Follow these steps: <pre><code># go to your IntelOwl directory\ncd /home/user/IntelOwl\n# switch to the develop branch\ngit checkout develop\n# locally build the development branch\n./start test build\n# run IntelOwl\n./start test up\n</code></pre></p>"},{"location":"IntelOwl/installation/#after-deployment","title":"After Deployment","text":""},{"location":"IntelOwl/installation/#users-creation","title":"Users creation","text":"<p>You may want to run <code>docker exec -ti intelowl_uwsgi python3 manage.py createsuperuser</code> after first run to create a superuser. Then you can add other users directly from the Django Admin Interface after having logged with the superuser account. To manage users, organizations and their visibility please refer to this section</p>"},{"location":"IntelOwl/installation/#update-and-rebuild","title":"Update and Rebuild","text":""},{"location":"IntelOwl/installation/#rebuilding-the-project-creating-custom-docker-build","title":"Rebuilding the project / Creating custom docker build","text":"<p>If you make some code changes and you like to rebuild the project, follow these steps:</p> <ol> <li><code>./start test build -- --tag=&lt;your_tag&gt; .</code> to build the new docker image.</li> <li>Add this new image tag in the <code>docker/test.override.yml</code> file.</li> <li>Start the containers with <code>./start test up -- --build</code>.</li> </ol>"},{"location":"IntelOwl/installation/#update-to-the-most-recent-version","title":"Update to the most recent version","text":"<p>To update the project with the most recent available code you have to follow these steps:</p> <pre><code>$ cd &lt;your_intel_owl_directory&gt; # go into the project directory\n$ git pull # pull new changes\n$ ./start prod down # kill and destroy the currently running IntelOwl containers\n$ ./start prod up # restart the IntelOwl application\n</code></pre> <p>Note</p> After an upgrade, sometimes a database error in Celery Containers could happen. That could be related to new DB migrations which are not applied by the main Uwsgi Container yet. Do not worry. Wait few seconds for the Uwsgi container to start correctly, then put down the application again and restart it. The problem should be solved. If not, please feel free to open an issue on Github  <p>Note</p> After having upgraded IntelOwl, in case the application does not start and you get an error like this:  <pre><code>PermissionError: [Errno 13] Permission denied: '/var/log/intel_owl/django/authentication.log\n</code></pre>  just run this:  <pre><code>sudo chown -R www-data:www-data /var/lib/docker/volumes/intel_owl_generic_logs/_data/django\n</code></pre>  and restart IntelOwl. It should solve the permissions problem.   <p>Warning</p> Major versions of IntelOwl are usually incompatible from one another. Maintainers strive to keep the upgrade between major version easy but it's not always like that. Below you can find the additional process required to upgrade from each major versions."},{"location":"IntelOwl/installation/#updating-to-600-from-a-5xx-version","title":"Updating to &gt;=6.0.0 from a 5.x.x version","text":"<p>IntelOwl v6 introduced some major changes regarding how the project is started. Before upgrading, some important things should be checked by the administrator:</p> <ul> <li>Docker Compose V1 support has been dropped project-wide. If you are still using a Compose version prior to v2.3.4, please upgrade to a newer version or install Docker Compose V2.</li> <li>IntelOwl is now started with the new Bash <code>start</code> script that has the same options as the old Python <code>start.py</code> script but is more manageable and has decreased the overall project dependencies. The <code>start.py</code> script has now been removed. Please use the new <code>start</code> script instead.</li> <li>The default message broker is now Redis. We have replaced Rabbit-MQ for Redis to allow support for Websockets in the application:<ul> <li>This change is transparent if you use our <code>start</code> script to run IntelOwl. That would spawn a Redis instance instead of a Rabbit-MQ one locally.</li> <li>If you were using an external broker like AWS SQS or a managed Rabbit-MQ, they are still supported but we suggest to move to a Redis supported service to simplify the architecture (because Redis is now mandatory for Websockets)</li> </ul> </li> <li>Support for multiple jobs with multiple playbooks has been removed. Every Observable or File in the request will be processed by a single playbook.</li> <li>We upgraded the base PostgreSQL image from version 12 to version 16. You have 2 choice:<ul> <li>remove your actual database and start from scratch with a new one</li> <li>maintain your database and do not update Postgres. This could break the application at anytime because we do not support it anymore.</li> <li>if you want to keep your old DB, follow the migration procedure you can find below</li> </ul> </li> </ul> <p>Warning</p> CARE! We are providing this database migration procedure to help the users to migrate to a new PostgreSQL version.  Upgrading PostgreSQL is outside the scope of the IntelOwl project so we do not guarantee that everything will work as intended.  In case of doubt, please check the official PostgreSQL documentation.  Upgrade at your own risk.   <p>The database migration procedure is as follows:</p> <ul> <li>You have IntelOwl version 5.x.x up and running</li> <li>Bring down the application (you can use the start script or manually concatenate your docker compose configuration )</li> <li>Go inside the docker folder <code>cd docker</code></li> <li>Bring only the postgres 12 container up <code>docker run -d --name intelowl_postgres_12 -v intel_owl_postgres_data:/var/lib/postgresql/data/ --env-file env_file_postgres  library/postgres:12-alpine</code></li> <li>Dump the entire database. You need the user and the database that you configured during startup for this <code>docker exec -t intelowl_postgres_12  pg_dump -U &lt;POSTGRES_USER&gt; -d &lt;POSTGRES_DB&gt; --no-owner &gt; /tmp/dump_intelowl.sql</code></li> <li>Stop che container <code>docker container stop intelowl_postgres_12</code></li> <li>Remove the backup container <code>docker container rm intelowl_postgres_12</code></li> <li>Remove the postgres volume <code>docker volume rm intel_owl_postgres_data</code> &lt;------------- remove old data, this is not exactly necessary because the new postgres has a different volume name</li> <li>Start the intermediary postgres 16 container <code>docker run -d --name intelowl_postgres_16 -v intelowl_postgres_data:/var/lib/postgresql/data/ --env-file env_file_postgres  library/postgres:16-alpine</code></li> <li>Add the data to the volume <code>cat /tmp/dump_intelowl.sql | docker exec -i intelowl_postgres_16 psql -U &lt;POSTGRES_USER&gt; -d &lt;POSTGRES_DB&gt;</code></li> <li>Stop the intermediary container <code>docker container stop intelowl_postgres_16</code></li> <li>Remove the intermediary container <code>docker container rm intelowl_postgres_16</code></li> <li>Update IntelOwl to the latest version</li> <li>Bring up the application back again (you can use the start script or manually concatenate your docker compose configuration)</li> </ul>"},{"location":"IntelOwl/installation/#updating-to-500-from-a-4xx-version","title":"Updating to &gt;=5.0.0 from a 4.x.x version","text":"<p>IntelOwl v5 introduced some major changes regarding how the plugins and their related configuration are managed in the application. Before upgrading, some important things should be checked by the administrator:</p> <ul> <li>A lot of database migrations will need to be applied. Just be patient few minutes once you install the new major release. If you get 500 status code errors in the GUI, just wait few minutes and then refresh the page.</li> <li>We moved away from the old big <code>analyzer_config.json</code> which was storing all the base configuration of the Analyzers to a database model (we did the same for all the other plugins types too). This allows us to manage plugins creation/modification/deletion in a more reliable manner and via the Django Admin Interface. If you have created custom plugins and changed those <code>&lt;plugins&gt;_config.json</code> file manually, you would need to re-create those custom plugins again from the Django Admin Interface. To do that please follow the related new documentation</li> <li>We have REMOVED all the analyzers that we deprecated during the v4 releases cycle. Please substitute them with their respective new names, in case they have a replacement.<ul> <li>REMOVED <code>Pulsedive_Active_IOC</code> analyzer. Please substitute it with the new <code>Pulsedive</code> analyzer.</li> <li>REMOVED <code>Fortiguard</code> analyzer because endpoint does not work anymore. No substitute.</li> <li>REMOVED <code>Rendertron</code> analyzer not working as intended. No substitute.</li> <li>REMOVED <code>ThreatMiner</code>, <code>SecurityTrails</code> and <code>Robtex</code> various analyzers and substituted with new versions.</li> <li>REMOVED <code>Doc_Info_Experimental</code>. Its functionality (XLM Macro parsing) is moved to <code>Doc_Info</code></li> <li>REMOVED <code>Strings_Info_Classic</code>. Please use <code>Strings_Info</code></li> <li>REMOVED <code>Strings_Info_ML</code>. Please use <code>Strings_Info</code> and set the parameter <code>rank_strings</code> to <code>True</code></li> <li>REMOVED all <code>Yara_Scan_&lt;repo&gt;</code> analyzers. They all went merged in the single <code>Yara</code> analyzer</li> <li>REMOVED <code>Darksearch_Query</code> analyzer because the service does not exist anymore. No substitute.</li> <li>REMOVED <code>UnpacMe_EXE_Unpacker</code>. Please use <code>UnpacMe</code></li> <li>REMOVED <code>BoxJS_Scan_JavaScript</code>. Please use <code>BoxJS</code></li> <li>REMOVED all <code>Anomali_Threatstream_&lt;option&gt;</code> analyzers. Now we have a single <code>Anomali_Threatstream</code> analyzer. Use the parameters to select the specific API you need.</li> </ul> </li> </ul>"},{"location":"IntelOwl/installation/#updating-to-500-from-a-3xx-version","title":"Updating to &gt;=5.0.0 from a 3.x.x version","text":"<p>This is not supported. Please perform a major upgrade once at a time.</p>"},{"location":"IntelOwl/installation/#updating-to-400-from-a-3xx-version","title":"Updating to &gt;=4.0.0 from a 3.x.x version","text":"<p>IntelOwl v4 introduced some major changes regarding the permission management, allowing an easier way to manage users and visibility. But that did break the previous available DB. So, to migrate to the new major version you would need to delete your DB. To do that, you would need to delete your volumes and start the application from scratch.</p> <pre><code>python3 start.py prod down -v\n</code></pre> <p>Please be aware that, while this can be an important effort to manage, the v4 IntelOwl provides an easier way to add, invite and manage users from the application itself. See the Organization section.</p>"},{"location":"IntelOwl/installation/#updating-to-200-from-a-1xx-version","title":"Updating to &gt;=2.0.0 from a 1.x.x version","text":"<p>Users upgrading from previous versions need to manually move <code>env_file_app</code>, <code>env_file_postgres</code> and <code>env_file_integrations</code> files under the new <code>docker</code> directory.</p>"},{"location":"IntelOwl/installation/#updating-to-v13x-from-any-prior-version","title":"Updating to &gt;v1.3.x from any prior version","text":"<p>If you are updating to &gt;v1.3.0 from any prior version, you need to execute a helper script so that the old data present in the database doesn't break.</p> <ol> <li> <p>Follow the above updation steps, once the docker containers are up and running execute the following in a new terminal</p> <pre><code>docker exec -ti intelowl_uwsgi bash\n</code></pre> <p>to get a shell session inside the IntelOwl's container.</p> </li> <li> <p>Now just copy and paste the below command into this new session,</p> <pre><code>curl https://gist.githubusercontent.com/Eshaan7/b111f887fa8b860c762aa38d99ec5482/raw/758517acf87f9b45bd22f06aee57251b1f3b1bbf/update_to_v1.3.0.py | python -\n</code></pre> </li> <li> <p>If you see \"Update successful!\", everything went fine and now you can enjoy the new features!</p> </li> </ol>"},{"location":"IntelOwl/installation/#releases-schedule","title":"Releases Schedule","text":"<p>From 2025 onwards, maintainers are adopting a new schedule for future releases containing new features: expect a new release on every April and October (like Ubuntu :P).</p> <p>In this way maintainers aim to provide constant support for the users and expected deadlines to get the new features from the project into the official releases.</p> <p>Please remember that you can always use the most recent features available in the development branch at anytime! See this section for additional details </p> <p>Obviously, as always, important bugs and fixes will be handled differently with dedicated patch releases.</p>"},{"location":"IntelOwl/introduction/","title":"Introduction","text":"<p> IntelOwl Repository</p>"},{"location":"IntelOwl/introduction/#introduction","title":"Introduction","text":"<p>IntelOwl was designed with the intent to help the community, in particular those researchers that can not afford commercial solutions, in the generation of threat intelligence data, in a simple, scalable and reliable way.</p> <p>Main features:</p> <ul> <li>Provides enrichment of Threat Intel for malware as well as observables (IP, Domain, URL, hash, etc).</li> <li>This application is built to scale out and to speed up the retrieval of threat info.</li> <li>Thanks to the official libraries pyintelowl and go-intelowl, it can be integrated easily in your stack of security tools to automate common jobs usually performed, for instance, by SOC analysts manually.</li> <li>Intel Owl is composed of:<ul> <li>analyzers that can be run to either retrieve data from external sources (like VirusTotal or AbuseIPDB) or to generate intel from internally available tools (like Yara or Oletools)</li> <li>connectors that can be run to export data to external platforms (like MISP or OpenCTI)</li> <li>visualizers that can be run to create custom visualizations of analyzers results</li> <li>playbooks that are meant to make analysis easily repeatable</li> </ul> </li> <li>API REST written in Django and Python 3.9.</li> <li>Built-in frontend client written in ReactJS, with certego-ui: provides features such as dashboard, visualizations of analysis data, easy to use forms for requesting new analysis, etc.</li> </ul>"},{"location":"IntelOwl/introduction/#publications-and-media","title":"Publications and media","text":"<p>To know more about the project and its growth over time, you may be interested in reading the following official blog posts and/or videos:</p> <ul> <li>HelpNetSecurity interview</li> <li>FIRSTCON 24 Fukuoka (Japan)</li> <li>The Honeynet Workshop: Denmark 2024</li> <li>Certego Blog: v6 Announcement (in Italian)</li> <li>HackinBo 2023 Presentation (in Italian)</li> <li>Certego Blog: v.5.0.0 Announcement</li> <li>Youtube demo: IntelOwl v4</li> <li>Certego Blog: v.4.0.0 Announcement</li> <li>LimaCharlie sponsorship</li> <li>Tines sponsorship</li> <li>APNIC blog</li> <li>Honeynet Blog: v3.0.0 Announcement</li> <li>Intel Owl on Daily Swig</li> <li>Honeynet Blog: v1.0.0 Announcement</li> <li>Certego Blog: First announcement</li> </ul> <p>Feel free to ask everything it comes to your mind about the project to the author: Matteo Lodi (Twitter).</p> <p>We also have a dedicated twitter account for the project: @intel_owl.</p>"},{"location":"IntelOwl/usage/","title":"Usage","text":"<p>This page includes the most important things to know and understand when using IntelOwl.</p>"},{"location":"IntelOwl/usage/#how-to-interact-with-intelowl","title":"How to interact with IntelOwl","text":"<p>Intel Owl main objective is to provide a single API interface to query in order to retrieve threat intelligence at scale.</p> <p>There are multiple ways to interact with the Intel Owl APIs,</p> <ol> <li> <p>Web Interface</p> <ul> <li>Built-in Web interface with dashboard, visualizations of analysis data, easy to use forms for requesting new analysis, tags management and more features</li> </ul> </li> <li> <p>pyIntelOwl (CLI/SDK)</p> <ul> <li>Official Python client that is available at: PyIntelOwl,</li> <li>Can be used as a library for your own python projects or...</li> <li>directly via the command line interface.</li> </ul> </li> <li> <p>goIntelOwl (CLI/SDK)</p> <ul> <li>Official GO client that is available at: go-intelowl</li> </ul> </li> </ol> <p>Hint: Tokens Creation</p> The server authentication is managed by API tokens. So, if you want to interact with Intel Owl, you have two ways to do that: <ul> <li>If you are a normal user, you can go to the \"API Access/Sessions\" section of the GUI and create a Token there.</li> <li>If you are an administrator of IntelOwl, you can create one or more unprivileged users from the Django Admin Interface and then generate a token for those users. </li> </ul> Afterwards you can leverage the created tokens with the Intel Owl Client."},{"location":"IntelOwl/usage/#plugins-framework","title":"Plugins Framework","text":"<p>Plugins are the core modular components of IntelOwl that can be easily added, changed and customized. There are several types of plugins:</p> <ul> <li>Analyzers</li> <li>Connectors</li> <li>Pivots</li> <li>Visualizers</li> <li>Ingestors</li> <li>Playbooks</li> </ul>"},{"location":"IntelOwl/usage/#analyzers","title":"Analyzers","text":"<p>Analyzers are the most important plugins in IntelOwl. They allow to perform data extraction on the observables and/or files that you would like to analyze.</p>"},{"location":"IntelOwl/usage/#analyzers-list","title":"Analyzers list","text":"<p>The following is the list of the available analyzers you can run out-of-the-box. You can also navigate the same list via the</p> <ul> <li>Graphical Interface: once your application is up and running, go to the \"Plugins\" section</li> <li>pyintelowl: <code>$ pyintelowl get-analyzer-config</code></li> </ul>"},{"location":"IntelOwl/usage/#file-analyzers","title":"File analyzers:","text":""},{"location":"IntelOwl/usage/#internal-tools","title":"Internal tools","text":"<ul> <li><code>APKiD</code>: APKiD identifies many compilers, packers, obfuscators, and other weird stuff from an APK or DEX file.</li> <li><code>Androguard</code> : Androguard is a python tool which can be leveraged to get useful information from the APK, for example, permissions, activities, services, 3<sup>rd</sup> party permissions, etc.</li> <li><code>BoxJS_Scan_Javascript</code>: Box-JS is a tool for studying JavaScript malware.</li> <li><code>Capa_Info</code>: Capa detects capabilities in executable files</li> <li><code>Capa_Info_Shellcode</code>: Capa detects capabilities in shellcode</li> <li><code>ClamAV</code>: scan a file via the ClamAV AntiVirus Engine. IntelOwl automatically keep ClamAV updated with official and unofficial open source signatures</li> <li><code>Doc_Info</code>: static document analysis with new features to analyze XLM macros, encrypted macros and more (combination of Oletools and XLMMacroDeobfuscator)</li> <li><code>ELF_Info</code>: static ELF analysis with pyelftools and telfhash</li> <li><code>File_Info</code>: static generic File analysis (hashes, magic and exiftool)</li> <li><code>Floss</code>: Mandiant Floss Obfuscated String Solver in files</li> <li><code>Hfinger</code>: create fingerprints of malware HTTPS requests using Hfinger</li> <li><code>PE_Info</code>: static PE analysis with pefile</li> <li><code>PEframe_Scan</code>: Perform static analysis on Portable Executable malware and malicious MS Office documents with PeFrame</li> <li><code>Permhash</code>: create hash of manifest permssions found in APK, Android manifest, Chrome extensions or Chrome extension manifest using Permhash</li> <li><code>PDF_Info</code>: static PDF analysis (peepdf + pdfid)</li> <li><code>Qiling_Linux</code>: Qiling qiling linux binary emulation.</li> <li><code>Qiling_Linux_Shellcode</code>: Qiling qiling linux shellcode emulation.</li> <li><code>Qiling_Windows</code>: Qiling qiling windows binary emulation.</li> <li><code>Qiling_Windows_Shellcode</code>: Qiling qiling windows shellcode emulation.</li> <li><code>Quark_Engine</code>: Quark Engine is an Obfuscation-Neglect Android Malware Scoring System.</li> <li><code>Rtf_Info</code>: static RTF analysis (Oletools)</li> <li><code>Signature_Info</code>: PE signature extractor with osslsigncode</li> <li><code>Speakeasy</code>: Mandiant Speakeasy binary emulation</li> <li><code>SpeakEasy_Shellcode</code>: Mandiant Speakeasy shellcode emulation</li> <li><code>Strings_Info</code>: Strings extraction. Leverages Mandiant's Stringsifter</li> <li><code>Suricata</code>: Analyze PCAPs with open IDS signatures with Suricata engine</li> <li><code>Thug_HTML_Info</code>: Perform hybrid dynamic/static analysis on a HTML file using Thug low-interaction honeyclient</li> <li><code>Xlm_Macro_Deobfuscator</code>: XlmMacroDeobfuscator deobfuscate xlm macros</li> <li><code>Yara</code>: scan a file with<ul> <li>ATM malware yara rules</li> <li>bartblaze yara rules</li> <li>community yara rules</li> <li>StrangerealIntel</li> <li>Neo23x0 yara rules</li> <li>Intezer yara rules</li> <li>Inquest yara rules</li> <li>McAfee yara rules</li> <li>Samir Threat Hunting yara rules</li> <li>Stratosphere yara rules</li> <li>Mandiant yara rules</li> <li>ReversingLabs yara rules</li> <li>YARAify rules</li> <li>SIFalcon rules</li> <li>Elastic rules</li> <li>JPCERTCC Yara rules</li> <li>HuntressLab Yara rules</li> <li>elceef Yara Rules</li> <li>dr4k0nia Yara rules</li> <li>Facebook Yara rules</li> <li>edelucia Yara rules</li> <li>LOLDrivers Yara Rules</li> <li>your own added signatures. See Advanced-Usage for more details.</li> </ul> </li> <li><code>Zippy_scan</code> : Zippy: Fast method to classify text as AI or human-generated; takes in <code>lzma</code>,<code>zlib</code>,<code>brotli</code> as input based engines; <code>ensemble</code> being default.</li> <li><code>Blint</code>: Blint is a Binary Linter that checks the security properties and capabilities of your executables. Supported binary formats: - Android (apk, aab) - ELF (GNU, musl) - PE (exe, dll) - Mach-O (x64, arm64)</li> <li><code>Mobsf</code>: MobSF is a static analysis tool that can find insecure code patterns in your Android and iOS source code. Supports Java, Kotlin, Android XML, Swift and Objective C Code.</li> <li><code>DroidLysis</code>: DroidLysis is a pre-analysis tool for Android apps: it performs repetitive and boring tasks we'd typically do at the beginning of any reverse engineering. It disassembles the Android sample, organizes output in directories, and searches for suspicious spots in the code to look at. The output helps the reverse engineer speed up the first few steps of analysis.</li> <li><code>Artifacts</code>: Artifacts is a tool that does APK strings analysis. Useful for first analysis.</li> </ul>"},{"location":"IntelOwl/usage/#external-services","title":"External services","text":"<ul> <li><code>CapeSandbox</code>: CAPESandbox automatically scans suspicious files using the CapeSandbox API. Analyzer works for private instances as well.</li> <li><code>Cymru_Hash_Registry_Get_File</code>: Check if a particular file is known to be malware by Team Cymru</li> <li><code>Cuckoo_Scan</code>: scan a file on Cuckoo (this analyzer is disabled by default. You have to change that flag in the config to use it)</li> <li><code>DocGuard_Upload_File</code>: Analyze office files in seconds. DocGuard.</li> <li><code>Dragonfly_Emulation</code>: Emulate malware against Dragonfly sandbox by Certego S.R.L.</li> <li><code>FileScan_Upload_File</code>: Upload your file to extract IoCs from executable files, documents and scripts via FileScan.io API.</li> <li><code>HashLookupServer_Get_File</code>: check if a md5 or sha1 is available in the database of known file hosted by CIRCL</li> <li><code>HybridAnalysis_Get_File</code>: check file hash on HybridAnalysis sandbox reports</li> <li><code>Intezer_Scan</code>: scan a file on Intezer. Register for a free community account here. With TLP <code>CLEAR</code>, in case the hash is not found, you would send the file to the service.</li> <li><code>Malpedia_Scan</code>: scan a binary or a zip file (pwd:infected) against all the yara rules available in Malpedia</li> <li><code>MalwareBazaar_Get_File</code>: Check if a particular malware sample is known to MalwareBazaar</li> <li><code>MISPFIRST_Check_Hash</code>: check a file hash on the FIRST MISP instance</li> <li><code>MISP_Check_Hash</code>: check a file hash on a MISP instance</li> <li><code>MWDB_Scan</code>: mwdblib Retrieve malware file analysis from repository maintained by CERT Polska MWDB. With TLP <code>CLEAR</code>, in case the hash is not found, you would send the file to the service.</li> <li><code>OTX_Check_Hash</code>: check file hash on Alienvault OTX</li> <li><code>SublimeSecurity</code>: Analyze an Email with Sublime Security live flow</li> <li><code>Triage_Scan</code>: leverage Triage sandbox environment to scan various files</li> <li><code>UnpacMe</code>: UnpacMe is an automated malware unpacking service</li> <li><code>Virushee_Scan</code>: Check file hash on Virushee API. With TLP <code>CLEAR</code>, in case the hash is not found, you would send the file to the service.</li> <li><code>VirusTotal_v3_File</code>: check the file hash on VirusTotal. With TLP <code>CLEAR</code>, in case the hash is not found, you would send the file to the service.</li> <li><code>YARAify_File_Scan</code>: scan a file against public and non-public YARA and ClamAV signatures in YARAify public service</li> <li><code>YARAify_File_Search</code>: scan an hash against YARAify database</li> </ul>"},{"location":"IntelOwl/usage/#observable-analyzers-ip-domain-url-hash","title":"Observable analyzers (ip, domain, url, hash)","text":""},{"location":"IntelOwl/usage/#internal-tools_1","title":"Internal tools","text":"<ul> <li><code>CheckDMARC</code>: An SPF and DMARC DNS records validator for domains.</li> <li><code>DNStwist</code>: Scan a url/domain to find potentially malicious permutations via dns fuzzing. dnstwist repo</li> <li><code>Thug_URL_Info</code>: Perform hybrid dynamic/static analysis on a URL using Thug low-interaction honeyclient</li> <li><code>AILTypoSquatting</code>:AILTypoSquatting is a Python library to generate list of potential typo squatting domains with domain name permutation engine to feed AIL and other systems.</li> </ul>"},{"location":"IntelOwl/usage/#external-services_1","title":"External services","text":"<ul> <li><code>AbuseIPDB</code>: check if an ip was reported on AbuseIPDB</li> <li><code>Abusix</code>: get abuse contacts of an IP address from Abusix</li> <li><code>BGP Ranking</code>: BGP-Ranking provides a way to collect such malicious activities, aggregate the information per ASN and provide a ranking model to rank the ASN from the most malicious to the less malicious ASN.</li> <li><code>Anomali_Threatstream_PassiveDNS</code>: Return information from passive dns of Anomali. On Anomali Threatstream PassiveDNS Api.</li> <li><code>Auth0</code>: scan an IP against the Auth0 API</li> <li><code>BinaryEdge</code>: Details about an Host. List of recent events for the specified host, including details of exposed ports and services using IP query and return list of subdomains known from the target domains using domain query</li> <li><code>BitcoinAbuse</code> : Check a BTC address against bitcoinabuse.com, a public database of BTC addresses used by hackers and criminals.</li> <li><code>Censys_Search</code>: scan an IP address against Censys View API</li> <li><code>CheckPhish</code>: CheckPhish can detect phishing and fraudulent sites.</li> <li><code>CIRCLPassiveDNS</code>: scan an observable against the CIRCL Passive DNS DB</li> <li><code>CIRCLPassiveSSL</code>: scan an observable against the CIRCL Passive SSL DB</li> <li><code>Classic_DNS</code>: Retrieve current domain resolution with default DNS</li> <li><code>CloudFlare_DNS</code>: Retrieve current domain resolution with CloudFlare DoH (DNS over HTTPS)</li> <li><code>CloudFlare_Malicious_Detector</code>: Leverages CloudFlare DoH to check if a domain is related to malware</li> <li><code>Crowdsec</code>: check if an IP was reported on Crowdsec Smoke Dataset</li> <li><code>Cymru_Hash_Registry_Get_Observable</code>: Check if a particular hash is available in the malware hash registry of Team Cymru</li> <li><code>DNSDB</code>: scan an observable against the Passive DNS Farsight Database (support both v1 and v2 versions)</li> <li><code>DNS0_EU</code>: Retrieve current domain resolution with DNS0.eu DoH (DNS over HTTPS)</li> <li><code>DNS0_EU_Malicious_Detector</code>: Check if a domain or an url is marked as malicious in DNS0.eu database (Zero service)</li> <li><code>DocGuard_Get</code>: check if an hash was analyzed on DocGuard. DocGuard</li> <li><code>DShield</code>: Service Provided by DShield to get useful information about IP addresses</li> <li><code>Feodo_Tracker</code>: Feodo Tracker offers various blocklists, helping network owners to protect their users from Dridex and Emotet/Heodo.</li> <li><code>FileScan_Search</code>: Finds reports and uploaded files by various tokens, like hash, filename, verdict, IOCs etc via FileScan.io API.</li> <li><code>FireHol_IPList</code>: check if an IP is in FireHol's IPList</li> <li><code>GoogleSafebrowsing</code>: Scan an observable against GoogleSafeBrowsing DB</li> <li><code>GoogleWebRisk</code>: Scan an observable against WebRisk API (Commercial version of Google Safe Browsing). Check the docs to enable this properly</li> <li><code>Google_DNS</code>: Retrieve current domain resolution with Google DoH (DNS over HTTPS)</li> <li><code>GreedyBear</code>: scan an IP or a domain against the GreedyBear API (requires API key)</li> <li><code>GreyNoise</code>: scan an IP against the Greynoise API (requires API key)</li> <li><code>GreyNoiseCommunity</code>: scan an IP against the Community Greynoise API (requires API key))</li> <li><code>Greynoise_Labs</code>: scan an IP against the Greynoise API (requires authentication token which can be obtained from cookies on Greynoise website after launching the playground from here)</li> <li><code>HashLookupServer_Get_Observable</code>: check if a md5 or sha1 is available in the database of known file hosted by CIRCL</li> <li><code>HoneyDB_Get</code>: HoneyDB IP lookup service</li> <li><code>HoneyDB_Scan_Twitter</code>: scan an IP against HoneyDB.io's Twitter Threat Feed</li> <li><code>Hunter_How</code>: Scans IP and domain against Hunter_How API.</li> <li><code>Hunter_Io</code>: Scans a domain name and returns set of data about the organisation, the email address found and additional information about the people owning those email addresses.</li> <li><code>HybridAnalysis_Get_Observable</code>: search an observable in the HybridAnalysis sandbox reports</li> <li><code>IP2WHOIS</code>: API Docs IP2Location.io IP2WHOIS Domain WHOIS API helps users to obtain domain information and WHOIS record by using a domain name.</li> <li><code>IPQS_Fraud_And_Risk_Scoring</code>: Scan an Observable against IPQualityscore</li> <li><code>InQuest_DFI</code>: Deep File Inspection by InQuest Labs</li> <li><code>InQuest_IOCdb</code>: Indicators of Compromise Database by InQuest Labs</li> <li><code>InQuest_REPdb</code>: Search in InQuest Lab's Reputation Database</li> <li><code>IPApi</code>: Get information about IPs using batch-endpoint and DNS using DNS-endpoint.</li> <li><code>IPInfo</code>: Location Information about an IP</li> <li><code>Ip2location</code>: API Docs IP2Location.io allows users to check IP address location in real time. (Supports both with or without key)</li> <li><code>Intezer_Get</code>: check if an analysis related to a hash is available in Intezer. Register for a free community account here.</li> <li><code>Koodous</code>: koodous API get information about android malware.</li> <li><code>MalwareBazaar_Get_Observable</code>: Check if a particular malware hash is known to MalwareBazaar</li> <li><code>MalwareBazaar_Google_Observable</code>: Check if a particular IP, domain or url is known to MalwareBazaar using google search</li> <li><code>MaxMindGeoIP</code>: extract GeoIP info for an observable</li> <li><code>MISP</code>: scan an observable on a MISP instance</li> <li><code>MISPFIRST</code>: scan an observable on the FIRST MISP instance</li> <li><code>Mmdb_server</code>: Mmdb_server mmdb-server is an open source fast API server to lookup IP addresses for their geographic location, AS number.</li> <li><code>Mnemonic_PassiveDNS</code> : Look up a domain or IP using the Mnemonic PassiveDNS public API.</li> <li><code>MWDB_Get</code>: mwdblib Retrieve malware file analysis by hash from repository maintained by CERT Polska MWDB.</li> <li><code>Netlas</code>: search an IP against Netlas</li> <li><code>NERD_analyzer</code>: search an IP against NERD reputation database NERD</li> <li><code>ONYPHE</code>: search an observable in ONYPHE</li> <li><code>OpenCTI</code>: scan an observable on an OpenCTI instance</li> <li><code>OTXQuery</code>: scan an observable on Alienvault OTX</li> <li><code>Phishstats</code>: Search PhishStats API to determine if an IP/URL/domain is malicious.</li> <li><code>Phishtank</code>: Search an url against Phishtank API</li> <li><code>PhishingArmy</code>: Search an observable in the PhishingArmy blocklist</li> <li><code>Pulsedive</code>: Scan indicators and retrieve results from Pulsedive's API.</li> <li><code>Quad9_DNS</code>: Retrieve current domain resolution with Quad9 DoH (DNS over HTTPS)</li> <li><code>Quad9_Malicious_Detector</code>: Leverages Quad9 DoH to check if a domain is related to malware</li> <li><code>Robtex</code>: scan a domain/IP against the Robtex Passive DNS DB</li> <li><code>Securitytrails</code>: scan an IP/Domain against Securitytrails API</li> <li><code>Shodan_Honeyscore</code>: scan an IP against Shodan Honeyscore API</li> <li><code>Shodan_Search</code>: scan an IP against Shodan Search API</li> <li><code>Spyse</code>: Scan domains, IPs, emails and CVEs using Spyse's API. Register here.</li> <li><code>SSAPINet</code>: get a screenshot of a web page using screenshotapi.net (external source); additional config options can be added to <code>extra_api_params</code> in the config.</li> <li><code>Stalkphish</code>: Search Stalkphish API to retrieve information about a potential phishing site (IP/URL/domain/Generic).</li> <li><code>Stratosphere_Blacklist</code>: Cross-reference an IP from blacklists maintained by Stratosphere Labs</li> <li><code>TalosReputation</code>: check an IP reputation from Talos</li> <li><code>ThreatFox</code>: search for an IOC in ThreatFox's database</li> <li><code>Threatminer</code>: retrieve data from Threatminer API</li> <li><code>TorNodesDanMeUk</code>: check if an IP is a Tor Node using a list of all Tor nodes provided by dan.me.uk</li> <li><code>TorProject</code>: check if an IP is a Tor Exit Node</li> <li><code>Triage_Search</code>: Search for reports of observables or upload from URL on triage cloud</li> <li><code>Tranco</code>: Check if a domain is in the latest Tranco ranking top sites list</li> <li><code>URLhaus</code>: Query a domain or URL against URLhaus API.</li> <li><code>UrlScan_Search</code>: Search an IP/domain/url/hash against URLScan API</li> <li><code>UrlScan_Submit_Result</code>: Submit &amp; retrieve result of an URL against URLScan API</li> <li><code>Virushee_CheckHash</code>: Search for a previous analysis of a file by its hash (SHA256/SHA1/MD5) on Virushee API.</li> <li><code>VirusTotal_v3_Get_Observable</code>: search an observable in the VirusTotal DB</li> <li><code>Whoisxmlapi</code>: Fetch WHOIS record data, of a domain name, an IP address, or an email address.</li> <li><code>WhoIs_RipeDB_Search</code> : Fetch whois record data of an IP address from Ripe DB using their search API (no API key required)</li> <li><code>XForceExchange</code>: scan an observable on IBM X-Force Exchange</li> <li><code>YARAify_Search</code>: lookup a file hash in Abuse.ch YARAify</li> <li><code>YETI</code> (Your Everyday Threat Intelligence): scan an observable on a YETI instance.</li> <li><code>Zoomeye</code>: Zoomeye Cyberspace Search Engine recording information of devices, websites, services and components etc..</li> <li><code>Validin</code>:Validin investigates historic and current data describing the structure and composition of the internet.</li> <li><code>TweetFeed</code>: TweetFeed collects Indicators of Compromise (IOCs) shared by the infosec community at Twitter.\\r\\nHere you will find malicious URLs, domains, IPs, and SHA256/MD5 hashes.</li> <li><code>HudsonRock</code>: Hudson Rock provides its clients the ability to query a database of over 27,541,128 computers which were compromised through global info-stealer campaigns performed by threat actors.</li> <li><code>CyCat</code>: CyCat or the CYbersecurity Resource CATalogue aims at mapping and documenting, in a single formalism and catalogue available cybersecurity tools, rules, playbooks, processes and controls.</li> <li><code>Vulners</code>: Vulners is the most complete and the only fully correlated security intelligence database, which goes through constant updates and links 200+ data sources in a unified machine-readable format. It contains 8 mln+ entries, including CVEs, advisories, exploits, and IoCs \u2014 everything you need to stay abreast on the latest security threats.</li> </ul>"},{"location":"IntelOwl/usage/#generic-analyzers-email-phone-number-etc-anything-really","title":"Generic analyzers (email, phone number, etc.; anything really)","text":"<p>Some analyzers require details other than just IP, URL, Domain, etc. We classified them as <code>generic</code> Analyzers. Since the type of field is not known, there is a format for strings to be followed.</p>"},{"location":"IntelOwl/usage/#internal-tools_2","title":"Internal tools","text":"<ul> <li><code>CyberChef</code>: Run a query on a CyberChef server using pre-defined or custom recipes.</li> </ul>"},{"location":"IntelOwl/usage/#external-services_2","title":"External services","text":"<ul> <li><code>Anomali_Threatstream_Confidence</code>: Give max, average and minimum confidence of maliciousness for an observable. On Anomali Threatstream Confidence API.</li> <li><code>Anomali_Threatstream_Intelligence</code>: Search for threat intelligence information about an observable. On Anomali Threatstream Intelligence API.</li> <li><code>CRXcavator</code>: scans a chrome extension against crxcavator.io</li> <li><code>Dehashed_Search</code>: Query any observable/keyword against https://dehashed.com's search API.</li> <li><code>EmailRep</code>: search an email address on emailrep.io</li> <li><code>HaveIBeenPwned</code>: HaveIBeenPwned checks if an email address has been involved in a data breach</li> <li><code>IntelX_Intelligent_Search</code>: IntelligenceX is a search engine and data archive. Fetches emails, urls, domains associated with an observable or a generic string.</li> <li><code>IntelX_Phonebook</code>: IntelligenceX is a search engine and data archive. Fetches emails, urls, domains associated with an observable or a generic string.</li> <li><code>IPQS_Fraud_And_Risk_Scoring</code>: Scan an Observable against IPQualityscore</li> <li><code>MISP</code>: scan an observable on a MISP instance</li> <li><code>VirusTotal_v3_Intelligence_Search</code>: Perform advanced queries with VirusTotal Intelligence (requires paid plan)</li> <li><code>WiGLE</code>: Maps and database of 802.11 wireless networks, with statistics, submitted by wardrivers, netstumblers, and net huggers.</li> <li><code>YARAify_Generics</code>: lookup a YARA rule (default), ClamAV rule, imphash, TLSH, telfhash or icon_dash in YARAify</li> <li><code>PhoneInfoga</code> : PhoneInfoga is one of the most advanced tools to scan international phone numbers.</li> <li><code>HudsonRock</code>: Hudson Rock provides its clients the ability to query a database of over 27,541,128 computers which were compromised through global info-stealer campaigns performed by threat actors.</li> <li><code>NIST_CVE_DB</code>: NIST_CVE_DB provides the details of supplied CVE Id.</li> </ul>"},{"location":"IntelOwl/usage/#optional-analyzers","title":"Optional analyzers","text":"<p>Some analyzers are optional and need to be enabled explicitly.</p>"},{"location":"IntelOwl/usage/#connectors","title":"Connectors","text":"<p>Connectors are designed to run after every successful analysis which makes them suitable for automated threat-sharing. They support integration with other SIEM/SOAR projects, specifically aimed at Threat Sharing Platforms.</p>"},{"location":"IntelOwl/usage/#connectors-list","title":"Connectors list","text":"<p>The following is the list of the available connectors. You can also navigate the same list via the</p> <ul> <li>Graphical Interface: once your application is up and running, go to the \"Plugins\" section</li> <li>pyintelowl: <code>$ pyintelowl get-connector-config</code></li> </ul>"},{"location":"IntelOwl/usage/#list-of-pre-built-connectors","title":"List of pre-built Connectors","text":"<ul> <li><code>MISP</code>: automatically creates an event on your MISP instance, linking the successful analysis on IntelOwl.</li> <li><code>OpenCTI</code>: automatically creates an observable and a linked report on your OpenCTI instance, linking the the successful analysis on IntelOwl.</li> <li><code>YETI</code>: YETI = Your Everyday Threat Intelligence. find or create observable on YETI, linking the successful analysis on IntelOwl.</li> <li><code>Slack</code>: Send the analysis link to a Slack channel (useful for external notifications)</li> <li><code>EmailSender</code>: Send a generic email.</li> <li><code>AbuseSubmitter</code>: Send an email to request to take down a malicious domain.</li> </ul>"},{"location":"IntelOwl/usage/#pivots","title":"Pivots","text":"<p>With IntelOwl v5.2.0 we introduced the <code>Pivot</code> Plugin.</p> <p>Pivots are designed to create a job from another job. This plugin allows the user to set certain conditions that trigger the execution of one or more subsequent jobs, strictly connected to the first one.</p> <p>This is a \"SOAR\" feature that allows the users to connect multiple analysis together.</p>"},{"location":"IntelOwl/usage/#list-of-pre-built-pivots","title":"List of pre-built Pivots","text":"<ul> <li><code>TakedownRequestToAbuseIp</code>: This Plugin leverages results from DNS resolver analyzers to extract a valid IP address to pivot to the Abusix analyzer.</li> <li><code>AbuseIpToSubmission</code>: This Plugin leverages results from the Abusix analyzer to extract the abuse contacts of an IP address to pivot to the AbuseSubmitter connector.</li> </ul> <p>You can build your own custom Pivot with your custom logic with just few lines of code. See the Contribute section for more info.</p>"},{"location":"IntelOwl/usage/#creating-pivots-from-the-gui","title":"Creating Pivots from the GUI","text":"<p>From the GUI, the users can pivot in two ways:</p> <ul> <li>If a Job executed a Visualizer, it is possible to select a field extracted and analyze its value by clicking the \"Pivot\" button (see following image). In this way, the user is able to \"jump\" from one indicator to another.   </li> </ul> <ul> <li>Starting from an already existing Investigation, it is possible to select a Job block and click the \"Pivot\" button to analyze the same observable again, usually choosing another Playbook (see following image)   </li> </ul> <p>In both cases, the user is redirected to the Scan Page that is precompiled with the observable selected. Then the user would be able to select the Playbook to execute in the new job. </p> <p>After the new Job is started, a new Investigation will be created (if it does not already exist) and both the jobs will be added to the same Investigation.</p> <p>In the following image you can find an example of an Investigation composed by 3 pivots generated manually:</p> <ul> <li>leveraging the first way to create a Pivot, the 2 Jobs that analyzed IP addresses have been generated from the first <code>test\\.com</code> Job</li> <li>leveraging the second way to create a Pivot, the second <code>test\\.com</code> analysis had been created with a different Playbook.</li> </ul> <p></p>"},{"location":"IntelOwl/usage/#visualizers","title":"Visualizers","text":"<p>With IntelOwl v5 we introduced a new plugin type called Visualizers. You can leverage it as a framework to create custom aggregated and simplified visualization of analyzer results.</p> <p>Visualizers are designed to run after the analyzers and the connectors. The visualizer adds logic after the computations, allowing to show the final result in a different way than merely the list of reports.</p> <p>Visualizers can be executed only during <code>Scans</code> through the playbook that has been configured on the visualizer itself.</p> <p>This framework is extremely powerful and allows every user to customize the GUI as they wish. But you know...with great power comes great responsability. To fully leverage this framework, you would need to put some effort in place. You would need to understand which data is useful for you and then write few code lines that would create your own GUI. To simplify the process, take example from the pre-built visualizers listed below and follow the dedicated documentation.</p>"},{"location":"IntelOwl/usage/#list-of-pre-built-visualizers","title":"List of pre-built Visualizers","text":"<ul> <li><code>DNS</code>: displays the aggregation of every DNS analyzer report</li> <li><code>Yara</code>: displays the aggregation of every matched rule by the <code>Yara</code> Analyzer</li> <li><code>Domain_Reputation</code>: Visualizer for the Playbook \"Popular_URL_Reputation_Services\"</li> <li><code>IP_Reputation</code>: Visualizer for the Playbook \"Popular_IP_Reputation_Services\"</li> <li><code>Pivot</code>: Visualizer that can be used in a Playbook to show the Pivot execution result. See Pivots for more info.</li> </ul>"},{"location":"IntelOwl/usage/#ingestors","title":"Ingestors","text":"<p>With IntelOwl v5.1.0 we introduced the <code>Ingestor</code> Plugin.</p> <p>Ingestors allow to automatically insert IOC streams from outside sources to IntelOwl itself. Each Ingestor must have a <code>Playbook</code> attached: this will allow to create a <code>Job</code> from every IOC retrieved.</p> <p>Ingestors are system-wide and disabled by default, meaning that only the administrator are able to configure them and enable them. Ingestors can be spammy so be careful about enabling them.</p> <p>A very powerful use is case is to combine Ingestors with Connectors to automatically extract data from external sources, analyze them with IntelOwl and push them externally to another platform (like MISP or a SIEM)</p>"},{"location":"IntelOwl/usage/#list-of-pre-built-ingestors","title":"List of pre-built Ingestors","text":"<ul> <li><code>ThreatFox</code>: Retrieves daily ioc from <code>https://threatfox.abuse.ch/</code> and analyze them.</li> <li><code>MalwareBazaar</code>: Retrieves hourly samples from <code>https://bazaar.abuse.ch/</code> and analyze them.</li> <li><code>VirusTotal</code>: Perform intelligence queries at hourly intervals from <code>https://www.virustotal.com/</code> (premium api key required), then retrieves the samples and analyzes them.</li> </ul>"},{"location":"IntelOwl/usage/#playbooks","title":"Playbooks","text":"<p>Playbooks are designed to be easy to share sequence of running Plugins (Analyzers, Connectors, ...) on a particular kind of observable.</p> <p>If you want to avoid to re-select/re-configure a particular combination of analyzers and connectors together every time, you should create a playbook out of it and use it instead. This is time saver.</p> <p>This is a feature introduced since IntelOwl v4.1.0! Please provide feedback about it!</p>"},{"location":"IntelOwl/usage/#playbooks-list","title":"Playbooks List","text":"<p>The following is the list of the available pre-built playbooks. You can also navigate the same list via the</p> <ul> <li>Graphical Interface: once your application is up and running, go to the \"Plugins\" section</li> <li>pyintelowl: <code>$ pyintelowl get-playbook-config</code></li> </ul>"},{"location":"IntelOwl/usage/#list-of-pre-built-playbooks","title":"List of pre-built playbooks","text":"<ul> <li><code>FREE_TO_USE_ANALYZERS</code>: A playbook containing all free to use analyzers.</li> <li><code>Sample_Static_Analysis</code>: A playbook containing all analyzers that perform static analysis on files.</li> <li><code>Popular_URL_Reputation_Services</code>: Collection of the most popular and free reputation analyzers for URLs and Domains</li> <li><code>Popular_IP_Reputation_Services</code>: Collection of the most popular and free reputation analyzers for IP addresses</li> <li><code>Dns</code>: A playbook containing all dns providers</li> <li><code>Takedown_Request</code>: Start investigation to request to take down a malicious domain. A mail will be sent to the domain's abuse contacts found</li> <li><code>Abuse_IP</code>: Playbook containing the Abusix analyzer. It is executed after the Takedown_Request playbook</li> <li><code>Send_Abuse_Email</code>: Playbook containing the AbuseSubmitter connector to send an email to request to take down a malicious domain. It is executed after the Abuse_IP playbook</li> </ul>"},{"location":"IntelOwl/usage/#playbooks-creation-and-customization","title":"Playbooks creation and customization","text":"<p>You can create new playbooks in different ways, based on the users you want to share them with:</p> <p>If you want to share them to every user in IntelOwl, create them via the Django Admin interface at <code>/admin/playbooks_manager/playbookconfig/</code>.</p> <p>If you want share them to yourself or your organization only, you need to leverage the \"Save as Playbook\" button that you can find on the top right of the Job Result Page. In this way, after you have done an analysis, you can save the configuration of the Plugins you executed for re-use with a single click.</p> <p></p> <p>The created Playbook would be available to yourself only. If you want either to share it with your organization or to delete it, you need to go to the \"Plugins\" section and enable it manually by clicking the dedicated button.</p> <p></p>"},{"location":"IntelOwl/usage/#generic-plugin-creation-configuration-and-customization","title":"Generic Plugin Creation, Configuration and Customization","text":"<p>If you want to create completely new Plugins (not based on already existing python modules), please refer to the Contribute section. This is usually the case when you want to integrate IntelOwl with either a new tool or a new service.</p> <p>On the contrary, if you would like to just customize the already existing plugins, this is the place.</p>"},{"location":"IntelOwl/usage/#superuser-customization","title":"SuperUser customization","text":"<p>If you are an IntelOwl superuser, you can create, modify, delete analyzers based on already existing modules by changing the configuration values inside the Django Admin interface at:</p> <ul> <li>for analyzers: <code>/admin/analyzers_manager/analyzerconfig/</code>.</li> <li>for connectors: <code>/admin/connectors_manager/connectorconfig/</code>.</li> <li>...and so on for all the Plugin types.</li> </ul> <p>The following are the most important fields that you can change without touching the source code:</p> <ul> <li><code>Name</code>: Name of the analyzer</li> <li><code>Description</code>: Description of the analyzer</li> <li><code>Disabled</code>: you can choose to disable certain analyzers, then they won't appear in the dropdown list and won't run if requested.</li> <li><code>Python Module</code>: Python path of the class that will be executed. This should not be changed most of the times.</li> <li><code>Maximum TLP</code>: see TLP Support</li> <li><code>Soft Time Limit</code>: this is the maximum time (in seconds) of execution for an analyzer. Once reached, the task will be killed (or managed in the code by a custom Exception). Default <code>300</code>.</li> <li><code>Routing Key</code>: this takes effects only when multi-queue is enabled. Choose which celery worker would execute the task: <code>local</code> (ideal for tasks that leverage local applications like Yara), <code>long</code> (ideal for long tasks) or <code>default</code> (ideal for simple webAPI-based analyzers).</li> </ul> <p>For analyzers only:</p> <ul> <li><code>Supported Filetypes</code>: can be populated as a list. If set, if you ask to analyze a file with a different mimetype from the ones you specified, it won't be executed</li> <li><code>Not Supported Filetypes</code>: can be populated as a list. If set, if you ask to analyze a file with a mimetype from the ones you specified, it won't be executed</li> <li><code>Observable Supported</code>: can be populated as a list. If set, if you ask to analyze an observable that is not in this list, it won't be executed. Valid values are: <code>ip</code>, <code>domain</code>, <code>url</code>, <code>hash</code>, <code>generic</code>.</li> </ul> <p>For connectors only:</p> <ul> <li><code>Run on Failure</code> (default: <code>true</code>): if they can be run even if the job has status <code>reported_with_fails</code></li> </ul> <p>For visualizers only:</p> <ul> <li><code>Playbooks</code>: list of playbooks that trigger the specified visualizer execution.</li> </ul> <p>Sometimes, it may happen that you would like to create a new analyzer very similar to an already existing one. Maybe you would like to just change the description and the default parameters. A helpful way to do that without having to copy/pasting the entire configuration, is to click on the analyzer that you want to copy, make the desired changes, and click the <code>save as new</code> button.</p> <p>Warning</p> Changing other keys can break a plugin. In that case, you should think about duplicating the configuration entry or python module with your changes.  <p>Other options can be added at the \"Python module\" level and not at the Plugin level. To do that, go to: <code>admin/api_app/pythonmodule/</code> and select the Python module used by the Plugin that you want to change. For example, the analyzer <code>AbuseIPDB</code> uses the Python module <code>abuseipdb.AbuseIPDB</code>.</p> <p></p> <p>Once there, you'll get this screen:</p> <p></p> <p>There you can change the following values:</p> <ul> <li><code>Update Schedule</code>: if the analyzer require some sort of update (local database, local rules, ...), you can specify the crontab schedule to update them.</li> <li><code>Health Check Schedule</code>: if the analyzer has implemented a Health Check, you can specify the crontab schedule to check whether the service works or not.</li> </ul>"},{"location":"IntelOwl/usage/#parameters","title":"Parameters","text":"<p>Each Plugin could have one or more parameters available to be configured. These parameters allow the users to customize the Plugin behavior.</p> <p>There are 2 types of Parameters:</p> <ul> <li>classic Parameters</li> <li><code>Secrets</code>: these parameters usually manage sensitive data, like API keys.</li> </ul> <p>To see the list of these parameters:</p> <ul> <li>You can view the \"Plugin\" Section in IntelOwl to have a complete and updated view of all the options available</li> <li>You can view the parameters by exploring the Django Admin Interface:<ul> <li><code>admin/api_app/parameter/</code></li> <li>or at the very end of each Plugin configuration like <code>/admin/analyzers_manager/analyzerconfig/</code></li> </ul> </li> </ul> <p>You can change the Plugin Parameters at 5 different levels:</p> <ul> <li>if you are an IntelOwl superuser, you can go in the Django Admin Interface and change the default values of the parameters for every plugin you like. This option would change the default behavior for every user in the platform.</li> <li>if you are either Owner or Admin of an org, you can customize the default values of the parameters for every member of the organization by leveraging the GUI in the \"Organization Config\" section. This overrides the previous option.</li> <li>if you are a normal user, you can customize the default values of the parameters for your analysis only by leveraging the GUI in the \"Plugin config\" section. This overrides the previous option.</li> <li>You can choose to provide runtime configuration when requesting an analysis that will override the previous options. This override is done only for the specific analysis. See Customize analyzer execution at time of request</li> </ul> <p>Playbook Exception</p> Please remember that, if you are executing a Playbook, the \"Runtime configuration\" of the Playbook take precedence over the Plugin Configuration.  <p>Plugin Configuration Order</p> Due to the multiple chances that are given to customize the parameters of the Plugins that are executed, it may be easy to confuse the order and launch Plugins without the awereness of what you are doing.  This is the order to define which values are used for the parameters, starting by the most important element:  - Runtime Configuration at Time of Request. - Runtime Configuration of the Playbook (if a Playbook is used and the Runtime Configuration at Time of Request is empty) - Plugin Configuration of the User - Plugin Configuration of the Organization - Default Plugin Configuration of the Parameter  If you are using the GUI, please remember that you can always check the Parameters before starting a \"Scan\" by clicking at the \"Runtime configuration\" ![img.png](./static/runtime_config.png) button.  Example: ![img.png](./static/runtime_config_2.png)"},{"location":"IntelOwl/usage/#enabling-or-disabling-plugins","title":"Enabling or Disabling Plugins","text":"<p>By default, each available plugin is configured as either disabled or not. The majority of them are enabled by default, while others may be disabled to avoid potential problems in the application usability for first time users.</p> <p>Considering the impact that this change could have in the application, the GUI does not allow a normal user to enable/disable any plugin. On the contrary, users with specific privileges may change this configuration:</p> <ul> <li>Org Administrators may leverage the feature documented here to enable/disable plugins for their org. This can be helpful to control users' behavior.</li> <li>IntelOwl Superusers (full admin) can go to the Django Admin Interface and enable/disable them from there. This operation does overwrite the Org administrators configuration. To find the plugin to change, they'll need to first choose the section of its type (\"ANALYZERS_MANAGER\", \"CONNECTORS_MANAGER\", etc), then select the chosen plugin, change the flag on that option and save the plugin by pressing the right button.</li> </ul> <p></p> <p></p>"},{"location":"IntelOwl/usage/#special-plugins-operations","title":"Special Plugins operations","text":"<p>All plugins, i.e. analyzers and connectors, have <code>kill</code> and <code>retry</code> actions. In addition to that, all docker-based analyzers and connectors have a <code>healthcheck</code> action to check if their associated instances are up or not.</p> <ul> <li> <p>kill:</p> <p>Stop a plugin whose status is <code>running</code>/<code>pending</code>:</p> <ul> <li>GUI: Buttons on reports table on job result page.</li> <li>PyIntelOwl: <code>IntelOwl.kill_analyzer</code> and <code>IntelOwl.kill_connector</code> function.</li> <li>CLI: <code>$ pyintelowl jobs kill-analyzer &lt;job_id&gt; &lt;analyzer_name&gt;</code> and <code>$ pyintelowl jobs kill-connector &lt;job_id&gt; &lt;connector_name&gt;</code></li> <li>API: <code>PATCH /api/job/{job_id}/{plugin_type/{plugin_name}/kill</code> and <code>PATCH /api/job/{job_id}/connector/{connector_name}/kill</code></li> </ul> </li> </ul> <ul> <li> <p>retry:</p> <p>Retry a plugin whose status is <code>failed</code>/<code>killed</code>:</p> <ul> <li>GUI: Buttons on reports table on job result page.</li> <li>PyIntelOwl: <code>IntelOwl.retry_analyzer</code> and <code>IntelOwl.retry_connector</code> function,</li> <li>CLI: <code>$ pyintelowl jobs retry-analyzer &lt;job_id&gt; &lt;analyzer_name&gt;</code> and <code>$ pyintelowl jobs retry-connector &lt;job_id&gt; &lt;connector_name&gt;</code></li> <li>API: <code>PATCH /api/job/{job_id}/{plugin_type}/{plugin_name}/retry</code></li> </ul> </li> </ul> <ul> <li> <p>healthcheck:</p> <p>Check if a plugin is able to connect to its provider:</p> <ul> <li>GUI: Buttons on every plugin table.</li> <li>PyIntelOwl: <code>IntelOwl.analyzer_healthcheck</code> and <code>IntelOwl.connector_healthcheck</code> methods.</li> <li>CLI: <code>$ pyintelowl analyzer-healthcheck &lt;analyzer_name&gt;</code> and <code>$ pyintelowl connector-healthcheck &lt;connector_name&gt;</code></li> <li>API: <code>GET /api/{plugin_type}/{plugin_name}/healthcheck</code></li> </ul> </li> </ul> <ul> <li> <p>pull:</p> <p>Update a plugin with the newest rules/database:</p> <ul> <li>GUI: Buttons on every plugin table.</li> <li>API: <code>POST /api/{plugin_type}/{plugin_name}/pull</code></li> </ul> </li> </ul>"},{"location":"IntelOwl/usage/#tlp-support","title":"TLP Support","text":"<p>The Traffic Light Protocol (TLP) is a standard that was created to facilitate greater sharing of potentially sensitive information and more effective collaboration.</p> <p>IntelOwl is not a threat intel sharing platform, like the MISP platform. However, IntelOwl is able to share analysis results to external platforms (via Connectors) and to send possible privacy related information to external services (via Analyzers).</p> <p>This is why IntelOwl does support a customized version of the Traffic Light Protocol (TLP): to allow the user to have a better knowledge of how their data are being shared.</p> <p>Every Analyzer and Connector can be configured with a <code>maximum_tlp</code> value. Based on that value, IntelOwl understands if the specific plugin is allowed or not to run (e.g. if <code>maximum_tlp</code> is <code>GREEN</code>, it would run for analysis with TLPs <code>WHITE</code> and <code>GREEN</code> only)</p> <p>These is how every available TLP value behaves once selected for an analysis execution:</p> <ol> <li><code>CLEAR</code>: no restriction (<code>WHITE</code> was replaced by <code>CLEAR</code> in TLP v2.0, but <code>WHITE</code> is supported for retrocompatibility)</li> <li><code>GREEN</code>: disable analyzers that could impact privacy</li> <li><code>AMBER</code> (default): disable analyzers that could impact privacy and limit view permissions to my group</li> <li><code>RED</code>: disable analyzers that could impact privacy, limit view permissions to my group and do not use any external service</li> </ol>"},{"location":"IntelOwl/usage/#running-a-plugin","title":"Running a plugin","text":"<p>A plugin can be run when all of the following requirements have been satisfied:</p> <ol> <li>All the required parameters of the plugin have been configured</li> <li>The plugin is not disabled</li> <li>The plugin is not disabled for the user's organization</li> <li>If the plugin has a health check schedule, the last check has to be successful</li> <li>The TLP selected to run the plugin cannot be higher than the maximum TLP configured for that plugin</li> <li>The observable classification or the file mimetype has to be supported by the plugin</li> </ol>"},{"location":"IntelOwl/usage/#investigations-framework","title":"Investigations Framework","text":"<p>Investigations are a new framework introduced in IntelOwl v6 with the goal to allow the users to connect the analysis they do with each other.</p> <p>In this way the analysts can use IntelOwl as the starting point of their \"Investigations\", register their findings, correlate the information found, and collaborate...all in a single place.</p> <p>Things to know about the framework:</p> <ul> <li>an Investigation is a superset of IntelOwl Jobs. It can have attached one or more existing IntelOwl Jobs</li> <li>an Investigation contains a \"description\" section that can be changed and updated at anytime with new information from the analysts.</li> <li>modification to the Investigation (description, jobs, etc) can be done by every member of the Organization where the creator of the Investigation belongs. However only they creator can delete an Investigation.</li> </ul>"},{"location":"IntelOwl/usage/#create-and-populate-an-investigation","title":"Create and populate an investigation","text":"<p>Investigations are created in 2 ways:</p> <ul> <li>automatically:<ul> <li>if you scan multiple observables at the same time, a new investigation will be created by default and all the observables they will be automatically connected to the same investigation.</li> <li>if you run a Job with a Playbook which contains a Pivot that triggers another Job, a new investigation will be created and both the Jobs will be added to the same investigation. See how you can create a new Pivot manually from the GUI.</li> </ul> </li> <li>manually: by clicking on the button in the \"History\" section you can create an Investigation from scratch without any job attached (see following image)</li> </ul> <p></p> <p>If you want to add a job to an Investigation, you should click to the root block of the Investigation (see following image):</p> <p></p> <p>Once a job has been added, you'll have something like this:</p> <p></p> <p>If you want to remove a Job, you can click on the Job block and click \"Remove branch\". On the contrary, if you just want to see Job Results, you can click in the \"Link\" button. (check next image)</p> <p></p>"},{"location":"IntelOwl/usage/#example-output-of-a-complex-investigation","title":"Example output of a complex investigation","text":""},{"location":"Submodules/GoIntelOwl/","title":"go-intelowl","text":"<p> go-intelowl is a client library/SDK that allows developers to easily automate and integrate IntelOwl with their own set of tools!</p>"},{"location":"Submodules/GoIntelOwl/#table-of-contents","title":"Table of Contents","text":"<ul> <li>go-intelowl</li> <li>Getting Started     - Pre requisites     - Installation     - Usage     - Examples</li> <li>Contribute</li> <li>License</li> <li>Links</li> <li>FAQ     - Generate API key         - v4.0 and above         - v4.0 below</li> </ul>"},{"location":"Submodules/GoIntelOwl/#getting-started","title":"Getting Started","text":""},{"location":"Submodules/GoIntelOwl/#pre-requisites","title":"Pre requisites","text":"<ul> <li>Go 1.17+</li> </ul>"},{"location":"Submodules/GoIntelOwl/#installation","title":"Installation","text":"<p>Use go get to retrieve the SDK to add it to your GOPATH workspace, or project's Go module dependencies.</p> <pre><code>$ go get github.com/intelowlproject/go-intelowl\n</code></pre>"},{"location":"Submodules/GoIntelOwl/#usage","title":"Usage","text":"<p>This library was built with ease of use in mind! Here are some quick examples to get you started. If you need more example you can go to the examples directory</p> <p>To start using the go-intelowl library you first need to import it: <pre><code>import \"github.com/intelowlproject/go-intelowl/gointelowl\"\n</code></pre> Construct a new <code>IntelOwlClient</code>, then use the various services to easily access different parts of Intelowl's REST API. Here's an example of getting all jobs:</p> <p><pre><code>clientOptions := gointelowl.IntelOwlClientOptions{\n    Url:         \"your-cool-URL-goes-here\",\n    Token:       \"your-super-secret-token-goes-here\",\n    // This is optional\n    Certificate: \"your-optional-certificate-goes-here\",\n}\n\nintelowl := gointelowl.NewIntelOwlClient(\n    &amp;clientOptions,\n    nil\n)\n\nctx := context.Background()\n\n// returns *[]Jobs or an IntelOwlError!\njobs, err := intelowl.JobService.List(ctx)\n</code></pre> For easy configuration and set up we opted for <code>options</code> structs. Where we can customize the client API or service endpoint to our liking! For more information go here. Here's a quick example!</p> <pre><code>// ...Making the client and context!\n\ntagOptions = gointelowl.TagParams{\n  Label: \"NEW TAG\",\n  Color: \"#ffb703\",\n}\n\ncreatedTag, err := intelowl.TagService.Create(ctx, tagOptions)\nif err != nil {\n    fmt.Println(err)\n} else {\n    fmt.Println(createdTag)\n}\n</code></pre>"},{"location":"Submodules/GoIntelOwl/#examples","title":"Examples","text":"<p>The examples directory contains a couple for clear examples, of which one is partially listed here as well:</p> <p><pre><code>package main\n\nimport (\n    \"fmt\"\n\n    \"github.com/intelowlproject/go-intelowl/gointelowl\"\n)\n\nfunc main(){\n    intelowlOptions := gointelowl.IntelOwlClientOptions{\n        Url:         \"your-cool-url-goes-here\",\n        Token:       \"your-super-secret-token-goes-here\",\n        Certificate: \"your-optional-certificate-goes-here\",\n    }   \n\n    client := gointelowl.NewIntelOwlClient(\n        &amp;intelowlOptions,\n        nil,\n    )\n\n    ctx := context.Background()\n\n    // Get User details!\n    user, err := client.UserService.Access(ctx)\n    if err != nil {\n        fmt.Println(\"err\")\n        fmt.Println(err)\n    } else {\n        fmt.Println(\"USER Details\")\n        fmt.Println(*user)\n    }\n}\n</code></pre> For complete usage of go-intelowl, see the full package docs.</p>"},{"location":"Submodules/GoIntelOwl/#contribute","title":"Contribute","text":"<p>If you want to follow the updates, discuss, contribute, or just chat then please join our slack channel we'd love to hear your feedback!</p>"},{"location":"Submodules/GoIntelOwl/#license","title":"License","text":"<p>Licensed under the GNU AFFERO GENERAL PUBLIC LICENSE.</p>"},{"location":"Submodules/GoIntelOwl/#links","title":"Links","text":"<ul> <li>Intelowl</li> <li>Documentation</li> <li>API documentation</li> <li>Examples</li> </ul>"},{"location":"Submodules/GoIntelOwl/#faq","title":"FAQ","text":""},{"location":"Submodules/GoIntelOwl/#generate-api-key","title":"Generate API key","text":"<p>You need a valid API key to interact with the IntelOwl server.</p>"},{"location":"Submodules/GoIntelOwl/#v40-and-above","title":"v4.0 and above","text":"<p>You can get an API by doing the following: 1. Log / Signin into intelowl 2. At the upper right click on your profile from the drop down select <code>API Access/ Sessions</code> 3. Then generate an API key or see it!</p>"},{"location":"Submodules/GoIntelOwl/#v40-below","title":"v4.0 below","text":"<p>Keys should be created from the admin interface of IntelOwl: you have to go in the Durin section (click on <code>Auth tokens</code>) and generate a key there.</p>"},{"location":"Submodules/GoIntelOwl/examples/basicExample/example/","title":"Example","text":"<p>This example will show you how to do a basic scan!</p>"},{"location":"Submodules/GoIntelOwl/examples/client/client/","title":"Client","text":""},{"location":"Submodules/GoIntelOwl/examples/client/client/#client","title":"Client","text":"<p>A good client is a client that is easy to use, configurable and customizable to a user\u2019s liking. Hence, the client has 4 great features: 1. Configurable HTTP client 2. Customizable timeouts 3. Logger 4. Easy ways to create the <code>IntelOwlClient</code></p>"},{"location":"Submodules/GoIntelOwl/examples/client/client/#configurable-http-client","title":"Configurable HTTP client","text":"<p>Now from the documentation, you can see you can pass your <code>http.Client</code>. This is to facilitate each user\u2019s requirement and taste! If you don\u2019t pass one (<code>nil</code>) a default <code>http.Client</code> will be made for you!</p>"},{"location":"Submodules/GoIntelOwl/examples/client/client/#customizable-timeouts","title":"Customizable timeouts","text":"<p>From <code>IntelOwlClientOptions</code> you can add your own timeout to your requests as well.</p>"},{"location":"Submodules/GoIntelOwl/examples/client/client/#logger","title":"Logger","text":"<p>To ease  developers' work go-intelowl provides a logger for easy debugging and tracking! For the logger we used logrus because of 2 reasons: 1. Easy to use 2. Extensible to your liking</p>"},{"location":"Submodules/GoIntelOwl/examples/client/client/#easy-ways-to-create-the-intelowlclient","title":"Easy ways to create the <code>IntelOwlClient</code>","text":"<p>As you know working with Golang structs is sometimes cumbersome we thought we could provide a simple way to create the client in a way that helps speed up development. This gave birth to the idea of using a <code>JSON</code> file to create the IntelOwlClient. The method <code>NewIntelOwlClientThroughJsonFile</code> does exactly that. Send the <code>IntelOwlClientOptions</code> JSON file path with your http.Client and LoggerParams in this method and you'll get the IntelOwlClient!</p>"},{"location":"Submodules/GoIntelOwl/examples/optionalParams/optionalParams/","title":"Optional Parameters","text":"<p>For the sake of simplicity, we decided that for some endpoints we\u2019ll be passing <code>Option Parameters</code> this is to facilitate easy access, configuration and automation so that you don\u2019t need to pass in many parameters but just a simple struct that can be easily converted to and from JSON!</p> <p>For example, let us look at the <code>TagParams</code> we use it as an argument for a method <code>Create</code> for <code>TagService</code>. From a glance, the <code>TagParams</code> look simple. They hold 2 fields: <code>Label</code>, and <code>Color</code> which can be passed seperatly to the method but imagine if you have many fields! (if you don\u2019t believe see the <code>ObservableAnalysisParams</code>)</p> <p>For a practical implementation you can see the example</p>"},{"location":"Submodules/GoIntelOwl/tests/CONTRIBUTING/","title":"CONTRIBUTING","text":""},{"location":"Submodules/GoIntelOwl/tests/CONTRIBUTING/#how-unit-tests-were-written","title":"How unit tests were written","text":"<p>The unit tests were written as a combination of table driven tests and the approach used by go-github</p> <p>Firstly we use a <code>TestData</code> struct that has the following fields: 1. <code>Input</code> - this is an <code>interface</code> as it is to be used as the input required for an endpoint 2. <code>Data</code> - this is a <code>string</code> as it'll be the <code>JSON</code> string that the endpoint is expected to return 2. <code>StatusCode</code> - this is an <code>int</code> as it is meant to be used as the expected response returned by the endpoint 3. <code>Want</code> - the expected struct that the method will return</p> <p>Now the reason we made this was that these fields were needed for every endpoint hence combining them into a single struct provided us reusability and flexibility.</p> <p>Now the testing suite used go's <code>httptest</code> library where we use <code>httptest.Server</code> as this setups a test server so that we can easily mock it. We also use <code>http.ServerMux</code> to mock our endpoints response.</p>"},{"location":"Submodules/GoIntelOwl/tests/CONTRIBUTING/#how-to-add-a-new-test-for-an-endpoint","title":"How to add a new test for an endpoint","text":"<p>Lets say IntelOwl added a new endpoint called supercool in <code>Tag</code>. Now you've implemented the endpoint as a method of <code>TagService</code> and now you want to add its unit tests.</p> <p>First go to <code>tagService_test.go</code> in the <code>tests</code> directory and add</p> <pre><code>func TestSuperCoolEndPoint(t *testing.T) {\n    testCases := make(map[string]TestData)\n    testCases[\"simple\"] = TestData{\n        Input:      nil,\n        Data:       `{ \"supercool\": \"you're a great developer :)\"}`,\n        StatusCode: http.StatusOK,\n        Want: \"you're a great developer :)\",\n    }\n    for name, testCase := range testCases {\n        // subtest\n        t.Run(name, func(t *testing.T) {\n            // setup will give you the client, mux/router, closeServer\n            client, apiHandler, closeServer := setup()\n            defer closeServer()\n            ctx := context.Background()\n            // now you can use apiHandler to mock how the server will handle this endpoints request\n            // you can use mux/router's Handle method or HandleFunc\n            apiHandler.Handle(\"/api/tag/supercool\", func(w http.ResponseWriter, r *http.Request) {\n                // this is a helper test to check if it is the expected request sent by the client\n                testMethod(t, r, \"GET\")\n                w.Write([]byte(testCase.Data))\n            })\n            expectedRespone, err := client.TagService.SuperCool(ctx)\n            if err != nil {\n                testError(t, testCase, err)\n            } else {\n                testWantData(t, testCase.Want, expectedRespone)\n            }\n        })\n    }\n}\n</code></pre> <p>Great! Now you've added your own unit tests.</p>"},{"location":"Submodules/GreedyBear/","title":"Index","text":""},{"location":"Submodules/GreedyBear/#greedybear","title":"GreedyBear","text":"<p>The project goal is to extract data of the attacks detected by a TPOT or a cluster of them and to generate some feeds that can be used to prevent and detect attacks.</p> <p>Official announcement here.</p>"},{"location":"Submodules/GreedyBear/#documentation","title":"Documentation","text":"<p>Documentation about GreedyBear installation, usage, configuration and contribution can be found at this link</p>"},{"location":"Submodules/GreedyBear/#public-feeds","title":"Public feeds","text":"<p>There are public feeds provided by The Honeynet Project in this site. Example</p> <p>Please do not perform too many requests to extract feeds or you will be banned.</p> <p>If you want to be updated regularly, please download the feeds only once every 10 minutes (this is the time between each internal update).</p> <p>To check all the available feeds, Please refer to our usage guide</p>"},{"location":"Submodules/GreedyBear/#enrichment-service","title":"Enrichment Service","text":"<p>GreedyBear provides an easy-to-query API to get the information available in GB regarding the queried observable (domain or IP address).</p> <p>To understand more, Please refer to our usage guide</p>"},{"location":"Submodules/GreedyBear/#run-greedybear-on-your-environment","title":"Run Greedybear on your environment","text":"<p>The tool has been created not only to provide the feeds from The Honeynet Project's cluster of TPOTs.</p> <p>If you manage one or more T-POTs of your own, you can get the code of this application and run Greedybear on your environment. In this way, you are able to provide new feeds of your own.</p> <p>To install it locally, Please refer to our installation guide</p>"},{"location":"Submodules/GreedyBear/#sponsors","title":"Sponsors","text":""},{"location":"Submodules/GreedyBear/#certego","title":"Certego","text":"<p>Certego is a MDR (Managed Detection and Response) and Threat Intelligence Provider based in Italy.</p> <p>Started as a personal Christmas project from Matteo Lodi, since then GreedyBear is being improved mainly thanks to the efforts of the Certego Threat Intelligence Team.</p>"},{"location":"Submodules/GreedyBear/#the-honeynet-project","title":"The Honeynet Project","text":"<p>The Honeynet Project is a non-profit organization working on creating open source cyber security tools and sharing knowledge about cyber threats.</p>"},{"location":"Submodules/GreedyBear/FEEDS_LICENSE/","title":"FEEDS LICENSE","text":"<p>The data provided from the site https://greedybear.honeynet.org are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.</p>"},{"location":"Submodules/GreedyBear/frontend/","title":"GreedyBear - frontend","text":"<p>Built with @certego/certego-ui.</p>"},{"location":"Submodules/GreedyBear/frontend/#design-thesis","title":"Design thesis","text":"<ul> <li>Re-usable components/hooks/stores that other projects can also benefit from should be added to certego-ui package.</li> <li>GreedyBear specific:<ul> <li>components should be added to <code>src/components</code>.</li> <li>general hooks should be added to <code>src/hooks</code>.</li> <li>zustand stores hooks should be added to <code>src/stores</code>.</li> </ul> </li> </ul>"},{"location":"Submodules/GreedyBear/frontend/#directory-structure","title":"Directory Structure","text":"<pre><code>public/                                   public static assets\n|- icons/                                 icons/favicon\n|- index.html/                            root HTML file\nsrc/                                      source code\n|- components/                            pages and components\n|  |- auth/                               `certego_saas.apps.auth` (login, logout pages)\n|  |- dashboard/                          dashboard page and charts\n|  |- home/                               landing/home page\n|  |- Routes.jsx                          lazy route-component mappings\n|- constants/                             constant values\n|  |- api.js                              API URLs\n|  |- environment.js                      environment variables\n|  |- index.js                            GreedyBear specific constants\n|- hooks/                                 react hooks\n|- layouts/                               header, main, footer containers\n|- stores/                                zustand stores hooks\n|- styles/                                scss files\n|- wrappers/                              Higher-Order components\n|- App.jsx                                App component\n|- index.jsx                              Root JS file (ReactDOM renderer)\n</code></pre>"},{"location":"Submodules/GreedyBear/frontend/#local-development-environment","title":"Local Development Environment","text":"<p>The frontend inside the docker containers does not hot-reload, so you need to use <code>CRA dev server</code> on your host machine to serve pages when doing development on the frontend, using docker nginx only as API source.</p> <ul> <li>Start GreedyBear containers (see docs). Original dockerized app is accessible on <code>http://localhost:80</code></li> </ul> <ul> <li>If you have not <code>node-js</code> installed, you have to do that. Follow the guide here. We tested this with NodeJS &gt;=16.6</li> </ul> <ul> <li>Install npm packages locally</li> </ul> <pre><code>cd ./frontend &amp;&amp; npm install\n</code></pre> <ul> <li>Start CRA dev server:</li> </ul> <pre><code>npm start\n</code></pre> <ul> <li>Now you can access the auto-reloading frontend on <code>http://localhost:3001</code>. It acts as proxy for API requests to original app web server.</li> </ul> <ul> <li>JS app main configs are available in <code>package.json</code> and <code>enviroments.js</code>.</li> </ul>"},{"location":"Submodules/GreedyBear/frontend/#external-docs","title":"External Docs","text":"<ul> <li>Create React App documentation.</li> <li>React documentation.</li> </ul>"},{"location":"Submodules/IntelOwl/","title":"Index","text":""},{"location":"Submodules/IntelOwl/#intel-owl","title":"Intel Owl","text":"<p>Do you want to get threat intelligence data about a malware, an IP address or a domain? Do you want to get this kind of data from multiple sources at the same time using a single API request?</p> <p>You are in the right place!</p> <p>IntelOwl is an Open Source solution for management of Threat Intelligence at scale. It integrates a number of analyzers available online and a lot of cutting-edge malware analysis tools.</p>"},{"location":"Submodules/IntelOwl/#features","title":"Features","text":"<p>This application is built to scale out and to speed up the retrieval of threat info.</p> <p>It provides: - Enrichment of Threat Intel for files as well as observables (IP, Domain, URL, hash, etc). - A Fully-fledged REST APIs written in Django and Python. - An easy way to be integrated in your stack of security tools to automate common jobs usually performed, for instance, by SOC analysts manually. (Thanks to the official libraries pyintelowl and go-intelowl) - A built-in GUI: provides features such as dashboard, visualizations of analysis data, easy to use forms for requesting new analysis, etc. - A framework composed of modular components called Plugins:   - analyzers that can be run to either retrieve data from external sources (like VirusTotal or AbuseIPDB) or to generate intel from internally available tools (like Yara or Oletools)   - connectors that can be run to export data to external platforms (like MISP or OpenCTI)   - pivots that are designed to trigger the execution of a chain of analysis and connect them to each other   - visualizers that are designed to create custom visualizations of analyzers results   - ingestors that allows to automatically ingest stream of observables or files to IntelOwl itself   - playbooks that are meant to make analysis easily repeatable</p>"},{"location":"Submodules/IntelOwl/#documentation","title":"Documentation","text":"<p>We try hard to keep our documentation well written, easy to understand and always updated. All info about installation, usage, configuration and contribution can be found here</p>"},{"location":"Submodules/IntelOwl/#publications-and-media","title":"Publications and Media","text":"<p>To know more about the project and its growth over time, you may be interested in reading the official blog posts and/or videos about the project by clicking on this link</p>"},{"location":"Submodules/IntelOwl/#available-services-or-analyzers","title":"Available services or analyzers","text":"<p>You can see the full list of all available analyzers in the documentation.</p> Type Analyzers Available Inbuilt modules - Static Office Document, RTF, PDF, PE File Analysis and metadata extraction - Strings Deobfuscation and analysis (FLOSS, Stringsifter, ...) - PE Emulation with Qiling and Speakeasy - PE Signature verification - PE Capabilities Extraction (CAPA) - Javascript Emulation (Box-js) - Android Malware Analysis (Quark-Engine, ...) - SPF and DMARC Validator - Yara (a lot of public rules are available. You can also add your own rules) - more... External services - Abuse.ch MalwareBazaar/URLhaus/Threatfox/YARAify -  GreyNoise v2 -  Intezer - VirusTotal v3 -  Crowdsec - URLscan - Shodan - AlienVault OTX - Intelligence_X - MISP - many more.."},{"location":"Submodules/IntelOwl/#partnerships-and-sponsors","title":"Partnerships and sponsors","text":"<p>As open source project maintainers, we strongly rely on external support to get the resources and time to work on keeping the project alive, with a constant release of new features, bug fixes and general improvements.</p> <p>Because of this, we joined Open Collective to obtain non-profit equal level status which allows the organization to receive and manage donations transparently. Please support IntelOwl and all the community by choosing a plan (BRONZE, SILVER, etc).</p> <p> </p>"},{"location":"Submodules/IntelOwl/#gold","title":"\ud83e\udd47 GOLD","text":""},{"location":"Submodules/IntelOwl/#certego","title":"Certego","text":"<p>Certego is a MDR (Managed Detection and Response) and Threat Intelligence Provider based in Italy.</p> <p>IntelOwl was born out of Certego's Threat intelligence R&amp;D division and is constantly maintained and updated thanks to them.</p>"},{"location":"Submodules/IntelOwl/#the-honeynet-project","title":"The Honeynet Project","text":"<p>The Honeynet Project is a non-profit organization working on creating open source cyber security tools and sharing knowledge about cyber threats.</p> <p>Thanks to Honeynet, we are hosting a public demo of the application here. If you are interested, please contact a member of Honeynet to get access to the public service.</p>"},{"location":"Submodules/IntelOwl/#google-summer-of-code","title":"Google Summer of Code","text":"<p>Since its birth this project has been participating in the Google Summer of Code (GSoC)!</p> <p>If you are interested in participating in the next Google Summer of Code, check all the info available in the dedicated repository!</p>"},{"location":"Submodules/IntelOwl/#silver","title":"\ud83e\udd48 SILVER","text":""},{"location":"Submodules/IntelOwl/#threathunterai","title":"ThreatHunter.ai","text":"<p>ThreatHunter.ai\u00ae, is a 100% Service-Disabled Veteran-Owned Small Business started in 2007 under the name Milton Security Group. ThreatHunter.ai is the global leader in Dynamic Threat Hunting. Operating a true 24x7x365 Security Operation Center with AI/ML-enhanced human Threat Hunters, ThreatHunter.ai has changed the industry in how threats are found, and mitigated in real time. For over 15 years, our teams of Threat Hunters have stopped hundreds of thousands of threats and assisted organizations in defending against threat actors around the clock.</p>"},{"location":"Submodules/IntelOwl/#bronze","title":"\ud83e\udd49 BRONZE","text":""},{"location":"Submodules/IntelOwl/#docker","title":"Docker","text":"<p>In 2021 IntelOwl joined the official Docker Open Source Program. This allows IntelOwl developers to easily manage Docker images and focus on writing the code. You may find the official IntelOwl Docker images here.</p>"},{"location":"Submodules/IntelOwl/#digitalocean","title":"DigitalOcean","text":"<p>In 2022 IntelOwl joined the official DigitalOcean Open Source Program.</p>"},{"location":"Submodules/IntelOwl/#about-the-author-and-maintainers","title":"About the author and maintainers","text":"<p>Feel free to contact the main developers at any time on Twitter:</p> <ul> <li>Matteo Lodi: Author and principal maintainer</li> <li>Simone Berni: Backend Maintainer</li> <li>Daniele Rosetti: Frontend Maintainer</li> <li>Eshaan Bansal: Key Contributor</li> </ul>"},{"location":"Submodules/IntelOwl/#consultancy","title":"Consultancy","text":"<p>IntelOwl's maintainers are available to offer paid consultancy and mentorship.</p>"},{"location":"Submodules/IntelOwl/docker/bin/","title":"Embedded binary list","text":""},{"location":"Submodules/IntelOwl/docker/bin/#osslsigncode","title":"Osslsigncode","text":"<p>We embedded the compiled version for Ubuntu for that can be retrieved from its original repo here.</p> <p>We decided to do not use the version shipped by default Ubuntu packages because it were too old (2.1)</p> <p>At the last time of writing we uploaded the version 2.6-dev</p>"},{"location":"Submodules/IntelOwl/frontend/","title":"IntelOwl - frontend","text":"<p>Built with @certego/certego-ui.</p>"},{"location":"Submodules/IntelOwl/frontend/#design-thesis","title":"Design thesis","text":"<ul> <li>Re-usable components/hooks/stores that other projects can also benefit from should be added to certego-ui package.</li> <li>IntelOwl specific:<ul> <li>components should be added to <code>src/components/common</code>.</li> <li>general hooks should be added to <code>src/hooks</code>.</li> <li>zustand stores hooks should be added to <code>src/stores</code>.</li> </ul> </li> </ul>"},{"location":"Submodules/IntelOwl/frontend/#directory-structure","title":"Directory Structure","text":"<pre><code>public/                                   public static assets\n|- icons/                                 icons/favicon\n|- index.html/                            root HTML file\nsrc/                                      source code\n|- components/                            pages and components\n|  |- auth/                               `authentication` (login, logout, OAuth pages)\n|  |- common/                             small re-usable components\n|  |- dashboard/                          dashboard page and charts\n|  |- home/                               landing/home page\n|  |- jobs/                               `api_app`\n|  |  |- result/                          JobResult.jsx\n|  |  |- table/                           JobsTable.jsx\n|  |- me/\n|  |  |- organization/                    `certego_saas.apps.organization`\n|  |  |- sessions/                        durin (sessions management)\n|  |- misc/\n|  |  |- notification/                    `certego_saas.apps.notifications`\n|  |- plugins/                            `api_app.analyzers_manager`, `api_app.connectors_manager`\n|  |- scan/                               new scan/job\n|  |- Routes.jsx                          lazy route-component mappings\n|- constants/                             constant values\n|  |- api.js                              API URLs\n|  |- environment.js                      environment variables\n|  |- index.js                            intelowl specific constants\n|- hooks/                                 react hooks\n|- layouts/                               header, main, footer containers\n|- stores/                                zustand stores hooks\n|- styles/                                scss files\n|- utils/                                 utility functions\n|- wrappers/                              Higher-Order components\n|- App.jsx                                App component\n|- index.jsx                              Root JS file (ReactDOM renderer)\n</code></pre>"},{"location":"Submodules/IntelOwl/frontend/#local-development-environment","title":"Local Development Environment","text":"<p>The frontend inside the docker containers does not hot-reload, so you need to use <code>CRA dev server</code> on your host machine to serve pages when doing development on the frontend, using docker nginx only as API source.</p> <ul> <li>Start IntelOwl containers (see docs). Original dockerized app is accessible on <code>http://localhost:80</code></li> </ul> <ul> <li>If you have not <code>node-js</code> installed, you have to do that. Follow the guide here. We tested this with NodeJS &gt;=16.6</li> </ul> <ul> <li>Install npm packages locally</li> </ul> <pre><code>cd ./frontend &amp;&amp; npm install\n</code></pre> <ul> <li>Start CRA dev server:</li> </ul> <pre><code>npm start\n</code></pre> <ul> <li>Now you can access the auto-reloading frontend on <code>http://localhost:3000</code>. It acts as proxy for API requests to original app web server.</li> </ul> <ul> <li>JS app main configs are available in <code>package.json</code>.</li> </ul> <ul> <li>(optional) Use local build of <code>certego-ui</code> package so it can also hot-reload. This is useful when you want to make changes in certego-ui and rapidly test them with IntelOwl. Refer here for setup instructions.</li> </ul>"},{"location":"Submodules/IntelOwl/frontend/#miscellaneous","title":"Miscellaneous","text":""},{"location":"Submodules/IntelOwl/frontend/#dependabot","title":"Dependabot","text":"<p>We have dependabot enabled for the React.js frontend application. The updates are scheduled for once a week.</p>"},{"location":"Submodules/IntelOwl/frontend/#external-docs","title":"External Docs","text":"<ul> <li>Create React App documentation.</li> <li>React documentation.</li> </ul>"},{"location":"Submodules/IntelOwl/integrations/malware_tools_analyzers/qiling/profiles/","title":"Please add your profile files here","text":""},{"location":"Submodules/IntelOwl/integrations/malware_tools_analyzers/qiling/rootfs/","title":"Please add your rootfs folder here","text":""},{"location":"Submodules/pyintelowl/","title":"PyIntelOwl","text":"<p>Robust Python SDK and Command Line Client for interacting with IntelOwl's API.</p>"},{"location":"Submodules/pyintelowl/#features","title":"Features","text":"<ul> <li>Easy one-time configuration with self documented help and hints along the way.</li> <li>Request new analysis for observables and files.<ul> <li>Select which analyzers you want to run for every analysis you perform.</li> <li>Choose whether you want to HTTP poll for the analysis to finish or not.</li> </ul> </li> <li>List all jobs or view one job in a prettified tabular form.</li> <li>List all tags or view one tag in a prettified tabular form.</li> </ul>"},{"location":"Submodules/pyintelowl/#demo","title":"Demo","text":""},{"location":"Submodules/pyintelowl/#installation","title":"Installation","text":"<pre><code>$ pip3 install pyintelowl\n</code></pre> <p>For development/testing, <code>pip3 install pyintelowl[dev]</code></p>"},{"location":"Submodules/pyintelowl/#quickstart","title":"Quickstart","text":""},{"location":"Submodules/pyintelowl/#as-command-line-client","title":"As Command Line Client","text":"<p>On successful installation, The <code>pyintelowl</code> entryscript should be directly invokable. For example,</p> <pre><code>$ pyintelowl\nUsage: pyintelowl [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  -d, --debug  Set log level to DEBUG\n  --version    Show the version and exit.\n  -h, --help   Show this message and exit.\n\nCommands:\n  analyse                Send new analysis request\n  analyzer-healthcheck   Send healthcheck request for an analyzer...\n  config                 Set or view config variables\n  connector-healthcheck  Send healthcheck request for a connector\n  jobs                   Manage Jobs\n  tags                   Manage tags\n</code></pre>"},{"location":"Submodules/pyintelowl/#as-a-library-sdk","title":"As a library / SDK","text":"<pre><code>from pyintelowl import IntelOwl\nobj = IntelOwl(\"&lt;your_api_key&gt;\", \"&lt;your_intelowl_instance_url&gt;\", \"optional&lt;path_to_pem_file&gt;\", \"optional&lt;proxies&gt;\")\n</code></pre> <p>For more comprehensive documentation, please see https://intelowlproject.github.io/docs/pyintelowl/.</p>"},{"location":"Submodules/pyintelowl/#changelog","title":"Changelog","text":"<p>View CHANGELOG.md.</p>"},{"location":"Submodules/pyintelowl/#how-to-generate-an-api-key","title":"How to generate an API key","text":"<p>You need a valid API key to interact with the IntelOwl server. Keys can be created from the \"API access\" section of the user's menu in the IntelOwl's GUI.</p> <p></p> <p>Otherwise, you can create them from the Django Admin Interface of the IntelOwl application with an administration account. Section \"Durin\" -&gt; \"Auth tokens\"</p>"},{"location":"pyintelowl/","title":"Quickstart","text":"<p> PyIntelOwl Repository</p>"},{"location":"pyintelowl/#welcome-to-pyintelowls-documentation","title":"Welcome to PyIntelOwl's documentation!","text":""},{"location":"pyintelowl/#robust-python-sdk-and-command-line-client-for-interacting-with-intelowl-api","title":"Robust Python SDK and Command Line Client for interacting with IntelOwl API.","text":""},{"location":"pyintelowl/#installation","title":"Installation","text":"<pre><code>pip install pyintelowl\n</code></pre>"},{"location":"pyintelowl/#usage-as-cli","title":"Usage as CLI","text":"<pre><code> pyintelowl\n Usage: pyintelowl [OPTIONS] COMMAND [ARGS]...\n\n Options:\n -d, --debug  Set log level to DEBUG\n --version    Show the version and exit.\n -h, --help   Show this message and exit.\n\n Commands:\n analyse                Send new analysis request\n analyzer-healthcheck   Send healthcheck request for an analyzer...\n config                 Set or view config variables\n connector-healthcheck  Send healthcheck request for a connector\n get-analyzer-config    Get current state of `analyzer_config.json` from...\n get-connector-config   Get current state of `connector_config.json` from...\n get-playbook-config    Get current state of `playbook_config.json` from...\n jobs                   Manage Jobs\n tags                   Manage tags\n</code></pre>"},{"location":"pyintelowl/#configuration","title":"Configuration:","text":"<p>You can use <code>set</code> to set the config variables and <code>get</code> to view them.</p> <pre><code>pyintelowl config set -k 4bf03f20add626e7138f4023e4cf52b8 -u \"http://localhost:80\"\npyintelowl config get\n</code></pre>"},{"location":"pyintelowl/#hint","title":"Hint","text":"<p>The CLI would is well-documented which will help you navigate various commands easily. Invoke <code>pyintelowl -h</code> or <code>pyintelowl &lt;command&gt; -h</code> to get help.</p>"},{"location":"pyintelowl/#usage-as-sdklibrary","title":"Usage as SDK/library","text":"<pre><code> from pyintelowl import IntelOwl, IntelOwlClientException\n obj = IntelOwl(\n    \"4bf03f20add626e7138f4023e4cf52b8\",\n    \"http://localhost:80\",\n    None,\n )\n \"\"\"\n obj = IntelOwl(\n    \"&lt;your_api_key&gt;\",\n    \"&lt;your_intelowl_instance_url&gt;\",\n    \"optional&lt;path_to_pem_file&gt;\"\n    \"optional&lt;proxies&gt;\"\n )\n \"\"\"\n\n try:\n    ans = obj.get_analyzer_configs()\n    print(ans)\n except IntelOwlClientException as e:\n    print(\"Oh no! Error: \", e)\n</code></pre>"},{"location":"pyintelowl/#tip","title":"Tip","text":"<p>We very much recommend going through the :class:<code>pyintelowl.pyintelowl.IntelOwl</code> docs.</p>"},{"location":"pyintelowl/#index","title":"Index","text":"<pre><code>.. toctree::\n   :maxdepth: 2\n   :caption: Usage\n\n   pyintelowl\n</code></pre> <pre><code>  .. toctree::\n   :maxdepth: 2\n   :caption: Development\n\n   tests\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/","title":"IntelOwlClass","text":""},{"location":"pyintelowl/IntelOwlClass/#intelowl-class","title":"IntelOwl Class","text":"Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>class IntelOwl:\n    logger: logging.Logger\n\n    def __init__(\n        self,\n        token: str,\n        instance_url: str,\n        certificate: str = None,\n        proxies: dict = None,\n        logger: logging.Logger = None,\n        cli: bool = False,\n    ):\n        self.token = token\n        self.instance = instance_url\n        self.certificate = certificate\n        if logger:\n            self.logger = logger\n        else:\n            self.logger = logging.getLogger(__name__)\n        if proxies and not isinstance(proxies, dict):\n            raise TypeError(\"proxies param must be a dictionary\")\n        self.proxies = proxies\n        self.cli = cli\n\n    @property\n    def session(self) -&gt; requests.Session:\n        \"\"\"\n        Internal use only.\n        \"\"\"\n        if not hasattr(self, \"_session\"):\n            session = requests.Session()\n            if self.certificate is not True:\n                session.verify = self.certificate\n            if self.proxies:\n                session.proxies = self.proxies\n            session.headers.update(\n                {\n                    \"Authorization\": f\"Token {self.token}\",\n                    \"User-Agent\": f\"PyIntelOwl/{__version__}\",\n                }\n            )\n            self._session = session\n\n        return self._session\n\n    def __make_request(\n        self,\n        method: Literal[\"GET\", \"POST\", \"PUT\", \"PATCH\", \"DELETE\"] = \"GET\",\n        *args,\n        **kwargs,\n    ) -&gt; requests.Response:\n        \"\"\"\n        For internal use only.\n        \"\"\"\n        response: requests.Response = None\n        requests_function_map: Dict[str, Callable] = {\n            \"GET\": self.session.get,\n            \"POST\": self.session.post,\n            \"PUT\": self.session.put,\n            \"PATCH\": self.session.patch,\n            \"DELETE\": self.session.delete,\n        }\n        func = requests_function_map.get(method, None)\n        if not func:\n            raise RuntimeError(f\"Unsupported method name: {method}\")\n\n        try:\n            response = func(*args, **kwargs)\n            self.logger.debug(\n                msg=(response.url, response.status_code, response.content)\n            )\n            response.raise_for_status()\n        except Exception as e:\n            raise IntelOwlClientException(e, response=response)\n\n        return response\n\n    def ask_analysis_availability(\n        self,\n        md5: str,\n        analyzers: List[str] = None,\n        check_reported_analysis_too: bool = False,\n        minutes_ago: int = None,\n    ) -&gt; Dict:\n        \"\"\"Search for already available analysis.\\n\n        Endpoint: ``/api/ask_analysis_availability``\n\n        Args:\n            md5 (str): md5sum of the observable or file\n            analyzers (List[str], optional):\n            list of analyzers to trigger.\n            Defaults to `None` meaning automatically select all configured analyzers.\n            check_reported_analysis_too (bool, optional):\n            Check against all existing jobs. Defaults to ``False``.\n            minutes_ago (int, optional):\n            number of minutes to check back for analysis.\n            Default is None so the check does not have any time limits.\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict: JSON body\n        \"\"\"\n        if not analyzers:\n            analyzers = []\n        data = {\"md5\": md5, \"analyzers\": analyzers}\n        if not check_reported_analysis_too:\n            data[\"running_only\"] = True\n        if minutes_ago:\n            data[\"minutes_ago\"] = int(minutes_ago)\n        url = self.instance + \"/api/ask_analysis_availability\"\n        response = self.__make_request(\"POST\", url=url, data=data)\n        answer = response.json()\n        status, job_id = answer.get(\"status\", None), answer.get(\"job_id\", None)\n        # check sanity cases\n        if not status:\n            raise IntelOwlClientException(\n                \"API ask_analysis_availability gave result without status ?\"\n                f\" Response: {answer}\"\n            )\n        if status != \"not_available\" and not job_id:\n            raise IntelOwlClientException(\n                \"API ask_analysis_availability gave result without job_id ?\"\n                f\" Response: {answer}\"\n            )\n        return answer\n\n    def send_file_analysis_request(\n        self,\n        filename: str,\n        binary: bytes,\n        tlp: TLPType = \"CLEAR\",\n        analyzers_requested: List[str] = None,\n        connectors_requested: List[str] = None,\n        runtime_configuration: Dict = None,\n        tags_labels: List[str] = None,\n    ) -&gt; Dict:\n        \"\"\"Send analysis request for a file.\\n\n        Endpoint: ``/api/analyze_file``\n\n        Args:\n\n            filename (str):\n                Filename\n            binary (bytes):\n                File contents as bytes\n            analyzers_requested (List[str], optional):\n                List of analyzers to invoke\n                Defaults to ``[]`` i.e. all analyzers.\n            connectors_requested (List[str], optional):\n                List of specific connectors to invoke.\n                Defaults to ``[]`` i.e. all connectors.\n            tlp (str, optional):\n                TLP for the analysis.\n                (options: ``CLEAR, GREEN, AMBER, RED``).\n            runtime_configuration (Dict, optional):\n                Overwrite configuration for analyzers. Defaults to ``{}``.\n            tags_labels (List[str], optional):\n                List of tag labels to assign (creates non-existing tags)\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict: JSON body\n        \"\"\"\n        try:\n            if not tlp:\n                tlp = \"CLEAR\"\n            if not analyzers_requested:\n                analyzers_requested = []\n            if not connectors_requested:\n                connectors_requested = []\n            if not tags_labels:\n                tags_labels = []\n            if not runtime_configuration:\n                runtime_configuration = {}\n            data = {\n                \"file_name\": filename,\n                \"analyzers_requested\": analyzers_requested,\n                \"connectors_requested\": connectors_requested,\n                \"tlp\": tlp,\n                \"tags_labels\": tags_labels,\n            }\n            if runtime_configuration:\n                data[\"runtime_configuration\"] = json.dumps(runtime_configuration)\n            files = {\"file\": (filename, binary)}\n            answer = self.__send_analysis_request(data=data, files=files)\n        except Exception as e:\n            raise IntelOwlClientException(e)\n        return answer\n\n    def send_file_analysis_playbook_request(\n        self,\n        filename: str,\n        binary: bytes,\n        playbook_requested: str,\n        tlp: TLPType = \"CLEAR\",\n        runtime_configuration: Dict = None,\n        tags_labels: List[str] = None,\n    ) -&gt; Dict:\n        \"\"\"Send playbook analysis request for a file.\\n\n        Endpoint: ``/api/playbook/analyze_multiple_files``\n\n        Args:\n\n            filename (str):\n                Filename\n            binary (bytes):\n                File contents as bytes\n            playbook_requested (str, optional):\n            tlp (str, optional):\n                TLP for the analysis.\n                (options: ``WHITE, GREEN, AMBER, RED``).\n            runtime_configuration (Dict, optional):\n                Overwrite configuration for analyzers. Defaults to ``{}``.\n            tags_labels (List[str], optional):\n                List of tag labels to assign (creates non-existing tags)\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict: JSON body\n        \"\"\"\n        try:\n            if not tags_labels:\n                tags_labels = []\n            if not runtime_configuration:\n                runtime_configuration = {}\n            data = {\n                \"playbook_requested\": playbook_requested,\n                \"tags_labels\": tags_labels,\n            }\n            # send this value only if populated,\n            # otherwise the backend would give you 400\n            if tlp:\n                data[\"tlp\"] = tlp\n\n            if runtime_configuration:\n                data[\"runtime_configuration\"] = json.dumps(runtime_configuration)\n            # `files` is wanted to be different from the other\n            # /api/analyze_file endpoint\n            # because the server is using different serializers\n            files = {\"files\": (filename, binary)}\n            answer = self.__send_analysis_request(\n                data=data, files=files, playbook_mode=True\n            )\n        except Exception as e:\n            raise IntelOwlClientException(e)\n        return answer\n\n    def send_observable_analysis_request(\n        self,\n        observable_name: str,\n        tlp: TLPType = \"CLEAR\",\n        analyzers_requested: List[str] = None,\n        connectors_requested: List[str] = None,\n        runtime_configuration: Dict = None,\n        tags_labels: List[str] = None,\n        observable_classification: str = None,\n    ) -&gt; Dict:\n        \"\"\"Send analysis request for an observable.\\n\n        Endpoint: ``/api/analyze_observable``\n\n        Args:\n            observable_name (str):\n                Observable value\n            analyzers_requested (List[str], optional):\n                List of analyzers to invoke\n                Defaults to ``[]`` i.e. all analyzers.\n            connectors_requested (List[str], optional):\n                List of specific connectors to invoke.\n                Defaults to ``[]`` i.e. all connectors.\n            tlp (str, optional):\n                TLP for the analysis.\n                (options: ``CLEAR, GREEN, AMBER, RED``).\n            runtime_configuration (Dict, optional):\n                Overwrite configuration for analyzers. Defaults to ``{}``.\n            tags_labels (List[str], optional):\n                List of tag labels to assign (creates non-existing tags)\n            observable_classification (str):\n                Observable classification, Default to None.\n                By default launch analysis with an automatic classification.\n                (options: ``url, domain, hash, ip, generic``)\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n            IntelOwlClientException: on wrong observable_classification\n\n        Returns:\n            Dict: JSON body\n        \"\"\"\n        try:\n            if not tlp:\n                tlp = \"CLEAR\"\n            if not analyzers_requested:\n                analyzers_requested = []\n            if not connectors_requested:\n                connectors_requested = []\n            if not tags_labels:\n                tags_labels = []\n            if not runtime_configuration:\n                runtime_configuration = {}\n            if not observable_classification:\n                observable_classification = self._get_observable_classification(\n                    observable_name\n                )\n            elif observable_classification not in [\n                \"generic\",\n                \"hash\",\n                \"ip\",\n                \"domain\",\n                \"url\",\n            ]:\n                raise IntelOwlClientException(\n                    \"Observable classification only handle\"\n                    \" 'generic', 'hash', 'ip', 'domain' and 'url' \"\n                )\n            data = {\n                \"observable_name\": observable_name,\n                \"observable_classification\": observable_classification,\n                \"analyzers_requested\": analyzers_requested,\n                \"connectors_requested\": connectors_requested,\n                \"tlp\": tlp,\n                \"tags_labels\": tags_labels,\n                \"runtime_configuration\": runtime_configuration,\n            }\n            answer = self.__send_analysis_request(data=data, files=None)\n        except Exception as e:\n            raise IntelOwlClientException(e)\n        return answer\n\n    def send_observable_analysis_playbook_request(\n        self,\n        observable_name: str,\n        playbook_requested: str,\n        tlp: TLPType = \"CLEAR\",\n        runtime_configuration: Dict = None,\n        tags_labels: List[str] = None,\n        observable_classification: str = None,\n    ) -&gt; Dict:\n        \"\"\"Send playbook analysis request for an observable.\\n\n        Endpoint: ``/api/playbook/analyze_multiple_observables``\n\n        Args:\n            observable_name (str):\n                Observable value\n            playbook_requested str:\n            tlp (str, optional):\n                TLP for the analysis.\n                (options: ``WHITE, GREEN, AMBER, RED``).\n            runtime_configuration (Dict, optional):\n                Overwrite configuration for analyzers. Defaults to ``{}``.\n            tags_labels (List[str], optional):\n                List of tag labels to assign (creates non-existing tags)\n            observable_classification (str):\n                Observable classification, Default to None.\n                By default launch analysis with an automatic classification.\n                (options: ``url, domain, hash, ip, generic``)\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n            IntelOwlClientException: on wrong observable_classification\n\n        Returns:\n            Dict: JSON body\n        \"\"\"\n        try:\n            if not tags_labels:\n                tags_labels = []\n            if not runtime_configuration:\n                runtime_configuration = {}\n            if not observable_classification:\n                observable_classification = self._get_observable_classification(\n                    observable_name\n                )\n            elif observable_classification not in [\n                \"generic\",\n                \"hash\",\n                \"ip\",\n                \"domain\",\n                \"url\",\n            ]:\n                raise IntelOwlClientException(\n                    \"Observable classification only handle\"\n                    \" 'generic', 'hash', 'ip', 'domain' and 'url' \"\n                )\n            data = {\n                \"observables\": [[observable_classification, observable_name]],\n                \"playbook_requested\": playbook_requested,\n                \"tags_labels\": tags_labels,\n                \"runtime_configuration\": runtime_configuration,\n            }\n            # send this value only if populated,\n            # otherwise the backend would give you 400\n            if tlp:\n                data[\"tlp\"] = tlp\n            answer = self.__send_analysis_request(\n                data=data, files=None, playbook_mode=True\n            )\n        except Exception as e:\n            raise IntelOwlClientException(e)\n        return answer\n\n    def send_analysis_batch(self, rows: List[Dict]):\n        \"\"\"\n        Send multiple analysis requests.\n        Can be mix of observable or file analysis requests.\n\n        Used by the pyintelowl CLI.\n\n        Args:\n            rows (List[Dict]):\n                Each row should be a dictionary with keys,\n                `value`, `type`, `check`, `tlp`,\n                `analyzers_list`, `connectors_list`, `runtime_config`\n                `tags_list`.\n        \"\"\"\n        for obj in rows:\n            try:\n                runtime_config = obj.get(\"runtime_config\", {})\n                if runtime_config:\n                    with open(runtime_config) as fp:\n                        runtime_config = json.load(fp)\n\n                analyzers_list = obj.get(\"analyzers_list\", [])\n                connectors_list = obj.get(\"connectors_list\", [])\n                if isinstance(analyzers_list, str):\n                    analyzers_list = analyzers_list.split(\",\")\n                if isinstance(connectors_list, str):\n                    connectors_list = connectors_list.split(\",\")\n\n                self._new_analysis_cli(\n                    obj[\"value\"],\n                    obj[\"type\"],\n                    obj.get(\"check\", None),\n                    obj.get(\"tlp\", \"WHITE\"),\n                    analyzers_list,\n                    connectors_list,\n                    runtime_config,\n                    obj.get(\"tags_list\", []),\n                    obj.get(\"should_poll\", False),\n                )\n            except IntelOwlClientException as e:\n                self.logger.fatal(str(e))\n\n    def __send_analysis_request(self, data=None, files=None, playbook_mode=False):\n        \"\"\"\n        Internal use only.\n        \"\"\"\n        response = None\n        answer = {}\n        if files is None:\n            url = self.instance + \"/api/analyze_observable\"\n            if playbook_mode:\n                url = self.instance + \"/api/playbook/analyze_multiple_observables\"\n            args = {\"json\": data}\n        else:\n            url = self.instance + \"/api/analyze_file\"\n            if playbook_mode:\n                url = self.instance + \"/api/playbook/analyze_multiple_files\"\n            args = {\"data\": data, \"files\": files}\n        try:\n            response = self.session.post(url, **args)\n            self.logger.debug(\n                msg={\n                    \"url\": response.url,\n                    \"code\": response.status_code,\n                    \"request\": response.request.headers,\n                    \"headers\": response.headers,\n                    \"body\": response.json(),\n                }\n            )\n            answer = response.json()\n            if playbook_mode:\n                # right now, we are only supporting single input result\n                answers = answer.get(\"results\", [])\n                if answers:\n                    answer = answers[0]\n\n            warnings = answer.get(\"warnings\", [])\n            errors = answer.get(\"errors\", {})\n            if self.cli:\n                info_log = f\"\"\"New Job running..\n                    ID: {answer.get('job_id')} | \n                    Status: [u blue]{answer.get('status')}[/].\n                    Got {len(warnings)} warnings:\n                    [i yellow]{warnings if warnings else None}[/]\n                    Got {len(errors)} errors:\n                    [i red]{errors if errors else None}[/]\n                \"\"\"\n            else:\n                info_log = (\n                    f\"New Job running.. ID: {answer.get('job_id')} \"\n                    f\"| Status: {answer.get('status')}.\"\n                    f\" Got {len(warnings)} warnings:\"\n                    f\" {warnings if warnings else None}\"\n                    f\" Got {len(errors)} errors:\"\n                    f\" {errors if errors else None}\"\n                )\n            self.logger.info(info_log)\n            response.raise_for_status()\n        except Exception as e:\n            raise IntelOwlClientException(e, response=response)\n        return answer\n\n    def create_tag(self, label: str, color: str):\n        \"\"\"Creates new tag by sending a POST Request\n        Endpoint: ``/api/tags``\n\n        Args:\n            label ([str]): [Label of the tag to be created]\n            color ([str]): [Color of the tag to be created]\n        \"\"\"\n        url = self.instance + \"/api/tags\"\n        data = {\"label\": label, \"color\": color}\n        response = self.__make_request(\"POST\", url=url, data=data)\n        return response.json()\n\n    def edit_tag(self, tag_id: Union[int, str], label: str, color: str):\n        \"\"\"Edits existing tag by sending PUT request\n        Endpoint: ``api/tags``\n\n        Args:\n            id ([int]): [Id of the existing tag]\n            label ([str]): [Label of the tag to be created]\n            color ([str]): [Color of the tag to be created]\n        \"\"\"\n        url = self.instance + \"/api/tags/\" + str(tag_id)\n        data = {\"label\": label, \"color\": color}\n        response = self.__make_request(\"PUT\", url=url, data=data)\n        return response.json()\n\n    def get_all_tags(self) -&gt; List[Dict[str, str]]:\n        \"\"\"\n        Fetch list of all tags.\\n\n        Endpoint: ``/api/tags``\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            List[Dict[str, str]]: List of tags\n        \"\"\"\n        url = self.instance + \"/api/tags\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    def get_all_jobs(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Fetch list of all jobs.\\n\n        Endpoint: ``/api/jobs``\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict: Dict with 3 keys: \"count\", \"total_pages\", \"results\"\n        \"\"\"\n        url = self.instance + \"/api/jobs\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    def get_tag_by_id(self, tag_id: Union[int, str]) -&gt; Dict[str, str]:\n        \"\"\"Fetch tag info by ID.\\n\n        Endpoint: ``/api/tag/{tag_id}``\n\n        Args:\n            tag_id (Union[int, str]): Tag ID\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict[str, str]: Dict with 3 keys: `id`, `label` and `color`.\n        \"\"\"\n\n        url = self.instance + \"/api/tags/\" + str(tag_id)\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    def get_job_by_id(self, job_id: Union[int, str]) -&gt; Dict[str, Any]:\n        \"\"\"Fetch job info by ID.\n        Endpoint: ``/api/jobs/{job_id}``\n\n        Args:\n            job_id (Union[int, str]): Job ID\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict[str, Any]: JSON body.\n        \"\"\"\n        url = self.instance + \"/api/jobs/\" + str(job_id)\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    def add_job_to_investigation(\n        self, investigation_id: Union[int, str], job_id: Union[int, str]\n    ):\n        \"\"\"Add an existing job to an existing investigation.\n        Endpoint: ``/api/investigation/{job_id}/add_job``\n\n        Args:\n            job_id (Union[int, str]): Job ID\n            investigation_id (Union[int, str]): Investigation ID\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict[str, Any]: JSON body.\n        \"\"\"\n        url: str = self.instance + f\"/api/investigation/{str(investigation_id)}/add_job\"\n        data: dict = {\"job\": job_id}\n        response = self.__make_request(\"POST\", url=url, data=data)\n        return response.json()\n\n    def delete_job_from_investigation(\n        self, investigation_id: Union[int, str], job_id: Union[int, str]\n    ):\n        \"\"\"Delete a job from an existing investigation.\n        Endpoint: ``/api/investigation/{job_id}/remove_job``\n\n        Args:\n            job_id (Union[int, str]): Job ID\n            investigation_id (Union[int, str]): Investigation ID\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict[str, Any]: JSON body.\n        \"\"\"\n        url: str = (\n            self.instance + f\"/api/investigation/{str(investigation_id)}/remove_job\"\n        )\n        data: dict = {\"job\": job_id}\n        response = self.__make_request(\"POST\", url=url, data=data)\n        return response.json()\n\n    def get_all_investigations(self) -&gt; Dict[str, Any]:\n        \"\"\"Fetch all investigations info.\n        Endpoint: ``/api/investigation/``\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict[str, Any]: JSON body.\n        \"\"\"\n        url = self.instance + \"/api/investigation\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    def get_investigation_by_id(\n        self, investigation_id: Union[int, str]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Fetch investigation info by ID.\n        Endpoint: ``/api/investigation/{job_id}``\n\n        Args:\n            investigation_id (Union[int, str]): Investigation ID to retrieve\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict[str, Any]: JSON body.\n        \"\"\"\n        url = self.instance + \"/api/investigation/\" + str(investigation_id)\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    def get_investigation_tree_by_id(\n        self, investigation_id: Union[int, str]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Fetch investigation tree info by ID.\n        Endpoint: ``/api/investigation/{job_id}/tree``\n\n        Args:\n            investigation_id (Union[int, str]): Investigation ID to retrieve\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict[str, Any]: JSON body.\n        \"\"\"\n        url = self.instance + \"/api/investigation/\" + str(investigation_id) + \"/tree\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    @staticmethod\n    def get_md5(\n        to_hash: AnyStr,\n        type_=\"observable\",\n    ) -&gt; str:\n        \"\"\"Returns md5sum of given observable or file object.\n\n        Args:\n            to_hash (AnyStr):\n                either an observable string, file contents as bytes or path to a file\n            type_ (Union[\"observable\", \"binary\", \"file\"], optional):\n                `observable`, `binary`, `file`. Defaults to \"observable\".\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            str: md5sum\n        \"\"\"\n        md5 = \"\"\n        if type_ == \"observable\":\n            md5 = hashlib.md5(str(to_hash).lower().encode(\"utf-8\")).hexdigest()\n        elif type_ == \"binary\":\n            md5 = hashlib.md5(to_hash).hexdigest()\n        elif type_ == \"file\":\n            path = pathlib.Path(to_hash)\n            if not path.exists():\n                raise IntelOwlClientException(f\"{to_hash} does not exists\")\n            binary = path.read_bytes()\n            md5 = hashlib.md5(binary).hexdigest()\n        return md5\n\n    def _new_analysis_cli(\n        self,\n        obj: str,\n        type_: str,\n        check,\n        tlp: TLPType = None,\n        analyzers_list: List[str] = None,\n        connectors_list: List[str] = None,\n        runtime_configuration: Dict = None,\n        tags_labels: List[str] = None,\n        should_poll: bool = False,\n        minutes_ago: int = None,\n    ) -&gt; None:\n        \"\"\"\n        For internal use by the pyintelowl CLI.\n        \"\"\"\n        if not analyzers_list:\n            analyzers_list = []\n        if not connectors_list:\n            connectors_list = []\n        if not runtime_configuration:\n            runtime_configuration = {}\n        if not tags_labels:\n            tags_labels = []\n        self.logger.info(\n            f\"\"\"Requesting analysis..\n            {type_}: [blue]{obj}[/]\n            analyzers: [i green]{analyzers_list if analyzers_list else 'none'}[/]\n            connectors: [i green]{connectors_list if connectors_list else 'none'}[/]\n            tags: [i green]{tags_labels}[/]\n            \"\"\"\n        )\n        # 1st step: ask analysis availability\n        if check != \"force-new\":\n            md5 = self.get_md5(obj, type_=type_)\n\n            resp = self.ask_analysis_availability(\n                md5,\n                analyzers_list,\n                True if check == \"reported\" else False,\n                minutes_ago,\n            )\n            status, job_id = resp.get(\"status\", None), resp.get(\"job_id\", None)\n            if status != \"not_available\":\n                self.logger.info(\n                    f\"\"\"Found existing analysis!\n                Job: #{job_id}\n                status: [u blue]{status}[/]\n\n                [i]Hint: use [#854442]--check force-new[/] to perform new scan anyway[/]\n                    \"\"\"\n                )\n                return\n        # 2nd step: send new analysis request\n        if type_ == \"observable\":\n            resp2 = self.send_observable_analysis_request(\n                observable_name=obj,\n                tlp=tlp,\n                analyzers_requested=analyzers_list,\n                connectors_requested=connectors_list,\n                runtime_configuration=runtime_configuration,\n                tags_labels=tags_labels,\n            )\n        else:\n            path = pathlib.Path(obj)\n            resp2 = self.send_file_analysis_request(\n                filename=path.name,\n                binary=path.read_bytes(),\n                tlp=tlp,\n                analyzers_requested=analyzers_list,\n                connectors_requested=connectors_list,\n                runtime_configuration=runtime_configuration,\n                tags_labels=tags_labels,\n            )\n        # 3rd step: poll for result\n        if should_poll:\n            if resp2[\"status\"] != \"accepted\":\n                self.logger.fatal(\"Can't poll a failed job\")\n            # import poll function\n            from .cli._jobs_utils import _poll_for_job_cli\n\n            job_id = resp2[\"job_id\"]\n            _ = _poll_for_job_cli(self, job_id)\n            self.logger.info(\n                f\"\"\"\n        Polling finished.\n        Execute [i blue]pyintelowl jobs view {job_id}[/] to view the result\n                \"\"\"\n            )\n\n    def _new_analysis_playbook_cli(\n        self,\n        obj: str,\n        type_: str,\n        playbook: str,\n        tlp: TLPType = None,\n        runtime_configuration: Dict = None,\n        tags_labels: List[str] = None,\n        should_poll: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        For internal use by the pyintelowl CLI.\n        \"\"\"\n        if not runtime_configuration:\n            runtime_configuration = {}\n        if not tags_labels:\n            tags_labels = []\n\n        self.logger.info(\n            f\"\"\"Requesting analysis..\n            {type_}: [blue]{obj}[/]\n            playbook: [i green]{playbook}[/]\n            tags: [i green]{tags_labels}[/]\n            \"\"\"\n        )\n\n        # 1st step, make request\n        if type_ == \"observable\":\n            resp = self.send_observable_analysis_playbook_request(\n                observable_name=obj,\n                playbook_requested=playbook,\n                tlp=tlp,\n                runtime_configuration=runtime_configuration,\n                tags_labels=tags_labels,\n            )\n        else:\n            path = pathlib.Path(obj)\n            resp = self.send_file_analysis_playbook_request(\n                filename=path.name,\n                binary=path.read_bytes(),\n                playbook_requested=playbook,\n                tlp=tlp,\n                runtime_configuration=runtime_configuration,\n                tags_labels=tags_labels,\n            )\n\n        # 2nd step: poll for result\n        if should_poll:\n            if resp.get(\"status\", \"\") != \"accepted\":\n                self.logger.fatal(\"Can't poll a failed job\")\n            # import poll function\n            from .cli._jobs_utils import _poll_for_job_cli\n\n            job_id = resp.get(\"job_id\", 0)\n            _ = _poll_for_job_cli(self, job_id)\n            self.logger.info(\n                f\"\"\"\n                    Polling finished.\n                    Execute [i blue]pyintelowl jobs view {job_id}[/] to view the result\n                \"\"\"\n            )\n\n    def _get_observable_classification(self, value: str) -&gt; str:\n        \"\"\"Returns observable classification for the given value.\\n\n        Only following types are supported:\n        ip, domain, url, hash (md5, sha1, sha256), generic (if no match)\n\n        Args:\n            value (str):\n                observable value\n\n        Raises:\n            IntelOwlClientException:\n                if value type is not recognized\n\n        Returns:\n            str: one of `ip`, `url`, `domain`, `hash` or 'generic'.\n        \"\"\"\n        try:\n            ipaddress.ip_address(value)\n        except ValueError:\n            if re.match(\n                r\"^(?:htt|ft|tc)ps?://[a-z\\d-]{1,63}(?:\\.[a-z\\d-]{1,63})+\"\n                r\"(?:/[a-z\\d-]{1,63})*(?:\\.\\w+)?\",\n                value,\n            ):\n                classification = \"url\"\n            elif re.match(r\"^(\\.)?[a-z\\d-]{1,63}(\\.[a-z\\d-]{1,63})+$\", value):\n                classification = \"domain\"\n            elif (\n                re.match(r\"^[a-f\\d]{32}$\", value)\n                or re.match(r\"^[a-f\\d]{40}$\", value)\n                or re.match(r\"^[a-f\\d]{64}$\", value)\n                or re.match(r\"^[A-F\\d]{32}$\", value)\n                or re.match(r\"^[A-F\\d]{40}$\", value)\n                or re.match(r\"^[A-F\\d]{64}$\", value)\n            ):\n                classification = \"hash\"\n            else:\n                classification = \"generic\"\n                self.logger.warning(\n                    \"Couldn't detect observable classification, setting as 'generic'...\"\n                )\n        else:\n            # its a simple IP\n            classification = \"ip\"\n\n        return classification\n\n    def download_sample(self, job_id: int) -&gt; bytes:\n        \"\"\"\n        Download file sample from job.\\n\n        Method: GET\n        Endpoint: ``/api/jobs/{job_id}/download_sample``\n\n        Args:\n            job_id (int):\n                id of job to download sample from\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Bytes: Raw file data.\n        \"\"\"\n\n        url = self.instance + f\"/api/jobs/{job_id}/download_sample\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.content\n\n    def kill_running_job(self, job_id: int) -&gt; bool:\n        \"\"\"Send kill_running_job request.\\n\n        Method: PATCH\n        Endpoint: ``/api/jobs/{job_id}/kill``\n\n        Args:\n            job_id (int):\n                id of job to kill\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Bool: killed or not\n        \"\"\"\n\n        url = self.instance + f\"/api/jobs/{job_id}/kill\"\n        response = self.__make_request(\"PATCH\", url=url)\n        killed = response.status_code == 204\n        return killed\n\n    def delete_job_by_id(self, job_id: int) -&gt; bool:\n        \"\"\"Send delete job request.\\n\n        Method: DELETE\n        Endpoint: ``/api/jobs/{job_id}``\n\n        Args:\n            job_id (int):\n                id of job to kill\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Bool: deleted or not\n        \"\"\"\n        url = self.instance + \"/api/jobs/\" + str(job_id)\n        response = self.__make_request(\"DELETE\", url=url)\n        deleted = response.status_code == 204\n        return deleted\n\n    def delete_tag_by_id(self, tag_id: int) -&gt; bool:\n        \"\"\"Send delete tag request.\\n\n        Method: DELETE\n        Endpoint: ``/api/tags/{tag_id}``\n\n        Args:\n            tag_id (int):\n                id of tag to delete\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Bool: deleted or not\n        \"\"\"\n\n        url = self.instance + \"/api/tags/\" + str(tag_id)\n        response = self.__make_request(\"DELETE\", url=url)\n        deleted = response.status_code == 204\n        return deleted\n\n    def __run_plugin_action(\n        self, job_id: int, plugin_type: str, plugin_name: str, plugin_action: str\n    ) -&gt; bool:\n        \"\"\"Internal method for kill/retry for analyzer/connector\"\"\"\n        response = None\n        url = (\n            self.instance\n            + f\"/api/jobs/{job_id}/{plugin_type}/{plugin_name}/{plugin_action}\"\n        )\n        response = self.__make_request(\"PATCH\", url=url)\n        success = response.status_code == 204\n        return success\n\n    def kill_analyzer(self, job_id: int, analyzer_name: str) -&gt; bool:\n        \"\"\"Send kill running/pending analyzer request.\\n\n        Method: PATCH\n        Endpoint: ``/api/jobs/{job_id}/analyzer/{analyzer_name}/kill``\n\n        Args:\n            job_id (int):\n                id of job\n            analyzer_name (str):\n                name of analyzer to kill\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Bool: killed or not\n        \"\"\"\n\n        killed = self.__run_plugin_action(\n            job_id=job_id,\n            plugin_name=analyzer_name,\n            plugin_type=\"analyzer\",\n            plugin_action=\"kill\",\n        )\n        return killed\n\n    def kill_connector(self, job_id: int, connector_name: str) -&gt; bool:\n        \"\"\"Send kill running/pending connector request.\\n\n        Method: PATCH\n        Endpoint: ``/api/jobs/{job_id}/connector/{connector_name}/kill``\n\n        Args:\n            job_id (int):\n                id of job\n            connector_name (str):\n                name of connector to kill\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Bool: killed or not\n        \"\"\"\n\n        killed = self.__run_plugin_action(\n            job_id=job_id,\n            plugin_name=connector_name,\n            plugin_type=\"connector\",\n            plugin_action=\"kill\",\n        )\n        return killed\n\n    def retry_analyzer(self, job_id: int, analyzer_name: str) -&gt; bool:\n        \"\"\"Send retry failed/killed analyzer request.\\n\n        Method: PATCH\n        Endpoint: ``/api/jobs/{job_id}/analyzer/{analyzer_name}/retry``\n\n        Args:\n            job_id (int):\n                id of job\n            analyzer_name (str):\n                name of analyzer to retry\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Bool: success or not\n        \"\"\"\n\n        success = self.__run_plugin_action(\n            job_id=job_id,\n            plugin_name=analyzer_name,\n            plugin_type=\"analyzer\",\n            plugin_action=\"retry\",\n        )\n        return success\n\n    def retry_connector(self, job_id: int, connector_name: str) -&gt; bool:\n        \"\"\"Send retry failed/killed connector request.\\n\n        Method: PATCH\n        Endpoint: ``/api/jobs/{job_id}/connector/{connector_name}/retry``\n\n        Args:\n            job_id (int):\n                id of job\n            connector_name (str):\n                name of connector to retry\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Bool: success or not\n        \"\"\"\n\n        success = self.__run_plugin_action(\n            job_id=job_id,\n            plugin_name=connector_name,\n            plugin_type=\"connector\",\n            plugin_action=\"retry\",\n        )\n        return success\n\n    def analyzer_healthcheck(self, analyzer_name: str) -&gt; Optional[bool]:\n        \"\"\"Send analyzer(docker-based) health check request.\\n\n        Method: GET\n        Endpoint: ``/api/analyzer/{analyzer_name}/healthcheck``\n\n        Args:\n            analyzer_name (str):\n                name of analyzer\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Bool: success or not\n        \"\"\"\n\n        url = self.instance + f\"/api/analyzer/{analyzer_name}/healthcheck\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.json().get(\"status\", None)\n\n    def connector_healthcheck(self, connector_name: str) -&gt; Optional[bool]:\n        \"\"\"Send connector health check request.\\n\n        Method: GET\n        Endpoint: ``/api/connector/{connector_name}/healthcheck``\n\n        Args:\n            connector_name (str):\n                name of connector\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Bool: success or not\n        \"\"\"\n        url = self.instance + f\"/api/connector/{connector_name}/healthcheck\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.json().get(\"status\", None)\n\n    def get_playbook_by_name(self, playbook_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Fetch playbook info by its name.\n        Endpoint: ``/api/playbook/{playbook_name}``\n\n        Args:\n            playbook_name (str): Playbook name to retrieve\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict[str, Any]: JSON body.\n        \"\"\"\n        url = self.instance + \"/api/playbook/\" + playbook_name\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    def get_all_playbooks(self) -&gt; Dict[str, Any]:\n        \"\"\"Fetch all playbooks info.\n        Endpoint: ``/api/playbook``\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n\n        Returns:\n            Dict[str, Any]: JSON body.\n        \"\"\"\n        url = self.instance + \"/api/playbook\"\n        response = self.__make_request(\"GET\", url=url)\n        return response.json()\n\n    def disable_playbook_for_org(self, playbook_name: str):\n        \"\"\"Disables the plugin for the organization of the user.\n        Endpoint: ``/api/playbook/{playbook_name}/organization``\n\n        Args:\n            playbook_name (str): Playbook name to disable for org\n\n        Raises:\n            IntelOwlClientException: on client/HTTP error\n        \"\"\"\n        url = self.instance + \"/api/playbook/\" + playbook_name + \"/organization\"\n        # this call doesn't have a response\n        self.__make_request(\"POST\", url=url)\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.session","title":"<code>session: requests.Session</code>  <code>property</code>","text":"<p>Internal use only.</p>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.__make_request","title":"<code>__make_request(method='GET', *args, **kwargs)</code>","text":"<p>For internal use only.</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def __make_request(\n    self,\n    method: Literal[\"GET\", \"POST\", \"PUT\", \"PATCH\", \"DELETE\"] = \"GET\",\n    *args,\n    **kwargs,\n) -&gt; requests.Response:\n    \"\"\"\n    For internal use only.\n    \"\"\"\n    response: requests.Response = None\n    requests_function_map: Dict[str, Callable] = {\n        \"GET\": self.session.get,\n        \"POST\": self.session.post,\n        \"PUT\": self.session.put,\n        \"PATCH\": self.session.patch,\n        \"DELETE\": self.session.delete,\n    }\n    func = requests_function_map.get(method, None)\n    if not func:\n        raise RuntimeError(f\"Unsupported method name: {method}\")\n\n    try:\n        response = func(*args, **kwargs)\n        self.logger.debug(\n            msg=(response.url, response.status_code, response.content)\n        )\n        response.raise_for_status()\n    except Exception as e:\n        raise IntelOwlClientException(e, response=response)\n\n    return response\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.__run_plugin_action","title":"<code>__run_plugin_action(job_id, plugin_type, plugin_name, plugin_action)</code>","text":"<p>Internal method for kill/retry for analyzer/connector</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def __run_plugin_action(\n    self, job_id: int, plugin_type: str, plugin_name: str, plugin_action: str\n) -&gt; bool:\n    \"\"\"Internal method for kill/retry for analyzer/connector\"\"\"\n    response = None\n    url = (\n        self.instance\n        + f\"/api/jobs/{job_id}/{plugin_type}/{plugin_name}/{plugin_action}\"\n    )\n    response = self.__make_request(\"PATCH\", url=url)\n    success = response.status_code == 204\n    return success\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.__send_analysis_request","title":"<code>__send_analysis_request(data=None, files=None, playbook_mode=False)</code>","text":"<p>Internal use only.</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def __send_analysis_request(self, data=None, files=None, playbook_mode=False):\n    \"\"\"\n    Internal use only.\n    \"\"\"\n    response = None\n    answer = {}\n    if files is None:\n        url = self.instance + \"/api/analyze_observable\"\n        if playbook_mode:\n            url = self.instance + \"/api/playbook/analyze_multiple_observables\"\n        args = {\"json\": data}\n    else:\n        url = self.instance + \"/api/analyze_file\"\n        if playbook_mode:\n            url = self.instance + \"/api/playbook/analyze_multiple_files\"\n        args = {\"data\": data, \"files\": files}\n    try:\n        response = self.session.post(url, **args)\n        self.logger.debug(\n            msg={\n                \"url\": response.url,\n                \"code\": response.status_code,\n                \"request\": response.request.headers,\n                \"headers\": response.headers,\n                \"body\": response.json(),\n            }\n        )\n        answer = response.json()\n        if playbook_mode:\n            # right now, we are only supporting single input result\n            answers = answer.get(\"results\", [])\n            if answers:\n                answer = answers[0]\n\n        warnings = answer.get(\"warnings\", [])\n        errors = answer.get(\"errors\", {})\n        if self.cli:\n            info_log = f\"\"\"New Job running..\n                ID: {answer.get('job_id')} | \n                Status: [u blue]{answer.get('status')}[/].\n                Got {len(warnings)} warnings:\n                [i yellow]{warnings if warnings else None}[/]\n                Got {len(errors)} errors:\n                [i red]{errors if errors else None}[/]\n            \"\"\"\n        else:\n            info_log = (\n                f\"New Job running.. ID: {answer.get('job_id')} \"\n                f\"| Status: {answer.get('status')}.\"\n                f\" Got {len(warnings)} warnings:\"\n                f\" {warnings if warnings else None}\"\n                f\" Got {len(errors)} errors:\"\n                f\" {errors if errors else None}\"\n            )\n        self.logger.info(info_log)\n        response.raise_for_status()\n    except Exception as e:\n        raise IntelOwlClientException(e, response=response)\n    return answer\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.add_job_to_investigation","title":"<code>add_job_to_investigation(investigation_id, job_id)</code>","text":"<p>Add an existing job to an existing investigation. Endpoint: <code>/api/investigation/{job_id}/add_job</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>Union[int, str]</code> <p>Job ID</p> required <code>investigation_id</code> <code>Union[int, str]</code> <p>Investigation ID</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <p>Dict[str, Any]: JSON body.</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def add_job_to_investigation(\n    self, investigation_id: Union[int, str], job_id: Union[int, str]\n):\n    \"\"\"Add an existing job to an existing investigation.\n    Endpoint: ``/api/investigation/{job_id}/add_job``\n\n    Args:\n        job_id (Union[int, str]): Job ID\n        investigation_id (Union[int, str]): Investigation ID\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict[str, Any]: JSON body.\n    \"\"\"\n    url: str = self.instance + f\"/api/investigation/{str(investigation_id)}/add_job\"\n    data: dict = {\"job\": job_id}\n    response = self.__make_request(\"POST\", url=url, data=data)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.analyzer_healthcheck","title":"<code>analyzer_healthcheck(analyzer_name)</code>","text":"<p>Send analyzer(docker-based) health check request.</p> <p>Method: GET Endpoint: <code>/api/analyzer/{analyzer_name}/healthcheck</code></p> <p>Parameters:</p> Name Type Description Default <code>analyzer_name</code> <code>str</code> <p>name of analyzer</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>Optional[bool]</code> <p>success or not</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def analyzer_healthcheck(self, analyzer_name: str) -&gt; Optional[bool]:\n    \"\"\"Send analyzer(docker-based) health check request.\\n\n    Method: GET\n    Endpoint: ``/api/analyzer/{analyzer_name}/healthcheck``\n\n    Args:\n        analyzer_name (str):\n            name of analyzer\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Bool: success or not\n    \"\"\"\n\n    url = self.instance + f\"/api/analyzer/{analyzer_name}/healthcheck\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.json().get(\"status\", None)\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.ask_analysis_availability","title":"<code>ask_analysis_availability(md5, analyzers=None, check_reported_analysis_too=False, minutes_ago=None)</code>","text":"<p>Search for already available analysis.</p> <p>Endpoint: <code>/api/ask_analysis_availability</code></p> <p>Parameters:</p> Name Type Description Default <code>md5</code> <code>str</code> <p>md5sum of the observable or file</p> required <code>analyzers</code> <code>List[str]</code> <code>None</code> <code>check_reported_analysis_too</code> <code>bool</code> <code>False</code> <code>minutes_ago</code> <code>int</code> <code>None</code> <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>JSON body</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def ask_analysis_availability(\n    self,\n    md5: str,\n    analyzers: List[str] = None,\n    check_reported_analysis_too: bool = False,\n    minutes_ago: int = None,\n) -&gt; Dict:\n    \"\"\"Search for already available analysis.\\n\n    Endpoint: ``/api/ask_analysis_availability``\n\n    Args:\n        md5 (str): md5sum of the observable or file\n        analyzers (List[str], optional):\n        list of analyzers to trigger.\n        Defaults to `None` meaning automatically select all configured analyzers.\n        check_reported_analysis_too (bool, optional):\n        Check against all existing jobs. Defaults to ``False``.\n        minutes_ago (int, optional):\n        number of minutes to check back for analysis.\n        Default is None so the check does not have any time limits.\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict: JSON body\n    \"\"\"\n    if not analyzers:\n        analyzers = []\n    data = {\"md5\": md5, \"analyzers\": analyzers}\n    if not check_reported_analysis_too:\n        data[\"running_only\"] = True\n    if minutes_ago:\n        data[\"minutes_ago\"] = int(minutes_ago)\n    url = self.instance + \"/api/ask_analysis_availability\"\n    response = self.__make_request(\"POST\", url=url, data=data)\n    answer = response.json()\n    status, job_id = answer.get(\"status\", None), answer.get(\"job_id\", None)\n    # check sanity cases\n    if not status:\n        raise IntelOwlClientException(\n            \"API ask_analysis_availability gave result without status ?\"\n            f\" Response: {answer}\"\n        )\n    if status != \"not_available\" and not job_id:\n        raise IntelOwlClientException(\n            \"API ask_analysis_availability gave result without job_id ?\"\n            f\" Response: {answer}\"\n        )\n    return answer\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.connector_healthcheck","title":"<code>connector_healthcheck(connector_name)</code>","text":"<p>Send connector health check request.</p> <p>Method: GET Endpoint: <code>/api/connector/{connector_name}/healthcheck</code></p> <p>Parameters:</p> Name Type Description Default <code>connector_name</code> <code>str</code> <p>name of connector</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>Optional[bool]</code> <p>success or not</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def connector_healthcheck(self, connector_name: str) -&gt; Optional[bool]:\n    \"\"\"Send connector health check request.\\n\n    Method: GET\n    Endpoint: ``/api/connector/{connector_name}/healthcheck``\n\n    Args:\n        connector_name (str):\n            name of connector\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Bool: success or not\n    \"\"\"\n    url = self.instance + f\"/api/connector/{connector_name}/healthcheck\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.json().get(\"status\", None)\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.create_tag","title":"<code>create_tag(label, color)</code>","text":"<p>Creates new tag by sending a POST Request Endpoint: <code>/api/tags</code></p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>[str]</code> <p>[Label of the tag to be created]</p> required <code>color</code> <code>[str]</code> <p>[Color of the tag to be created]</p> required Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def create_tag(self, label: str, color: str):\n    \"\"\"Creates new tag by sending a POST Request\n    Endpoint: ``/api/tags``\n\n    Args:\n        label ([str]): [Label of the tag to be created]\n        color ([str]): [Color of the tag to be created]\n    \"\"\"\n    url = self.instance + \"/api/tags\"\n    data = {\"label\": label, \"color\": color}\n    response = self.__make_request(\"POST\", url=url, data=data)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.delete_job_by_id","title":"<code>delete_job_by_id(job_id)</code>","text":"<p>Send delete job request.</p> <p>Method: DELETE Endpoint: <code>/api/jobs/{job_id}</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job to kill</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>deleted or not</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def delete_job_by_id(self, job_id: int) -&gt; bool:\n    \"\"\"Send delete job request.\\n\n    Method: DELETE\n    Endpoint: ``/api/jobs/{job_id}``\n\n    Args:\n        job_id (int):\n            id of job to kill\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Bool: deleted or not\n    \"\"\"\n    url = self.instance + \"/api/jobs/\" + str(job_id)\n    response = self.__make_request(\"DELETE\", url=url)\n    deleted = response.status_code == 204\n    return deleted\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.delete_job_from_investigation","title":"<code>delete_job_from_investigation(investigation_id, job_id)</code>","text":"<p>Delete a job from an existing investigation. Endpoint: <code>/api/investigation/{job_id}/remove_job</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>Union[int, str]</code> <p>Job ID</p> required <code>investigation_id</code> <code>Union[int, str]</code> <p>Investigation ID</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <p>Dict[str, Any]: JSON body.</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def delete_job_from_investigation(\n    self, investigation_id: Union[int, str], job_id: Union[int, str]\n):\n    \"\"\"Delete a job from an existing investigation.\n    Endpoint: ``/api/investigation/{job_id}/remove_job``\n\n    Args:\n        job_id (Union[int, str]): Job ID\n        investigation_id (Union[int, str]): Investigation ID\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict[str, Any]: JSON body.\n    \"\"\"\n    url: str = (\n        self.instance + f\"/api/investigation/{str(investigation_id)}/remove_job\"\n    )\n    data: dict = {\"job\": job_id}\n    response = self.__make_request(\"POST\", url=url, data=data)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.delete_tag_by_id","title":"<code>delete_tag_by_id(tag_id)</code>","text":"<p>Send delete tag request.</p> <p>Method: DELETE Endpoint: <code>/api/tags/{tag_id}</code></p> <p>Parameters:</p> Name Type Description Default <code>tag_id</code> <code>int</code> <p>id of tag to delete</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>deleted or not</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def delete_tag_by_id(self, tag_id: int) -&gt; bool:\n    \"\"\"Send delete tag request.\\n\n    Method: DELETE\n    Endpoint: ``/api/tags/{tag_id}``\n\n    Args:\n        tag_id (int):\n            id of tag to delete\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Bool: deleted or not\n    \"\"\"\n\n    url = self.instance + \"/api/tags/\" + str(tag_id)\n    response = self.__make_request(\"DELETE\", url=url)\n    deleted = response.status_code == 204\n    return deleted\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.disable_playbook_for_org","title":"<code>disable_playbook_for_org(playbook_name)</code>","text":"<p>Disables the plugin for the organization of the user. Endpoint: <code>/api/playbook/{playbook_name}/organization</code></p> <p>Parameters:</p> Name Type Description Default <code>playbook_name</code> <code>str</code> <p>Playbook name to disable for org</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def disable_playbook_for_org(self, playbook_name: str):\n    \"\"\"Disables the plugin for the organization of the user.\n    Endpoint: ``/api/playbook/{playbook_name}/organization``\n\n    Args:\n        playbook_name (str): Playbook name to disable for org\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n    \"\"\"\n    url = self.instance + \"/api/playbook/\" + playbook_name + \"/organization\"\n    # this call doesn't have a response\n    self.__make_request(\"POST\", url=url)\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.download_sample","title":"<code>download_sample(job_id)</code>","text":"<p>Download file sample from job.</p> <p>Method: GET Endpoint: <code>/api/jobs/{job_id}/download_sample</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job to download sample from</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bytes</code> <code>bytes</code> <p>Raw file data.</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def download_sample(self, job_id: int) -&gt; bytes:\n    \"\"\"\n    Download file sample from job.\\n\n    Method: GET\n    Endpoint: ``/api/jobs/{job_id}/download_sample``\n\n    Args:\n        job_id (int):\n            id of job to download sample from\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Bytes: Raw file data.\n    \"\"\"\n\n    url = self.instance + f\"/api/jobs/{job_id}/download_sample\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.content\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.edit_tag","title":"<code>edit_tag(tag_id, label, color)</code>","text":"<p>Edits existing tag by sending PUT request Endpoint: <code>api/tags</code></p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>[int]</code> <p>[Id of the existing tag]</p> required <code>label</code> <code>[str]</code> <p>[Label of the tag to be created]</p> required <code>color</code> <code>[str]</code> <p>[Color of the tag to be created]</p> required Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def edit_tag(self, tag_id: Union[int, str], label: str, color: str):\n    \"\"\"Edits existing tag by sending PUT request\n    Endpoint: ``api/tags``\n\n    Args:\n        id ([int]): [Id of the existing tag]\n        label ([str]): [Label of the tag to be created]\n        color ([str]): [Color of the tag to be created]\n    \"\"\"\n    url = self.instance + \"/api/tags/\" + str(tag_id)\n    data = {\"label\": label, \"color\": color}\n    response = self.__make_request(\"PUT\", url=url, data=data)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.get_all_investigations","title":"<code>get_all_investigations()</code>","text":"<p>Fetch all investigations info. Endpoint: <code>/api/investigation/</code></p> <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: JSON body.</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def get_all_investigations(self) -&gt; Dict[str, Any]:\n    \"\"\"Fetch all investigations info.\n    Endpoint: ``/api/investigation/``\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict[str, Any]: JSON body.\n    \"\"\"\n    url = self.instance + \"/api/investigation\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.get_all_jobs","title":"<code>get_all_jobs()</code>","text":"<p>Fetch list of all jobs.</p> <p>Endpoint: <code>/api/jobs</code></p> <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>List[Dict[str, Any]]</code> <p>Dict with 3 keys: \"count\", \"total_pages\", \"results\"</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def get_all_jobs(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Fetch list of all jobs.\\n\n    Endpoint: ``/api/jobs``\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict: Dict with 3 keys: \"count\", \"total_pages\", \"results\"\n    \"\"\"\n    url = self.instance + \"/api/jobs\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.get_all_playbooks","title":"<code>get_all_playbooks()</code>","text":"<p>Fetch all playbooks info. Endpoint: <code>/api/playbook</code></p> <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: JSON body.</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def get_all_playbooks(self) -&gt; Dict[str, Any]:\n    \"\"\"Fetch all playbooks info.\n    Endpoint: ``/api/playbook``\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict[str, Any]: JSON body.\n    \"\"\"\n    url = self.instance + \"/api/playbook\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.get_all_tags","title":"<code>get_all_tags()</code>","text":"<p>Fetch list of all tags.</p> <p>Endpoint: <code>/api/tags</code></p> <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: List of tags</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def get_all_tags(self) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Fetch list of all tags.\\n\n    Endpoint: ``/api/tags``\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        List[Dict[str, str]]: List of tags\n    \"\"\"\n    url = self.instance + \"/api/tags\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.get_investigation_by_id","title":"<code>get_investigation_by_id(investigation_id)</code>","text":"<p>Fetch investigation info by ID. Endpoint: <code>/api/investigation/{job_id}</code></p> <p>Parameters:</p> Name Type Description Default <code>investigation_id</code> <code>Union[int, str]</code> <p>Investigation ID to retrieve</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: JSON body.</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def get_investigation_by_id(\n    self, investigation_id: Union[int, str]\n) -&gt; Dict[str, Any]:\n    \"\"\"Fetch investigation info by ID.\n    Endpoint: ``/api/investigation/{job_id}``\n\n    Args:\n        investigation_id (Union[int, str]): Investigation ID to retrieve\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict[str, Any]: JSON body.\n    \"\"\"\n    url = self.instance + \"/api/investigation/\" + str(investigation_id)\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.get_investigation_tree_by_id","title":"<code>get_investigation_tree_by_id(investigation_id)</code>","text":"<p>Fetch investigation tree info by ID. Endpoint: <code>/api/investigation/{job_id}/tree</code></p> <p>Parameters:</p> Name Type Description Default <code>investigation_id</code> <code>Union[int, str]</code> <p>Investigation ID to retrieve</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: JSON body.</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def get_investigation_tree_by_id(\n    self, investigation_id: Union[int, str]\n) -&gt; Dict[str, Any]:\n    \"\"\"Fetch investigation tree info by ID.\n    Endpoint: ``/api/investigation/{job_id}/tree``\n\n    Args:\n        investigation_id (Union[int, str]): Investigation ID to retrieve\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict[str, Any]: JSON body.\n    \"\"\"\n    url = self.instance + \"/api/investigation/\" + str(investigation_id) + \"/tree\"\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.get_job_by_id","title":"<code>get_job_by_id(job_id)</code>","text":"<p>Fetch job info by ID. Endpoint: <code>/api/jobs/{job_id}</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>Union[int, str]</code> <p>Job ID</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: JSON body.</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def get_job_by_id(self, job_id: Union[int, str]) -&gt; Dict[str, Any]:\n    \"\"\"Fetch job info by ID.\n    Endpoint: ``/api/jobs/{job_id}``\n\n    Args:\n        job_id (Union[int, str]): Job ID\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict[str, Any]: JSON body.\n    \"\"\"\n    url = self.instance + \"/api/jobs/\" + str(job_id)\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.get_md5","title":"<code>get_md5(to_hash, type_='observable')</code>  <code>staticmethod</code>","text":"<p>Returns md5sum of given observable or file object.</p> <p>Parameters:</p> Name Type Description Default <code>to_hash</code> <code>AnyStr</code> <p>either an observable string, file contents as bytes or path to a file</p> required <code>type_</code> <code>Union[observable, binary, file]</code> <p><code>observable</code>, <code>binary</code>, <code>file</code>. Defaults to \"observable\".</p> <code>'observable'</code> <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>md5sum</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>@staticmethod\ndef get_md5(\n    to_hash: AnyStr,\n    type_=\"observable\",\n) -&gt; str:\n    \"\"\"Returns md5sum of given observable or file object.\n\n    Args:\n        to_hash (AnyStr):\n            either an observable string, file contents as bytes or path to a file\n        type_ (Union[\"observable\", \"binary\", \"file\"], optional):\n            `observable`, `binary`, `file`. Defaults to \"observable\".\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        str: md5sum\n    \"\"\"\n    md5 = \"\"\n    if type_ == \"observable\":\n        md5 = hashlib.md5(str(to_hash).lower().encode(\"utf-8\")).hexdigest()\n    elif type_ == \"binary\":\n        md5 = hashlib.md5(to_hash).hexdigest()\n    elif type_ == \"file\":\n        path = pathlib.Path(to_hash)\n        if not path.exists():\n            raise IntelOwlClientException(f\"{to_hash} does not exists\")\n        binary = path.read_bytes()\n        md5 = hashlib.md5(binary).hexdigest()\n    return md5\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.get_playbook_by_name","title":"<code>get_playbook_by_name(playbook_name)</code>","text":"<p>Fetch playbook info by its name. Endpoint: <code>/api/playbook/{playbook_name}</code></p> <p>Parameters:</p> Name Type Description Default <code>playbook_name</code> <code>str</code> <p>Playbook name to retrieve</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: JSON body.</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def get_playbook_by_name(self, playbook_name: str) -&gt; Dict[str, Any]:\n    \"\"\"Fetch playbook info by its name.\n    Endpoint: ``/api/playbook/{playbook_name}``\n\n    Args:\n        playbook_name (str): Playbook name to retrieve\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict[str, Any]: JSON body.\n    \"\"\"\n    url = self.instance + \"/api/playbook/\" + playbook_name\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.get_tag_by_id","title":"<code>get_tag_by_id(tag_id)</code>","text":"<p>Fetch tag info by ID.</p> <p>Endpoint: <code>/api/tag/{tag_id}</code></p> <p>Parameters:</p> Name Type Description Default <code>tag_id</code> <code>Union[int, str]</code> <p>Tag ID</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Dict with 3 keys: <code>id</code>, <code>label</code> and <code>color</code>.</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def get_tag_by_id(self, tag_id: Union[int, str]) -&gt; Dict[str, str]:\n    \"\"\"Fetch tag info by ID.\\n\n    Endpoint: ``/api/tag/{tag_id}``\n\n    Args:\n        tag_id (Union[int, str]): Tag ID\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict[str, str]: Dict with 3 keys: `id`, `label` and `color`.\n    \"\"\"\n\n    url = self.instance + \"/api/tags/\" + str(tag_id)\n    response = self.__make_request(\"GET\", url=url)\n    return response.json()\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.kill_analyzer","title":"<code>kill_analyzer(job_id, analyzer_name)</code>","text":"<p>Send kill running/pending analyzer request.</p> <p>Method: PATCH Endpoint: <code>/api/jobs/{job_id}/analyzer/{analyzer_name}/kill</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job</p> required <code>analyzer_name</code> <code>str</code> <p>name of analyzer to kill</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>killed or not</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def kill_analyzer(self, job_id: int, analyzer_name: str) -&gt; bool:\n    \"\"\"Send kill running/pending analyzer request.\\n\n    Method: PATCH\n    Endpoint: ``/api/jobs/{job_id}/analyzer/{analyzer_name}/kill``\n\n    Args:\n        job_id (int):\n            id of job\n        analyzer_name (str):\n            name of analyzer to kill\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Bool: killed or not\n    \"\"\"\n\n    killed = self.__run_plugin_action(\n        job_id=job_id,\n        plugin_name=analyzer_name,\n        plugin_type=\"analyzer\",\n        plugin_action=\"kill\",\n    )\n    return killed\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.kill_connector","title":"<code>kill_connector(job_id, connector_name)</code>","text":"<p>Send kill running/pending connector request.</p> <p>Method: PATCH Endpoint: <code>/api/jobs/{job_id}/connector/{connector_name}/kill</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job</p> required <code>connector_name</code> <code>str</code> <p>name of connector to kill</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>killed or not</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def kill_connector(self, job_id: int, connector_name: str) -&gt; bool:\n    \"\"\"Send kill running/pending connector request.\\n\n    Method: PATCH\n    Endpoint: ``/api/jobs/{job_id}/connector/{connector_name}/kill``\n\n    Args:\n        job_id (int):\n            id of job\n        connector_name (str):\n            name of connector to kill\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Bool: killed or not\n    \"\"\"\n\n    killed = self.__run_plugin_action(\n        job_id=job_id,\n        plugin_name=connector_name,\n        plugin_type=\"connector\",\n        plugin_action=\"kill\",\n    )\n    return killed\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.kill_running_job","title":"<code>kill_running_job(job_id)</code>","text":"<p>Send kill_running_job request.</p> <p>Method: PATCH Endpoint: <code>/api/jobs/{job_id}/kill</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job to kill</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>killed or not</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def kill_running_job(self, job_id: int) -&gt; bool:\n    \"\"\"Send kill_running_job request.\\n\n    Method: PATCH\n    Endpoint: ``/api/jobs/{job_id}/kill``\n\n    Args:\n        job_id (int):\n            id of job to kill\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Bool: killed or not\n    \"\"\"\n\n    url = self.instance + f\"/api/jobs/{job_id}/kill\"\n    response = self.__make_request(\"PATCH\", url=url)\n    killed = response.status_code == 204\n    return killed\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.retry_analyzer","title":"<code>retry_analyzer(job_id, analyzer_name)</code>","text":"<p>Send retry failed/killed analyzer request.</p> <p>Method: PATCH Endpoint: <code>/api/jobs/{job_id}/analyzer/{analyzer_name}/retry</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job</p> required <code>analyzer_name</code> <code>str</code> <p>name of analyzer to retry</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>success or not</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def retry_analyzer(self, job_id: int, analyzer_name: str) -&gt; bool:\n    \"\"\"Send retry failed/killed analyzer request.\\n\n    Method: PATCH\n    Endpoint: ``/api/jobs/{job_id}/analyzer/{analyzer_name}/retry``\n\n    Args:\n        job_id (int):\n            id of job\n        analyzer_name (str):\n            name of analyzer to retry\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Bool: success or not\n    \"\"\"\n\n    success = self.__run_plugin_action(\n        job_id=job_id,\n        plugin_name=analyzer_name,\n        plugin_type=\"analyzer\",\n        plugin_action=\"retry\",\n    )\n    return success\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.retry_connector","title":"<code>retry_connector(job_id, connector_name)</code>","text":"<p>Send retry failed/killed connector request.</p> <p>Method: PATCH Endpoint: <code>/api/jobs/{job_id}/connector/{connector_name}/retry</code></p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>int</code> <p>id of job</p> required <code>connector_name</code> <code>str</code> <p>name of connector to retry</p> required <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>success or not</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def retry_connector(self, job_id: int, connector_name: str) -&gt; bool:\n    \"\"\"Send retry failed/killed connector request.\\n\n    Method: PATCH\n    Endpoint: ``/api/jobs/{job_id}/connector/{connector_name}/retry``\n\n    Args:\n        job_id (int):\n            id of job\n        connector_name (str):\n            name of connector to retry\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Bool: success or not\n    \"\"\"\n\n    success = self.__run_plugin_action(\n        job_id=job_id,\n        plugin_name=connector_name,\n        plugin_type=\"connector\",\n        plugin_action=\"retry\",\n    )\n    return success\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.send_analysis_batch","title":"<code>send_analysis_batch(rows)</code>","text":"<p>Send multiple analysis requests. Can be mix of observable or file analysis requests.</p> <p>Used by the pyintelowl CLI.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>List[Dict]</code> <p>Each row should be a dictionary with keys, <code>value</code>, <code>type</code>, <code>check</code>, <code>tlp</code>, <code>analyzers_list</code>, <code>connectors_list</code>, <code>runtime_config</code> <code>tags_list</code>.</p> required Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def send_analysis_batch(self, rows: List[Dict]):\n    \"\"\"\n    Send multiple analysis requests.\n    Can be mix of observable or file analysis requests.\n\n    Used by the pyintelowl CLI.\n\n    Args:\n        rows (List[Dict]):\n            Each row should be a dictionary with keys,\n            `value`, `type`, `check`, `tlp`,\n            `analyzers_list`, `connectors_list`, `runtime_config`\n            `tags_list`.\n    \"\"\"\n    for obj in rows:\n        try:\n            runtime_config = obj.get(\"runtime_config\", {})\n            if runtime_config:\n                with open(runtime_config) as fp:\n                    runtime_config = json.load(fp)\n\n            analyzers_list = obj.get(\"analyzers_list\", [])\n            connectors_list = obj.get(\"connectors_list\", [])\n            if isinstance(analyzers_list, str):\n                analyzers_list = analyzers_list.split(\",\")\n            if isinstance(connectors_list, str):\n                connectors_list = connectors_list.split(\",\")\n\n            self._new_analysis_cli(\n                obj[\"value\"],\n                obj[\"type\"],\n                obj.get(\"check\", None),\n                obj.get(\"tlp\", \"WHITE\"),\n                analyzers_list,\n                connectors_list,\n                runtime_config,\n                obj.get(\"tags_list\", []),\n                obj.get(\"should_poll\", False),\n            )\n        except IntelOwlClientException as e:\n            self.logger.fatal(str(e))\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.send_file_analysis_playbook_request","title":"<code>send_file_analysis_playbook_request(filename, binary, playbook_requested, tlp='CLEAR', runtime_configuration=None, tags_labels=None)</code>","text":"<p>Send playbook analysis request for a file.</p> <p>Endpoint: <code>/api/playbook/analyze_multiple_files</code></p> <p>Args:</p> <pre><code>filename (str):\n    Filename\nbinary (bytes):\n    File contents as bytes\nplaybook_requested (str, optional):\ntlp (str, optional):\n    TLP for the analysis.\n    (options: ``WHITE, GREEN, AMBER, RED``).\nruntime_configuration (Dict, optional):\n    Overwrite configuration for analyzers. Defaults to ``{}``.\ntags_labels (List[str], optional):\n    List of tag labels to assign (creates non-existing tags)\n</code></pre> <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>JSON body</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def send_file_analysis_playbook_request(\n    self,\n    filename: str,\n    binary: bytes,\n    playbook_requested: str,\n    tlp: TLPType = \"CLEAR\",\n    runtime_configuration: Dict = None,\n    tags_labels: List[str] = None,\n) -&gt; Dict:\n    \"\"\"Send playbook analysis request for a file.\\n\n    Endpoint: ``/api/playbook/analyze_multiple_files``\n\n    Args:\n\n        filename (str):\n            Filename\n        binary (bytes):\n            File contents as bytes\n        playbook_requested (str, optional):\n        tlp (str, optional):\n            TLP for the analysis.\n            (options: ``WHITE, GREEN, AMBER, RED``).\n        runtime_configuration (Dict, optional):\n            Overwrite configuration for analyzers. Defaults to ``{}``.\n        tags_labels (List[str], optional):\n            List of tag labels to assign (creates non-existing tags)\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict: JSON body\n    \"\"\"\n    try:\n        if not tags_labels:\n            tags_labels = []\n        if not runtime_configuration:\n            runtime_configuration = {}\n        data = {\n            \"playbook_requested\": playbook_requested,\n            \"tags_labels\": tags_labels,\n        }\n        # send this value only if populated,\n        # otherwise the backend would give you 400\n        if tlp:\n            data[\"tlp\"] = tlp\n\n        if runtime_configuration:\n            data[\"runtime_configuration\"] = json.dumps(runtime_configuration)\n        # `files` is wanted to be different from the other\n        # /api/analyze_file endpoint\n        # because the server is using different serializers\n        files = {\"files\": (filename, binary)}\n        answer = self.__send_analysis_request(\n            data=data, files=files, playbook_mode=True\n        )\n    except Exception as e:\n        raise IntelOwlClientException(e)\n    return answer\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.send_file_analysis_request","title":"<code>send_file_analysis_request(filename, binary, tlp='CLEAR', analyzers_requested=None, connectors_requested=None, runtime_configuration=None, tags_labels=None)</code>","text":"<p>Send analysis request for a file.</p> <p>Endpoint: <code>/api/analyze_file</code></p> <p>Args:</p> <pre><code>filename (str):\n    Filename\nbinary (bytes):\n    File contents as bytes\nanalyzers_requested (List[str], optional):\n    List of analyzers to invoke\n    Defaults to ``[]`` i.e. all analyzers.\nconnectors_requested (List[str], optional):\n    List of specific connectors to invoke.\n    Defaults to ``[]`` i.e. all connectors.\ntlp (str, optional):\n    TLP for the analysis.\n    (options: ``CLEAR, GREEN, AMBER, RED``).\nruntime_configuration (Dict, optional):\n    Overwrite configuration for analyzers. Defaults to ``{}``.\ntags_labels (List[str], optional):\n    List of tag labels to assign (creates non-existing tags)\n</code></pre> <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>JSON body</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def send_file_analysis_request(\n    self,\n    filename: str,\n    binary: bytes,\n    tlp: TLPType = \"CLEAR\",\n    analyzers_requested: List[str] = None,\n    connectors_requested: List[str] = None,\n    runtime_configuration: Dict = None,\n    tags_labels: List[str] = None,\n) -&gt; Dict:\n    \"\"\"Send analysis request for a file.\\n\n    Endpoint: ``/api/analyze_file``\n\n    Args:\n\n        filename (str):\n            Filename\n        binary (bytes):\n            File contents as bytes\n        analyzers_requested (List[str], optional):\n            List of analyzers to invoke\n            Defaults to ``[]`` i.e. all analyzers.\n        connectors_requested (List[str], optional):\n            List of specific connectors to invoke.\n            Defaults to ``[]`` i.e. all connectors.\n        tlp (str, optional):\n            TLP for the analysis.\n            (options: ``CLEAR, GREEN, AMBER, RED``).\n        runtime_configuration (Dict, optional):\n            Overwrite configuration for analyzers. Defaults to ``{}``.\n        tags_labels (List[str], optional):\n            List of tag labels to assign (creates non-existing tags)\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n\n    Returns:\n        Dict: JSON body\n    \"\"\"\n    try:\n        if not tlp:\n            tlp = \"CLEAR\"\n        if not analyzers_requested:\n            analyzers_requested = []\n        if not connectors_requested:\n            connectors_requested = []\n        if not tags_labels:\n            tags_labels = []\n        if not runtime_configuration:\n            runtime_configuration = {}\n        data = {\n            \"file_name\": filename,\n            \"analyzers_requested\": analyzers_requested,\n            \"connectors_requested\": connectors_requested,\n            \"tlp\": tlp,\n            \"tags_labels\": tags_labels,\n        }\n        if runtime_configuration:\n            data[\"runtime_configuration\"] = json.dumps(runtime_configuration)\n        files = {\"file\": (filename, binary)}\n        answer = self.__send_analysis_request(data=data, files=files)\n    except Exception as e:\n        raise IntelOwlClientException(e)\n    return answer\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.send_observable_analysis_playbook_request","title":"<code>send_observable_analysis_playbook_request(observable_name, playbook_requested, tlp='CLEAR', runtime_configuration=None, tags_labels=None, observable_classification=None)</code>","text":"<p>Send playbook analysis request for an observable.</p> <p>Endpoint: <code>/api/playbook/analyze_multiple_observables</code></p> <p>Parameters:</p> Name Type Description Default <code>observable_name</code> <code>str</code> <p>Observable value</p> required <code>playbook_requested</code> <code>str</code> required <code>tlp</code> <code>str</code> <p>TLP for the analysis. (options: <code>WHITE, GREEN, AMBER, RED</code>).</p> <code>'CLEAR'</code> <code>runtime_configuration</code> <code>Dict</code> <p>Overwrite configuration for analyzers. Defaults to <code>{}</code>.</p> <code>None</code> <code>tags_labels</code> <code>List[str]</code> <p>List of tag labels to assign (creates non-existing tags)</p> <code>None</code> <code>observable_classification</code> <code>str</code> <p>Observable classification, Default to None. By default launch analysis with an automatic classification. (options: <code>url, domain, hash, ip, generic</code>)</p> <code>None</code> <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <code>IntelOwlClientException</code> <p>on wrong observable_classification</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>JSON body</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def send_observable_analysis_playbook_request(\n    self,\n    observable_name: str,\n    playbook_requested: str,\n    tlp: TLPType = \"CLEAR\",\n    runtime_configuration: Dict = None,\n    tags_labels: List[str] = None,\n    observable_classification: str = None,\n) -&gt; Dict:\n    \"\"\"Send playbook analysis request for an observable.\\n\n    Endpoint: ``/api/playbook/analyze_multiple_observables``\n\n    Args:\n        observable_name (str):\n            Observable value\n        playbook_requested str:\n        tlp (str, optional):\n            TLP for the analysis.\n            (options: ``WHITE, GREEN, AMBER, RED``).\n        runtime_configuration (Dict, optional):\n            Overwrite configuration for analyzers. Defaults to ``{}``.\n        tags_labels (List[str], optional):\n            List of tag labels to assign (creates non-existing tags)\n        observable_classification (str):\n            Observable classification, Default to None.\n            By default launch analysis with an automatic classification.\n            (options: ``url, domain, hash, ip, generic``)\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n        IntelOwlClientException: on wrong observable_classification\n\n    Returns:\n        Dict: JSON body\n    \"\"\"\n    try:\n        if not tags_labels:\n            tags_labels = []\n        if not runtime_configuration:\n            runtime_configuration = {}\n        if not observable_classification:\n            observable_classification = self._get_observable_classification(\n                observable_name\n            )\n        elif observable_classification not in [\n            \"generic\",\n            \"hash\",\n            \"ip\",\n            \"domain\",\n            \"url\",\n        ]:\n            raise IntelOwlClientException(\n                \"Observable classification only handle\"\n                \" 'generic', 'hash', 'ip', 'domain' and 'url' \"\n            )\n        data = {\n            \"observables\": [[observable_classification, observable_name]],\n            \"playbook_requested\": playbook_requested,\n            \"tags_labels\": tags_labels,\n            \"runtime_configuration\": runtime_configuration,\n        }\n        # send this value only if populated,\n        # otherwise the backend would give you 400\n        if tlp:\n            data[\"tlp\"] = tlp\n        answer = self.__send_analysis_request(\n            data=data, files=None, playbook_mode=True\n        )\n    except Exception as e:\n        raise IntelOwlClientException(e)\n    return answer\n</code></pre>"},{"location":"pyintelowl/IntelOwlClass/#docs.Submodules.pyintelowl.pyintelowl.IntelOwl.send_observable_analysis_request","title":"<code>send_observable_analysis_request(observable_name, tlp='CLEAR', analyzers_requested=None, connectors_requested=None, runtime_configuration=None, tags_labels=None, observable_classification=None)</code>","text":"<p>Send analysis request for an observable.</p> <p>Endpoint: <code>/api/analyze_observable</code></p> <p>Parameters:</p> Name Type Description Default <code>observable_name</code> <code>str</code> <p>Observable value</p> required <code>analyzers_requested</code> <code>List[str]</code> <p>List of analyzers to invoke Defaults to <code>[]</code> i.e. all analyzers.</p> <code>None</code> <code>connectors_requested</code> <code>List[str]</code> <p>List of specific connectors to invoke. Defaults to <code>[]</code> i.e. all connectors.</p> <code>None</code> <code>tlp</code> <code>str</code> <p>TLP for the analysis. (options: <code>CLEAR, GREEN, AMBER, RED</code>).</p> <code>'CLEAR'</code> <code>runtime_configuration</code> <code>Dict</code> <p>Overwrite configuration for analyzers. Defaults to <code>{}</code>.</p> <code>None</code> <code>tags_labels</code> <code>List[str]</code> <p>List of tag labels to assign (creates non-existing tags)</p> <code>None</code> <code>observable_classification</code> <code>str</code> <p>Observable classification, Default to None. By default launch analysis with an automatic classification. (options: <code>url, domain, hash, ip, generic</code>)</p> <code>None</code> <p>Raises:</p> Type Description <code>IntelOwlClientException</code> <p>on client/HTTP error</p> <code>IntelOwlClientException</code> <p>on wrong observable_classification</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>JSON body</p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/pyintelowl.py</code> <pre><code>def send_observable_analysis_request(\n    self,\n    observable_name: str,\n    tlp: TLPType = \"CLEAR\",\n    analyzers_requested: List[str] = None,\n    connectors_requested: List[str] = None,\n    runtime_configuration: Dict = None,\n    tags_labels: List[str] = None,\n    observable_classification: str = None,\n) -&gt; Dict:\n    \"\"\"Send analysis request for an observable.\\n\n    Endpoint: ``/api/analyze_observable``\n\n    Args:\n        observable_name (str):\n            Observable value\n        analyzers_requested (List[str], optional):\n            List of analyzers to invoke\n            Defaults to ``[]`` i.e. all analyzers.\n        connectors_requested (List[str], optional):\n            List of specific connectors to invoke.\n            Defaults to ``[]`` i.e. all connectors.\n        tlp (str, optional):\n            TLP for the analysis.\n            (options: ``CLEAR, GREEN, AMBER, RED``).\n        runtime_configuration (Dict, optional):\n            Overwrite configuration for analyzers. Defaults to ``{}``.\n        tags_labels (List[str], optional):\n            List of tag labels to assign (creates non-existing tags)\n        observable_classification (str):\n            Observable classification, Default to None.\n            By default launch analysis with an automatic classification.\n            (options: ``url, domain, hash, ip, generic``)\n\n    Raises:\n        IntelOwlClientException: on client/HTTP error\n        IntelOwlClientException: on wrong observable_classification\n\n    Returns:\n        Dict: JSON body\n    \"\"\"\n    try:\n        if not tlp:\n            tlp = \"CLEAR\"\n        if not analyzers_requested:\n            analyzers_requested = []\n        if not connectors_requested:\n            connectors_requested = []\n        if not tags_labels:\n            tags_labels = []\n        if not runtime_configuration:\n            runtime_configuration = {}\n        if not observable_classification:\n            observable_classification = self._get_observable_classification(\n                observable_name\n            )\n        elif observable_classification not in [\n            \"generic\",\n            \"hash\",\n            \"ip\",\n            \"domain\",\n            \"url\",\n        ]:\n            raise IntelOwlClientException(\n                \"Observable classification only handle\"\n                \" 'generic', 'hash', 'ip', 'domain' and 'url' \"\n            )\n        data = {\n            \"observable_name\": observable_name,\n            \"observable_classification\": observable_classification,\n            \"analyzers_requested\": analyzers_requested,\n            \"connectors_requested\": connectors_requested,\n            \"tlp\": tlp,\n            \"tags_labels\": tags_labels,\n            \"runtime_configuration\": runtime_configuration,\n        }\n        answer = self.__send_analysis_request(data=data, files=None)\n    except Exception as e:\n        raise IntelOwlClientException(e)\n    return answer\n</code></pre>"},{"location":"pyintelowl/IntelOwlClientException/","title":"IntelOwlClientException","text":""},{"location":"pyintelowl/IntelOwlClientException/#intelowlclientexception-class","title":"IntelOwlClientException Class","text":"<p>               Bases: <code>RequestException</code></p> Source code in <code>docs/Submodules/pyintelowl/pyintelowl/exceptions.py</code> <pre><code>class IntelOwlClientException(RequestException):\n    @property\n    def error_detail(self) -&gt; typing.Union[typing.Dict, typing.AnyStr]:\n        content = None\n        try:\n            content = self.response.json()\n            detail = content.get(\"detail\", None)\n            if detail:\n                content = detail\n        except json.JSONDecodeError:\n            content = self.response.content\n        except Exception:\n            pass\n\n        return content\n\n    def __str__(self):\n        err_msg = super().__str__()\n        detail = self.error_detail\n        return err_msg + f\". Details: {detail}\"\n</code></pre>"},{"location":"pyintelowl/Tests/","title":"Tests","text":""},{"location":"pyintelowl/Tests/#configuration","title":"Configuration","text":"<p>Some tests require file samples, which can be found in the encrypted folder <code>tests/test_files.zip</code> (password: \"infected\"). Unzip the archive in <code>tests/test_files</code> folder before running the tests.</p> <p>Please remember that these are dangerous malware! They come encrypted and locked for a reason! Do NOT run them unless you are absolutely sure of what you are doing! They are to be used only for launching specific tests that require them (<code>__send_analysis_request</code>)</p> <ul> <li> <p>With the following constants in <code>__init__.py</code>, you can customize your tests:</p> <ul> <li>MOCKING_CONNECTIONS: Mock connections to external API to test functions without a real connection or a valid API Key.</li> </ul> </li> </ul> <ul> <li> <p>If you prefer to use custom inputs for tests, you can change the following constants:</p> <ul> <li>TEST_JOB_ID</li> <li>TEST_HASH</li> <li>TEST_URL</li> <li>TEST_IP</li> <li>TEST_DOMAIN</li> <li>TEST_GENERIC</li> <li>TEST_FILE</li> <li>TEST_FILE_HASH</li> </ul> </li> </ul>"},{"location":"pyintelowl/Tests/#launch-tests","title":"Launch Tests","text":"<ul> <li>The test requirements are specified in the <code>test-requirements.txt</code> file. Install them using,</li> </ul> <p>.. code-block:: bash</p> <pre><code>$ pip3 install -r test-requirements.txt\n</code></pre> <ul> <li>Launch the tests using <code>tox</code>:</li> </ul> <p>.. code-block:: bash</p> <pre><code>$ tox\n</code></pre>"}]}